{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 02 Sampling and the Central Limit Theorem\n",
        "\n",
        "Why sample statistics work, how sampling distributions behave, and the theorem that makes inference possible.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Table of Contents\n",
        "- [Population vs sample](#population-vs-sample)\n",
        "- [Sampling variability](#sampling-variability)\n",
        "- [The Law of Large Numbers](#the-law-of-large-numbers)\n",
        "- [The Central Limit Theorem](#the-central-limit-theorem)\n",
        "- [When does n=30 suffice?](#when-does-n30-suffice)\n",
        "- [Standard error of the mean](#standard-error-of-the-mean)\n",
        "- [CLT with real economic data](#clt-with-real-economic-data)\n",
        "- [Checkpoint (Self-Check)](#checkpoint-self-check)\n",
        "- [Solutions (Reference)](#solutions-reference)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Why This Notebook Matters\n",
        "The Central Limit Theorem is the single most important result in applied statistics.\n",
        "It explains why confidence intervals work, why t-tests are valid, and why regression\n",
        "inference is possible even when your data is not normally distributed. Without the CLT,\n",
        "most of the statistical tools in this project would not be justified.\n",
        "\n",
        "## Prerequisites (Quick Self-Check)\n",
        "- Completed notebooks 00-01 (descriptive statistics and distributions).\n",
        "- Familiarity with the normal distribution.\n",
        "\n",
        "## What You Will Produce\n",
        "- (no file output; learning/analysis notebook)\n",
        "\n",
        "## Success Criteria\n",
        "- You can explain the difference between a population parameter and a sample statistic.\n",
        "- You can demonstrate the CLT through simulation and interpret the result.\n",
        "- You can compute and interpret the standard error of the mean.\n",
        "- You know when the \"n=30\" rule of thumb breaks down.\n",
        "\n",
        "## Common Pitfalls\n",
        "- Confusing the distribution of the data with the distribution of the sample mean.\n",
        "- Thinking the CLT says individual observations become normal (it does not).\n",
        "- Assuming the CLT works for any n regardless of how skewed the data is.\n",
        "- Forgetting that the CLT requires independent observations (violated in time series).\n",
        "\n",
        "## Quick Fixes (When You Get Stuck)\n",
        "- If your simulation is slow, reduce the number of repetitions to 500 first, then increase.\n",
        "- If histograms look choppy, increase bins or use KDE.\n",
        "- If you see `ModuleNotFoundError`, re-run the bootstrap cell.\n",
        "\n",
        "## Matching Guide\n",
        "- `docs/guides/00_statistics_primer/02_sampling_and_central_limit_theorem.md`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## How To Use This Notebook\n",
        "- Work section-by-section; don't skip the markdown.\n",
        "- Most code cells are incomplete on purpose: replace TODOs and `...`, then run.\n",
        "- After each section, write 2\u20134 sentences answering the interpretation prompts (what changed, why it matters).\n",
        "- Prefer `data/processed/*` if you have built the real datasets; otherwise use the bundled `data/sample/*` fallbacks.\n",
        "- Use the **Checkpoint (Self-Check)** section to catch mistakes early.\n",
        "- Use **Solutions (Reference)** only to unblock yourself; then re-implement without looking.\n",
        "- Use the matching guide (`docs/guides/00_statistics_primer/02_sampling_and_central_limit_theorem.md`) for the math, assumptions, and deeper context.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"environment-bootstrap\"></a>\n",
        "## Environment Bootstrap\n",
        "Run this cell first. It makes the repo importable and defines common directories.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "\n",
        "def find_repo_root(start: Path) -> Path:\n",
        "    p = start\n",
        "    for _ in range(8):\n",
        "        if (p / 'src').exists() and (p / 'docs').exists():\n",
        "            return p\n",
        "        p = p.parent\n",
        "    raise RuntimeError('Could not find repo root. Start Jupyter from the repo root.')\n",
        "\n",
        "\n",
        "PROJECT_ROOT = find_repo_root(Path.cwd())\n",
        "if str(PROJECT_ROOT) not in sys.path:\n",
        "    sys.path.append(str(PROJECT_ROOT))\n",
        "\n",
        "DATA_DIR = PROJECT_ROOT / 'data'\n",
        "RAW_DIR = DATA_DIR / 'raw'\n",
        "PROCESSED_DIR = DATA_DIR / 'processed'\n",
        "SAMPLE_DIR = DATA_DIR / 'sample'\n",
        "\n",
        "PROJECT_ROOT\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Concept\n",
        "This notebook builds your intuition for the mechanics of statistical inference.\n",
        "\n",
        "You will see:\n",
        "- why a sample mean is a useful estimate of a population mean,\n",
        "- how sample-to-sample variability shrinks as $n$ grows,\n",
        "- why the Law of Large Numbers guarantees convergence,\n",
        "- why the Central Limit Theorem makes the normal distribution show up everywhere,\n",
        "- when the CLT approximation is good enough and when it is not.\n",
        "\n",
        "Everything in this notebook is simulation-based: you will *see* the theorems work\n",
        "before you rely on them for regression inference later in the project.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"population-vs-sample\"></a>\n",
        "## Population vs sample\n",
        "\n",
        "### Goal\n",
        "Understand the distinction between a population parameter and a sample statistic.\n",
        "\n",
        "### Why this matters in economics\n",
        "In economics, we almost never observe the full population. The true mean quarterly\n",
        "GDP growth rate across *all possible* quarters (the population parameter $\\mu$) is\n",
        "unknown. What we have is a finite sample\u2014say, 80 quarters of data\u2014from which we\n",
        "compute a sample mean $\\bar{x}$. The entire goal of inference is to say something\n",
        "reliable about $\\mu$ using only $\\bar{x}$ and its uncertainty.\n",
        "\n",
        "### Key definitions\n",
        "- **Population parameter** ($\\mu$, $\\sigma$): a fixed but unknown quantity describing\n",
        "  the full data-generating process.\n",
        "- **Sample statistic** ($\\bar{x}$, $s$): a quantity computed from observed data.\n",
        "  It varies from sample to sample.\n",
        "- **Sampling distribution**: the distribution of a sample statistic across many\n",
        "  hypothetical repeated samples of the same size.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Your Turn (1): Population vs sample in action\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "rng = np.random.default_rng(42)\n",
        "\n",
        "# Imagine a \"population\" of quarterly GDP growth rates (in %).\n",
        "# True mean = 0.8%, true std = 1.5%\n",
        "pop_mean = 0.8\n",
        "pop_std = 1.5\n",
        "population = rng.normal(loc=pop_mean, scale=pop_std, size=100_000)\n",
        "\n",
        "# TODO: Draw a sample of n=80 quarters from the population\n",
        "sample = ...\n",
        "\n",
        "# TODO: Compute the sample mean and sample std\n",
        "sample_mean = ...\n",
        "sample_std = ...\n",
        "\n",
        "print(f'Population mean (true):  {pop_mean}')\n",
        "print(f'Sample mean (n=80):      {sample_mean:.4f}')\n",
        "print(f'Population std (true):   {pop_std}')\n",
        "print(f'Sample std (n=80):       {sample_std:.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Interpretation prompt:**\n",
        "- Is the sample mean exactly equal to the population mean? Why or why not?\n",
        "- If you drew a *different* sample of 80 quarters, would you get the same sample mean?\n",
        "- Write 2\u20133 sentences.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"sampling-variability\"></a>\n",
        "## Sampling variability\n",
        "\n",
        "### Goal\n",
        "See empirically that sample means vary from sample to sample, and that larger samples\n",
        "produce less variable estimates.\n",
        "\n",
        "### Why this matters in economics\n",
        "When you read that \"average GDP growth was 2.1% over the last 40 quarters,\" that number\n",
        "carries uncertainty. A different 40-quarter window would give a different number.\n",
        "Understanding sampling variability is the first step toward quantifying that uncertainty.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Your Turn (1): Draw many samples, compute many means\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "rng = np.random.default_rng(42)\n",
        "\n",
        "# Population: 100,000 values from a normal distribution\n",
        "pop_mean = 0.8\n",
        "pop_std = 1.5\n",
        "population = rng.normal(loc=pop_mean, scale=pop_std, size=100_000)\n",
        "\n",
        "n_reps = 1_000  # number of repeated samples\n",
        "sample_sizes = [10, 50, 200]\n",
        "\n",
        "# TODO: For each sample size, draw n_reps samples and compute the mean of each.\n",
        "# Store results in a dict: {n: array_of_means}\n",
        "means_by_n = {}\n",
        "for n in sample_sizes:\n",
        "    means = ...\n",
        "    means_by_n[n] = means\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Your Turn (2): Plot histograms of sample means\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# TODO: Create a figure with 3 subplots (one per sample size).\n",
        "# Plot a histogram of sample means for each n.\n",
        "# Add a vertical line at the true population mean.\n",
        "# Hint: use plt.subplots(1, 3, figsize=(14, 4))\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(14, 4), sharey=True)\n",
        "\n",
        "for ax, n in zip(axes, sample_sizes):\n",
        "    ...\n",
        "    ax.set_title(f'n = {n} (std = {means_by_n[n].std():.4f})')\n",
        "    ax.set_xlabel('Sample mean')\n",
        "\n",
        "axes[0].set_ylabel('Frequency')\n",
        "fig.suptitle('Sampling Distribution of the Mean', fontsize=13)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Interpretation prompt:**\n",
        "- How does the spread of sample means change as n increases?\n",
        "- Which sample size gives the most \"precise\" estimate of the population mean?\n",
        "- Write 2\u20133 sentences relating this to the idea of using more data in economics.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"the-law-of-large-numbers\"></a>\n",
        "## The Law of Large Numbers\n",
        "\n",
        "### Goal\n",
        "Demonstrate that the sample mean converges to the population mean as $n$ grows.\n",
        "\n",
        "### Why this matters in economics\n",
        "The LLN is why we trust averages. If you compute the average inflation rate over\n",
        "more and more months, it gets closer and closer to the true long-run average.\n",
        "Without LLN, collecting more data would not help.\n",
        "\n",
        "### Key statement\n",
        "If $X_1, X_2, \\ldots$ are i.i.d. with mean $\\mu$, then\n",
        "\n",
        "$$\\bar{X}_n = \\frac{1}{n}\\sum_{i=1}^{n} X_i \\xrightarrow{p} \\mu \\quad \\text{as } n \\to \\infty$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Your Turn (1): Running mean with normal data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "rng = np.random.default_rng(42)\n",
        "\n",
        "true_mean = 2.5\n",
        "n_obs = 5_000\n",
        "\n",
        "# TODO: Generate n_obs draws from a normal distribution with mean=true_mean, std=3.0\n",
        "data_normal = ...\n",
        "\n",
        "# TODO: Compute the cumulative (running) mean\n",
        "# Hint: np.cumsum(data_normal) / np.arange(1, n_obs + 1)\n",
        "running_mean_normal = ...\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(running_mean_normal, label='Running mean (normal data)')\n",
        "plt.axhline(true_mean, color='red', linestyle='--', label=f'True mean = {true_mean}')\n",
        "plt.xlabel('Number of observations')\n",
        "plt.ylabel('Running mean')\n",
        "plt.title('Law of Large Numbers: Convergence of the Running Mean')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Your Turn (2): Running mean with skewed data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# The LLN does not require normality. It works for any distribution with a finite mean.\n",
        "# TODO: Generate n_obs draws from an exponential distribution (rate=1.0, so mean=1.0)\n",
        "data_skewed = ...\n",
        "\n",
        "# TODO: Compute the running mean\n",
        "running_mean_skewed = ...\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(running_mean_skewed, label='Running mean (exponential data)')\n",
        "plt.axhline(1.0, color='red', linestyle='--', label='True mean = 1.0')\n",
        "plt.xlabel('Number of observations')\n",
        "plt.ylabel('Running mean')\n",
        "plt.title('LLN with Highly Skewed (Exponential) Data')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Interpretation prompt:**\n",
        "- Does the running mean converge for the exponential data even though the data itself is skewed?\n",
        "- How does the convergence path differ between normal and exponential data?\n",
        "- Write 2\u20133 sentences.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"the-central-limit-theorem\"></a>\n",
        "## The Central Limit Theorem\n",
        "\n",
        "### Goal\n",
        "Show that the sampling distribution of the mean becomes approximately normal\n",
        "regardless of the shape of the underlying data distribution.\n",
        "\n",
        "### Why this matters in economics\n",
        "Economic data is rarely normally distributed. Income is right-skewed. Unemployment\n",
        "durations are exponential. Housing prices are multimodal. Yet we use t-tests,\n",
        "confidence intervals, and regression inference that assume normality of *estimators*.\n",
        "The CLT is the justification: even if the data is non-normal, the sample mean\n",
        "(and by extension, OLS coefficients) is approximately normal for large enough $n$.\n",
        "\n",
        "### Key statement\n",
        "If $X_1, \\ldots, X_n$ are i.i.d. with mean $\\mu$ and finite variance $\\sigma^2$, then\n",
        "\n",
        "$$\\frac{\\bar{X}_n - \\mu}{\\sigma / \\sqrt{n}} \\xrightarrow{d} N(0, 1) \\quad \\text{as } n \\to \\infty$$\n",
        "\n",
        "In plain language: the distribution of the sample mean gets closer and closer to\n",
        "a normal distribution, centered at $\\mu$, with standard deviation $\\sigma / \\sqrt{n}$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Your Turn (1): CLT from a uniform distribution\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "rng = np.random.default_rng(42)\n",
        "\n",
        "n_reps = 2_000\n",
        "sample_sizes = [5, 30, 100, 500]\n",
        "\n",
        "# Source distribution: Uniform(0, 1) -- flat, not bell-shaped at all\n",
        "# True mean = 0.5, true std = 1/sqrt(12) ~ 0.2887\n",
        "\n",
        "# TODO: For each sample size, draw n_reps samples from Uniform(0,1),\n",
        "# compute the mean of each sample, and store the array of means.\n",
        "uniform_means = {}\n",
        "for n in sample_sizes:\n",
        "    ...\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# TODO: Plot histograms (with KDE) of sample means for each n in a 1x4 grid.\n",
        "# Overlay a normal PDF with the theoretical mean and SE.\n",
        "from scipy import stats\n",
        "\n",
        "fig, axes = plt.subplots(1, 4, figsize=(16, 4), sharey=True)\n",
        "true_mean = 0.5\n",
        "true_std = 1.0 / np.sqrt(12)\n",
        "\n",
        "for ax, n in zip(axes, sample_sizes):\n",
        "    se = true_std / np.sqrt(n)\n",
        "    ax.hist(uniform_means[n], bins=40, density=True, alpha=0.6, label='Simulated')\n",
        "    # TODO: Overlay a normal PDF with mean=true_mean, std=se\n",
        "    x_grid = np.linspace(true_mean - 4 * se, true_mean + 4 * se, 200)\n",
        "    ...\n",
        "    ax.set_title(f'Uniform, n={n}')\n",
        "    ax.legend(fontsize=8)\n",
        "\n",
        "fig.suptitle('CLT: Sampling Distribution of the Mean (Uniform Source)', fontsize=13)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Your Turn (2): CLT from an exponential distribution\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "rng = np.random.default_rng(42)\n",
        "\n",
        "# Source distribution: Exponential(rate=1) -- heavily right-skewed\n",
        "# True mean = 1.0, true std = 1.0\n",
        "\n",
        "# TODO: For each sample size, draw n_reps samples from Exponential(scale=1.0),\n",
        "# compute the mean of each, and store.\n",
        "exp_means = {}\n",
        "for n in sample_sizes:\n",
        "    ...\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# TODO: Plot the sampling distributions for exponential source (same layout as above).\n",
        "fig, axes = plt.subplots(1, 4, figsize=(16, 4), sharey=True)\n",
        "true_mean_exp = 1.0\n",
        "true_std_exp = 1.0\n",
        "\n",
        "for ax, n in zip(axes, sample_sizes):\n",
        "    se = true_std_exp / np.sqrt(n)\n",
        "    ...\n",
        "    ax.set_title(f'Exponential, n={n}')\n",
        "\n",
        "fig.suptitle('CLT: Sampling Distribution of the Mean (Exponential Source)', fontsize=13)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Your Turn (3): CLT from a bimodal mixture\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "rng = np.random.default_rng(42)\n",
        "\n",
        "# Bimodal mixture: 50% from N(-2, 0.5^2) and 50% from N(2, 0.5^2)\n",
        "# True mean = 0.0, true variance = 0.5^2 + 2^2 = 4.25, true std ~ 2.062\n",
        "\n",
        "def draw_bimodal(rng, size):\n",
        "    \"\"\"Draw from a 50/50 mixture of N(-2, 0.5) and N(2, 0.5).\"\"\"\n",
        "    mask = rng.random(size) < 0.5\n",
        "    vals = np.where(mask,\n",
        "                    rng.normal(-2, 0.5, size),\n",
        "                    rng.normal(2, 0.5, size))\n",
        "    return vals\n",
        "\n",
        "# TODO: For each sample size, draw n_reps samples from the bimodal mixture,\n",
        "# compute the mean of each, and store.\n",
        "bimodal_means = {}\n",
        "for n in sample_sizes:\n",
        "    ...\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# TODO: Plot the sampling distributions for bimodal source.\n",
        "fig, axes = plt.subplots(1, 4, figsize=(16, 4), sharey=True)\n",
        "true_mean_bi = 0.0\n",
        "true_std_bi = np.sqrt(0.5**2 + 2.0**2)  # ~ 2.062\n",
        "\n",
        "for ax, n in zip(axes, sample_sizes):\n",
        "    se = true_std_bi / np.sqrt(n)\n",
        "    ...\n",
        "    ax.set_title(f'Bimodal, n={n}')\n",
        "\n",
        "fig.suptitle('CLT: Sampling Distribution of the Mean (Bimodal Source)', fontsize=13)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Interpretation prompt:**\n",
        "- For which source distribution does the normal approximation kick in fastest?\n",
        "- For which source distribution does it take the largest n?\n",
        "- Does the bimodal source look normal at n=5? At n=100?\n",
        "- Write 3\u20134 sentences connecting this to the types of data you encounter in economics.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"when-does-n30-suffice\"></a>\n",
        "## When does n=30 suffice?\n",
        "\n",
        "### Goal\n",
        "Test the common textbook rule of thumb that \"n=30 is enough for the CLT\" and\n",
        "show where it breaks down.\n",
        "\n",
        "### Why this matters in economics\n",
        "Many applied researchers assume their sample is \"large enough\" without checking.\n",
        "For mildly skewed data, n=30 is often fine. But for heavily skewed distributions\n",
        "(e.g., income, wealth, firm size), you may need n=100 or more before the sampling\n",
        "distribution of the mean is approximately normal.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Your Turn (1): Shapiro-Wilk test on sampling distributions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from scipy.stats import shapiro\n",
        "\n",
        "rng = np.random.default_rng(42)\n",
        "\n",
        "n_reps = 1_000\n",
        "test_ns = [10, 30, 50, 100, 200, 500]\n",
        "\n",
        "# We test three distributions with increasing skewness:\n",
        "# (a) Normal(0, 1)      -- skewness = 0\n",
        "# (b) Exponential(1)    -- skewness = 2\n",
        "# (c) Lognormal(0, 1)   -- skewness ~ 6.18 (very heavy right tail)\n",
        "\n",
        "distributions = {\n",
        "    'Normal':      lambda rng, size: rng.normal(0, 1, size),\n",
        "    'Exponential':  lambda rng, size: rng.exponential(1.0, size),\n",
        "    'Lognormal':    lambda rng, size: rng.lognormal(0, 1, size),\n",
        "}\n",
        "\n",
        "# TODO: For each distribution and each n, generate n_reps sample means,\n",
        "# then run the Shapiro-Wilk test on those means.\n",
        "# Store p-values in a DataFrame with rows=distributions, columns=sample sizes.\n",
        "results = {}\n",
        "for dist_name, draw_fn in distributions.items():\n",
        "    row = {}\n",
        "    for n in test_ns:\n",
        "        means = ...\n",
        "        _, p_val = shapiro(means)\n",
        "        row[n] = p_val\n",
        "    results[dist_name] = row\n",
        "\n",
        "sw_df = pd.DataFrame(results).T\n",
        "sw_df.columns = [f'n={n}' for n in test_ns]\n",
        "sw_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Interpretation prompt:**\n",
        "- For Normal source data, is the Shapiro-Wilk p-value high at every n? Why?\n",
        "- For Lognormal source data, at what n does the p-value first exceed 0.05?\n",
        "- Does the \"n=30 rule\" work for the lognormal? What would you recommend instead?\n",
        "- Write 3\u20134 sentences.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"standard-error-of-the-mean\"></a>\n",
        "## Standard error of the mean\n",
        "\n",
        "### Goal\n",
        "Verify empirically that the standard deviation of sample means equals $\\sigma / \\sqrt{n}$.\n",
        "\n",
        "### Why this matters in economics\n",
        "The standard error (SE) is the bridge between a point estimate and a confidence interval.\n",
        "When you see $\\hat{\\beta} \\pm 1.96 \\times SE$ in a regression table, the SE is doing the\n",
        "heavy lifting. Understanding where it comes from\u2014and that it shrinks as $\\sqrt{n}$\u2014is\n",
        "essential for interpreting any empirical result.\n",
        "\n",
        "### Key formula\n",
        "$$SE(\\bar{X}) = \\frac{\\sigma}{\\sqrt{n}}$$\n",
        "\n",
        "In practice, $\\sigma$ is unknown and estimated by the sample standard deviation $s$,\n",
        "so $\\widehat{SE} = s / \\sqrt{n}$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Your Turn (1): Empirical SE vs theoretical SE\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "rng = np.random.default_rng(42)\n",
        "\n",
        "pop_std = 1.5\n",
        "n_reps = 5_000\n",
        "sample_sizes = [5, 10, 25, 50, 100, 200, 500, 1000]\n",
        "\n",
        "# TODO: For each n, compute the empirical std of the sample means\n",
        "# and compare to the theoretical SE = pop_std / sqrt(n).\n",
        "empirical_se = []\n",
        "theoretical_se = []\n",
        "\n",
        "for n in sample_sizes:\n",
        "    means = ...\n",
        "    empirical_se.append(means.std())\n",
        "    theoretical_se.append(pop_std / np.sqrt(n))\n",
        "\n",
        "se_df = pd.DataFrame({\n",
        "    'n': sample_sizes,\n",
        "    'Empirical SE': empirical_se,\n",
        "    'Theoretical SE': theoretical_se,\n",
        "})\n",
        "se_df['Ratio (Empirical/Theoretical)'] = se_df['Empirical SE'] / se_df['Theoretical SE']\n",
        "se_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Your Turn (2): Plot SE vs n\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# TODO: Plot empirical SE and theoretical SE vs n on the same axes.\n",
        "# Use a log-log scale to see the sqrt(n) relationship clearly.\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 5))\n",
        "...\n",
        "ax.set_xlabel('Sample size (n)')\n",
        "ax.set_ylabel('Standard Error')\n",
        "ax.set_title('Standard Error of the Mean: Empirical vs Theoretical')\n",
        "ax.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Interpretation prompt:**\n",
        "- Is the ratio of empirical to theoretical SE close to 1.0 for all n?\n",
        "- On the log-log plot, what is the slope? Why does the SE shrink as $1/\\sqrt{n}$?\n",
        "- To cut the SE in half, you need to multiply n by what factor?\n",
        "- Write 2\u20133 sentences connecting this to the cost of data collection in economics.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"clt-with-real-economic-data\"></a>\n",
        "## CLT with real economic data\n",
        "\n",
        "### Goal\n",
        "Apply the bootstrap to real macro data and verify that the sampling distribution\n",
        "of the mean is approximately normal.\n",
        "\n",
        "### Why this matters in economics\n",
        "The bootstrap is one of the most practical tools in applied econometrics. When you\n",
        "cannot derive the sampling distribution analytically, you can approximate it by\n",
        "resampling your data. This section bridges simulation to real-world practice.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Your Turn (1): Load the macro quarterly dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load macro_quarterly_sample.csv\n",
        "# Fall back to sample data if processed is not available.\n",
        "processed_path = PROCESSED_DIR / 'macro_quarterly.csv'\n",
        "sample_path = SAMPLE_DIR / 'macro_quarterly_sample.csv'\n",
        "\n",
        "csv_path = processed_path if processed_path.exists() else sample_path\n",
        "df_macro = pd.read_csv(csv_path, index_col=0, parse_dates=True)\n",
        "\n",
        "print(f'Loaded: {csv_path.name}  shape: {df_macro.shape}')\n",
        "df_macro.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Your Turn (2): Bootstrap the mean GDP growth\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "rng = np.random.default_rng(42)\n",
        "\n",
        "# TODO: Identify the GDP growth column (inspect df_macro.columns).\n",
        "# Drop NaNs for that column.\n",
        "gdp_col = ...  # e.g., 'gdp_growth_qoq' or similar; inspect df_macro.columns\n",
        "gdp_data = df_macro[gdp_col].dropna().values\n",
        "\n",
        "n_boot = 5_000\n",
        "n_obs = len(gdp_data)\n",
        "\n",
        "# TODO: Bootstrap: resample n_obs values WITH replacement, compute the mean.\n",
        "# Repeat n_boot times.\n",
        "boot_means = ...\n",
        "\n",
        "print(f'Original sample mean: {gdp_data.mean():.4f}')\n",
        "print(f'Bootstrap mean of means: {boot_means.mean():.4f}')\n",
        "print(f'Bootstrap SE: {boot_means.std():.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Your Turn (3): Plot bootstrap distribution vs normal\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from scipy import stats\n",
        "\n",
        "# TODO: Plot the bootstrap distribution of means as a histogram.\n",
        "# Overlay a normal PDF with mean = boot_means.mean() and std = boot_means.std().\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 5))\n",
        "...\n",
        "ax.set_xlabel('Mean GDP growth (quarterly, %)')\n",
        "ax.set_ylabel('Density')\n",
        "ax.set_title('Bootstrap Distribution of Mean GDP Growth')\n",
        "ax.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# TODO: Run a Shapiro-Wilk test on boot_means to check normality.\n",
        "stat, p_val = ...\n",
        "print(f'Shapiro-Wilk test: statistic={stat:.4f}, p-value={p_val:.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Interpretation prompt:**\n",
        "- Does the bootstrap distribution look normal?\n",
        "- How does the bootstrap SE compare to the analytical SE ($s / \\sqrt{n}$)?\n",
        "- Why might bootstrapping be especially useful when you have a small macro sample?\n",
        "- Write 3\u20134 sentences.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Where This Shows Up Later\n",
        "The ideas in this notebook appear throughout the rest of the project:\n",
        "\n",
        "- **Confidence intervals (notebook 04):** The SE formula from this notebook is used\n",
        "  directly to construct confidence intervals around point estimates.\n",
        "- **Hypothesis testing (notebook 05):** The CLT justifies using the normal/t-distribution\n",
        "  to compute p-values for regression coefficients.\n",
        "- **Regression inference (module 02):** Every `statsmodels` summary table relies on\n",
        "  the CLT. The `std err` column is the estimated SE; the `t` column is the coefficient\n",
        "  divided by its SE; the confidence interval is $\\hat{\\beta} \\pm t_{\\alpha/2} \\times SE$.\n",
        "- **Bootstrap and HAC standard errors (notebooks 02_regression/04):** When CLT assumptions\n",
        "  are strained (small n, autocorrelation), robust SE methods and the bootstrap provide\n",
        "  alternatives grounded in the same logic.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"checkpoint-self-check\"></a>\n",
        "## Checkpoint (Self-Check)\n",
        "Run a few asserts and write 2\u20133 sentences summarizing what you verified.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Sanity checks (adjust variable names to match your work)\n",
        "\n",
        "# 1) The mean of many sample means should be close to the true population mean.\n",
        "# assert abs(np.mean(means_by_n[200]) - pop_mean) < 0.1\n",
        "\n",
        "# 2) The empirical SE should be close to the theoretical SE.\n",
        "# assert abs(se_df['Ratio (Empirical/Theoretical)'].mean() - 1.0) < 0.05\n",
        "\n",
        "# 3) The bootstrap SE should be positive and finite.\n",
        "# assert 0 < boot_means.std() < 10\n",
        "\n",
        "# TODO: Uncomment the checks above once you've completed the TODO cells.\n",
        "# TODO: Write 2-3 sentences:\n",
        "# - What is the difference between the distribution of data and the distribution of the mean?\n",
        "# - Why does the CLT not say individual observations become normal?\n",
        "...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extensions (Optional)\n",
        "- Investigate the CLT for the *sample median* instead of the sample mean. Does it also converge to normal? (Hint: yes, but the SE formula is different.)\n",
        "- Try the bootstrap on a different column in the macro dataset (e.g., unemployment rate or inflation).\n",
        "- Explore the relationship between skewness and the minimum $n$ needed for the CLT to hold. Plot \"minimum n\" vs skewness for several distributions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reflection\n",
        "- What assumptions does the CLT require that might be violated in real economic data?\n",
        "  (Hint: think about independence in time series.)\n",
        "- If you were advising a colleague who only has 25 observations of a skewed variable,\n",
        "  what would you tell them about using normal-theory confidence intervals?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"solutions-reference\"></a>\n",
        "## Solutions (Reference)\n",
        "\n",
        "Try the TODOs first. Use these only to unblock yourself or to compare approaches.\n",
        "\n",
        "<details><summary>Solution: Population vs sample</summary>\n",
        "\n",
        "_One possible approach. Your variable names may differ; align them with the notebook._\n",
        "\n",
        "```python\n",
        "# Reference solution for 02_sampling_and_central_limit_theorem \u2014 Population vs sample\n",
        "import numpy as np\n",
        "\n",
        "rng = np.random.default_rng(42)\n",
        "pop_mean = 0.8\n",
        "pop_std = 1.5\n",
        "population = rng.normal(loc=pop_mean, scale=pop_std, size=100_000)\n",
        "\n",
        "sample = rng.choice(population, size=80, replace=False)\n",
        "sample_mean = sample.mean()\n",
        "sample_std = sample.std(ddof=1)\n",
        "\n",
        "print(f'Population mean (true):  {pop_mean}')\n",
        "print(f'Sample mean (n=80):      {sample_mean:.4f}')\n",
        "print(f'Population std (true):   {pop_std}')\n",
        "print(f'Sample std (n=80):       {sample_std:.4f}')\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "<details><summary>Solution: Sampling variability</summary>\n",
        "\n",
        "_One possible approach. Your variable names may differ; align them with the notebook._\n",
        "\n",
        "```python\n",
        "# Reference solution for 02_sampling_and_central_limit_theorem \u2014 Sampling variability\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "rng = np.random.default_rng(42)\n",
        "pop_mean = 0.8\n",
        "pop_std = 1.5\n",
        "population = rng.normal(loc=pop_mean, scale=pop_std, size=100_000)\n",
        "\n",
        "n_reps = 1_000\n",
        "sample_sizes = [10, 50, 200]\n",
        "\n",
        "means_by_n = {}\n",
        "for n in sample_sizes:\n",
        "    means = np.array([rng.choice(population, size=n).mean() for _ in range(n_reps)])\n",
        "    means_by_n[n] = means\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(14, 4), sharey=True)\n",
        "for ax, n in zip(axes, sample_sizes):\n",
        "    ax.hist(means_by_n[n], bins=40, alpha=0.7, edgecolor='black')\n",
        "    ax.axvline(pop_mean, color='red', linestyle='--', label=f'True mean={pop_mean}')\n",
        "    ax.set_title(f'n = {n} (std = {means_by_n[n].std():.4f})')\n",
        "    ax.set_xlabel('Sample mean')\n",
        "    ax.legend(fontsize=8)\n",
        "axes[0].set_ylabel('Frequency')\n",
        "fig.suptitle('Sampling Distribution of the Mean', fontsize=13)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "<details><summary>Solution: Law of Large Numbers</summary>\n",
        "\n",
        "_One possible approach. Your variable names may differ; align them with the notebook._\n",
        "\n",
        "```python\n",
        "# Reference solution for 02_sampling_and_central_limit_theorem \u2014 Law of Large Numbers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "rng = np.random.default_rng(42)\n",
        "true_mean = 2.5\n",
        "n_obs = 5_000\n",
        "\n",
        "# Normal data\n",
        "data_normal = rng.normal(loc=true_mean, scale=3.0, size=n_obs)\n",
        "running_mean_normal = np.cumsum(data_normal) / np.arange(1, n_obs + 1)\n",
        "\n",
        "# Skewed (exponential) data\n",
        "data_skewed = rng.exponential(scale=1.0, size=n_obs)\n",
        "running_mean_skewed = np.cumsum(data_skewed) / np.arange(1, n_obs + 1)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
        "axes[0].plot(running_mean_normal)\n",
        "axes[0].axhline(true_mean, color='red', linestyle='--')\n",
        "axes[0].set_title('Normal Data')\n",
        "axes[1].plot(running_mean_skewed)\n",
        "axes[1].axhline(1.0, color='red', linestyle='--')\n",
        "axes[1].set_title('Exponential Data')\n",
        "for ax in axes:\n",
        "    ax.set_xlabel('n')\n",
        "    ax.set_ylabel('Running mean')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "<details><summary>Solution: Central Limit Theorem (uniform)</summary>\n",
        "\n",
        "_One possible approach. Your variable names may differ; align them with the notebook._\n",
        "\n",
        "```python\n",
        "# Reference solution for 02_sampling_and_central_limit_theorem \u2014 CLT (uniform)\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "\n",
        "rng = np.random.default_rng(42)\n",
        "n_reps = 2_000\n",
        "sample_sizes = [5, 30, 100, 500]\n",
        "\n",
        "uniform_means = {}\n",
        "for n in sample_sizes:\n",
        "    uniform_means[n] = np.array([rng.uniform(0, 1, size=n).mean() for _ in range(n_reps)])\n",
        "\n",
        "fig, axes = plt.subplots(1, 4, figsize=(16, 4), sharey=True)\n",
        "true_mean = 0.5\n",
        "true_std = 1.0 / np.sqrt(12)\n",
        "\n",
        "for ax, n in zip(axes, sample_sizes):\n",
        "    se = true_std / np.sqrt(n)\n",
        "    ax.hist(uniform_means[n], bins=40, density=True, alpha=0.6, label='Simulated')\n",
        "    x_grid = np.linspace(true_mean - 4*se, true_mean + 4*se, 200)\n",
        "    ax.plot(x_grid, stats.norm.pdf(x_grid, true_mean, se), 'r-', lw=2, label='Normal')\n",
        "    ax.set_title(f'Uniform, n={n}')\n",
        "    ax.legend(fontsize=8)\n",
        "fig.suptitle('CLT: Sampling Distribution of the Mean (Uniform Source)', fontsize=13)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "<details><summary>Solution: When does n=30 suffice?</summary>\n",
        "\n",
        "_One possible approach. Your variable names may differ; align them with the notebook._\n",
        "\n",
        "```python\n",
        "# Reference solution for 02_sampling_and_central_limit_theorem \u2014 n=30 rule\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.stats import shapiro\n",
        "\n",
        "rng = np.random.default_rng(42)\n",
        "n_reps = 1_000\n",
        "test_ns = [10, 30, 50, 100, 200, 500]\n",
        "\n",
        "distributions = {\n",
        "    'Normal':      lambda rng, size: rng.normal(0, 1, size),\n",
        "    'Exponential':  lambda rng, size: rng.exponential(1.0, size),\n",
        "    'Lognormal':    lambda rng, size: rng.lognormal(0, 1, size),\n",
        "}\n",
        "\n",
        "results = {}\n",
        "for dist_name, draw_fn in distributions.items():\n",
        "    row = {}\n",
        "    for n in test_ns:\n",
        "        means = np.array([draw_fn(rng, n).mean() for _ in range(n_reps)])\n",
        "        _, p_val = shapiro(means)\n",
        "        row[n] = p_val\n",
        "    results[dist_name] = row\n",
        "\n",
        "sw_df = pd.DataFrame(results).T\n",
        "sw_df.columns = [f'n={n}' for n in test_ns]\n",
        "sw_df\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "<details><summary>Solution: Standard error of the mean</summary>\n",
        "\n",
        "_One possible approach. Your variable names may differ; align them with the notebook._\n",
        "\n",
        "```python\n",
        "# Reference solution for 02_sampling_and_central_limit_theorem \u2014 Standard error\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "rng = np.random.default_rng(42)\n",
        "pop_std = 1.5\n",
        "n_reps = 5_000\n",
        "sample_sizes = [5, 10, 25, 50, 100, 200, 500, 1000]\n",
        "\n",
        "empirical_se = []\n",
        "theoretical_se = []\n",
        "for n in sample_sizes:\n",
        "    means = np.array([rng.normal(0, pop_std, size=n).mean() for _ in range(n_reps)])\n",
        "    empirical_se.append(means.std())\n",
        "    theoretical_se.append(pop_std / np.sqrt(n))\n",
        "\n",
        "se_df = pd.DataFrame({\n",
        "    'n': sample_sizes,\n",
        "    'Empirical SE': empirical_se,\n",
        "    'Theoretical SE': theoretical_se,\n",
        "})\n",
        "se_df['Ratio (Empirical/Theoretical)'] = se_df['Empirical SE'] / se_df['Theoretical SE']\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 5))\n",
        "ax.plot(sample_sizes, empirical_se, 'o-', label='Empirical SE')\n",
        "ax.plot(sample_sizes, theoretical_se, 's--', label='Theoretical SE')\n",
        "ax.set_xscale('log')\n",
        "ax.set_yscale('log')\n",
        "ax.set_xlabel('Sample size (n)')\n",
        "ax.set_ylabel('Standard Error')\n",
        "ax.set_title('Standard Error of the Mean: Empirical vs Theoretical')\n",
        "ax.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "<details><summary>Solution: CLT with real economic data</summary>\n",
        "\n",
        "_One possible approach. Your variable names may differ; align them with the notebook._\n",
        "\n",
        "```python\n",
        "# Reference solution for 02_sampling_and_central_limit_theorem \u2014 Bootstrap\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "from scipy.stats import shapiro\n",
        "\n",
        "rng = np.random.default_rng(42)\n",
        "\n",
        "# Load data (adjust column name to match your dataset)\n",
        "processed_path = PROCESSED_DIR / 'macro_quarterly.csv'\n",
        "sample_path = SAMPLE_DIR / 'macro_quarterly_sample.csv'\n",
        "csv_path = processed_path if processed_path.exists() else sample_path\n",
        "df_macro = pd.read_csv(csv_path, index_col=0, parse_dates=True)\n",
        "\n",
        "# Identify GDP growth column (inspect df_macro.columns)\n",
        "gdp_col = 'gdp_growth_qoq'  # adjust if needed\n",
        "gdp_data = df_macro[gdp_col].dropna().values\n",
        "\n",
        "n_boot = 5_000\n",
        "n_obs = len(gdp_data)\n",
        "boot_means = np.array([rng.choice(gdp_data, size=n_obs, replace=True).mean()\n",
        "                        for _ in range(n_boot)])\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 5))\n",
        "ax.hist(boot_means, bins=50, density=True, alpha=0.6, label='Bootstrap')\n",
        "x_grid = np.linspace(boot_means.min(), boot_means.max(), 200)\n",
        "ax.plot(x_grid, stats.norm.pdf(x_grid, boot_means.mean(), boot_means.std()),\n",
        "        'r-', lw=2, label='Normal fit')\n",
        "ax.set_xlabel('Mean GDP growth')\n",
        "ax.set_ylabel('Density')\n",
        "ax.set_title('Bootstrap Distribution of Mean GDP Growth')\n",
        "ax.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "stat, p_val = shapiro(boot_means)\n",
        "print(f'Shapiro-Wilk: statistic={stat:.4f}, p-value={p_val:.4f}')\n",
        "```\n",
        "\n",
        "</details>\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "python3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
