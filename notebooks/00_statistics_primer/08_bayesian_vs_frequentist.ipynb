{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-title",
   "metadata": {},
   "source": [
    "# 08 Bayesian vs Frequentist Thinking\n",
    "\n",
    "Two philosophies of probability and inference: understanding the debate and when each perspective is useful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-toc",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [Two ways to think about probability](#two-ways-to-think-about-probability)\n",
    "- [Bayes' theorem](#bayes-theorem)\n",
    "- [Prior, likelihood, and posterior](#prior-likelihood-and-posterior)\n",
    "- [Bayesian vs frequentist intervals](#bayesian-vs-frequentist-intervals)\n",
    "- [When does the prior matter?](#when-does-the-prior-matter)\n",
    "- [Practical implications for this project](#practical-implications-for-this-project)\n",
    "- [Checkpoint (Self-Check)](#checkpoint-self-check)\n",
    "- [Solutions (Reference)](#solutions-reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-why",
   "metadata": {},
   "source": [
    "## Why This Notebook Matters\n",
    "The frequentist vs Bayesian distinction is one of the deepest in statistics. This project\n",
    "primarily uses frequentist tools (p-values, confidence intervals, OLS), but understanding\n",
    "the Bayesian perspective helps you: (1) interpret frequentist results more carefully,\n",
    "(2) understand why regularization works (it's Bayesian!), and (3) think more clearly\n",
    "about uncertainty. This is a conceptual notebook \u2014 lighter on code, heavier on ideas.\n",
    "\n",
    "## Prerequisites (Quick Self-Check)\n",
    "- Completed notebooks 00\u201307 (the full primer sequence).\n",
    "- Solid understanding of hypothesis testing and confidence intervals.\n",
    "\n",
    "## What You Will Produce\n",
    "- (no file output; learning/analysis notebook)\n",
    "\n",
    "## Success Criteria\n",
    "- You can explain the difference between frequentist and Bayesian interpretations of probability.\n",
    "- You can apply Bayes' theorem to a concrete example.\n",
    "- You can describe how prior, likelihood, and posterior relate.\n",
    "- You can articulate when Bayesian and frequentist answers converge.\n",
    "\n",
    "## Common Pitfalls\n",
    "- Thinking one approach is \"right\" and the other is \"wrong\" (both have strengths).\n",
    "- Confusing a frequentist CI with a Bayesian credible interval.\n",
    "- Ignoring the prior in Bayesian analysis (always state it explicitly).\n",
    "- Thinking Bayesian methods are always better (they depend on prior choice).\n",
    "\n",
    "## Quick Fixes (When You Get Stuck)\n",
    "- Bayes' theorem: P(A|B) = P(B|A) * P(A) / P(B).\n",
    "- `scipy.stats.beta` for Beta distributions (conjugate prior for binomial).\n",
    "- If plots look wrong, check your x-axis range (0 to 1 for probability parameters).\n",
    "- If you see `ModuleNotFoundError`, re-run the bootstrap cell.\n",
    "\n",
    "## Matching Guide\n",
    "- `docs/guides/00_statistics_primer/08_bayesian_vs_frequentist.md`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-howto",
   "metadata": {},
   "source": [
    "## How To Use This Notebook\n",
    "- Work section-by-section; don't skip the markdown.\n",
    "- Most code cells are incomplete on purpose: replace TODOs and `...`, then run.\n",
    "- After each section, write 2\u20134 sentences answering the interpretation prompts (what changed, why it matters).\n",
    "- This notebook is lighter on code and heavier on conceptual understanding than earlier primers.\n",
    "- Use the **Checkpoint (Self-Check)** section to catch mistakes early.\n",
    "- Use **Solutions (Reference)** only to unblock yourself; then re-implement without looking.\n",
    "- Use the matching guide (`docs/guides/00_statistics_primer/08_bayesian_vs_frequentist.md`) for the math, assumptions, and deeper context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-bootstrap-header",
   "metadata": {},
   "source": [
    "<a id=\"environment-bootstrap\"></a>\n",
    "## Environment Bootstrap\n",
    "Run this cell first. It makes the repo importable and defines common directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-bootstrap",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "\n",
    "def find_repo_root(start: Path) -> Path:\n",
    "    p = start\n",
    "    for _ in range(8):\n",
    "        if (p / 'src').exists() and (p / 'docs').exists():\n",
    "            return p\n",
    "        p = p.parent\n",
    "    raise RuntimeError('Could not find repo root. Start Jupyter from the repo root.')\n",
    "\n",
    "\n",
    "PROJECT_ROOT = find_repo_root(Path.cwd())\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "RAW_DIR = DATA_DIR / 'raw'\n",
    "PROCESSED_DIR = DATA_DIR / 'processed'\n",
    "SAMPLE_DIR = DATA_DIR / 'sample'\n",
    "\n",
    "PROJECT_ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-two-ways-header",
   "metadata": {},
   "source": [
    "<a id=\"two-ways-to-think-about-probability\"></a>\n",
    "## Two Ways to Think About Probability\n",
    "\n",
    "### Goal\n",
    "Understand the fundamental philosophical difference between frequentist and Bayesian\n",
    "interpretations of probability.\n",
    "\n",
    "### Why this matters\n",
    "Everything downstream \u2014 how you construct confidence intervals, test hypotheses, and\n",
    "interpret p-values \u2014 depends on what you think \"probability\" *means*. Most of this\n",
    "project uses frequentist methods because they are the standard in econometrics, but\n",
    "understanding the Bayesian perspective will make you a sharper thinker.\n",
    "\n",
    "---\n",
    "\n",
    "### The frequentist view: probability as long-run frequency\n",
    "\n",
    "A coin has P(heads) = 0.5 because *if you flip it infinitely many times*, half the\n",
    "outcomes will be heads. Probability is an objective property of a repeatable process.\n",
    "\n",
    "**Strengths:**\n",
    "- No subjective input required \u2014 the data speak for themselves.\n",
    "- Well-suited to repeatable experiments (manufacturing, quality control, clinical trials).\n",
    "\n",
    "**Limitations:**\n",
    "- How do you assign probability to one-time events? What is the \"long-run frequency\"\n",
    "  of \"the US enters a recession in 2025\"? The event either happens or it doesn't.\n",
    "- Cannot formally incorporate prior knowledge (e.g., expert opinion) into the analysis.\n",
    "\n",
    "### The Bayesian view: probability as degree of belief\n",
    "\n",
    "Probability measures *your uncertainty* about an event. You can say \"I believe there is\n",
    "a 30% chance the US enters a recession next year\" \u2014 this reflects your state of\n",
    "knowledge, not a frequency.\n",
    "\n",
    "**Strengths:**\n",
    "- Can handle one-time events naturally.\n",
    "- Formally incorporates prior knowledge via Bayes' theorem.\n",
    "- Produces intuitive probability statements (\"there is a 95% probability the parameter\n",
    "  is in this interval\").\n",
    "\n",
    "**Limitations:**\n",
    "- Requires specifying a prior, which is subjective.\n",
    "- Can be computationally expensive for complex models.\n",
    "- Different priors can lead to different conclusions (especially with limited data).\n",
    "\n",
    "### The big picture\n",
    "\n",
    "| Aspect | Frequentist | Bayesian |\n",
    "|--------|-------------|----------|\n",
    "| Probability means... | Long-run frequency | Degree of belief |\n",
    "| Parameters are... | Fixed but unknown | Random variables with distributions |\n",
    "| Data are... | Random (vary across samples) | Fixed (we observed what we observed) |\n",
    "| Prior information? | Not formally used | Encoded in the prior distribution |\n",
    "| Interval meaning | \"95% of such intervals contain the true value\" | \"95% probability the parameter is here\" |\n",
    "\n",
    "> **Key insight for this project:** The rest of this project is mostly frequentist \u2014 OLS,\n",
    "> p-values, confidence intervals. But understanding the Bayesian perspective enriches\n",
    "> your thinking and helps you interpret results more carefully."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-two-ways-interp",
   "metadata": {},
   "source": [
    "**Interpretation prompt** (write 2\u20134 sentences below):\n",
    "- In your own words, what is the key difference between the two views?\n",
    "- Can you think of an economic question where the Bayesian interpretation feels more\n",
    "  natural? Where the frequentist interpretation feels more natural?\n",
    "- Why might econometrics textbooks lean frequentist?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-bayes-theorem-header",
   "metadata": {},
   "source": [
    "<a id=\"bayes-theorem\"></a>\n",
    "## Bayes' Theorem\n",
    "\n",
    "### Goal\n",
    "Apply Bayes' theorem to a concrete economic example and build intuition for\n",
    "why base rates matter.\n",
    "\n",
    "### Why this matters\n",
    "Bayes' theorem is the mathematical engine of Bayesian inference. Even if you\n",
    "use frequentist methods day-to-day, understanding Bayes' theorem helps you\n",
    "avoid the **base-rate fallacy** \u2014 one of the most common mistakes in applied work.\n",
    "\n",
    "---\n",
    "\n",
    "### The formula\n",
    "\n",
    "$$P(A \\mid B) = \\frac{P(B \\mid A) \\cdot P(A)}{P(B)}$$\n",
    "\n",
    "Where:\n",
    "- $P(A \\mid B)$ = **posterior** \u2014 probability of A given we observed B\n",
    "- $P(B \\mid A)$ = **likelihood** \u2014 probability of B if A were true\n",
    "- $P(A)$ = **prior** \u2014 probability of A before seeing B\n",
    "- $P(B)$ = **marginal likelihood** (normalizing constant)\n",
    "\n",
    "The denominator can be expanded using the law of total probability:\n",
    "\n",
    "$$P(B) = P(B \\mid A) \\cdot P(A) + P(B \\mid \\neg A) \\cdot P(\\neg A)$$\n",
    "\n",
    "### Concrete example: recession indicator\n",
    "\n",
    "Suppose you have a recession prediction model. Here are the facts:\n",
    "\n",
    "- **Sensitivity (true positive rate):** P(flag | recession) = 0.80\n",
    "  The model correctly flags 80% of actual recessions.\n",
    "- **False positive rate:** P(flag | no recession) = 0.10\n",
    "  The model incorrectly flags 10% of non-recession periods.\n",
    "- **Base rate:** P(recession) = 0.15\n",
    "  Historically, the US is in recession about 15% of the time.\n",
    "\n",
    "**Question:** The model just flagged positive. What is the probability we are\n",
    "actually in a recession?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-bayes-theorem-turn",
   "metadata": {},
   "source": [
    "### Your Turn\n",
    "\n",
    "First, try to compute this by hand (or at least set up the calculation on paper).\n",
    "Then fill in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-bayes-theorem-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Apply Bayes' theorem to the recession indicator example.\n",
    "#\n",
    "# Given:\n",
    "#   P(flag | recession)    = 0.80  (sensitivity / true positive rate)\n",
    "#   P(flag | no recession) = 0.10  (false positive rate)\n",
    "#   P(recession)           = 0.15  (base rate)\n",
    "#\n",
    "# Compute:\n",
    "#   P(recession | flag) = ?\n",
    "\n",
    "sensitivity = 0.80          # P(flag | recession)\n",
    "false_positive_rate = 0.10  # P(flag | no recession)\n",
    "base_rate = 0.15            # P(recession)\n",
    "\n",
    "# Step 1: compute P(flag) using the law of total probability\n",
    "# P(flag) = P(flag|recession)*P(recession) + P(flag|no recession)*P(no recession)\n",
    "p_flag = ...\n",
    "\n",
    "# Step 2: apply Bayes' theorem\n",
    "# P(recession | flag) = P(flag | recession) * P(recession) / P(flag)\n",
    "p_recession_given_flag = ...\n",
    "\n",
    "print(f'P(flag)                = {p_flag:.4f}')\n",
    "print(f'P(recession | flag)    = {p_recession_given_flag:.4f}')\n",
    "print(f'\\nEven with a positive flag, the probability of recession is only '\n",
    "      f'{p_recession_given_flag:.1%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-bayes-baserate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Explore how the base rate affects the posterior.\n",
    "# Vary the base rate from 0.01 to 0.50 and plot P(recession | flag).\n",
    "# This demonstrates the base-rate fallacy: even a good test can be misleading\n",
    "# when the base rate is low.\n",
    "\n",
    "base_rates = np.linspace(0.01, 0.50, 100)\n",
    "\n",
    "# TODO: compute posterior for each base rate\n",
    "posteriors = ...\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ax.plot(base_rates, posteriors, 'b-', linewidth=2)\n",
    "ax.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5, label='50% threshold')\n",
    "ax.axvline(x=0.15, color='red', linestyle='--', alpha=0.5, label='Actual base rate (15%)')\n",
    "ax.set_xlabel('Base rate P(recession)', fontsize=12)\n",
    "ax.set_ylabel('P(recession | positive flag)', fontsize=12)\n",
    "ax.set_title('How the Base Rate Affects Posterior Probability', fontsize=13)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-bayes-theorem-interp",
   "metadata": {},
   "source": [
    "**Interpretation prompt** (write 2\u20134 sentences below):\n",
    "- Were you surprised by the result? Most people expect a higher number.\n",
    "- Why does the base rate matter so much, even when the model has 80% sensitivity?\n",
    "- At what base rate does a positive flag make a recession more likely than not (>50%)?\n",
    "- How does this relate to p-values in hypothesis testing? (Hint: a significant\n",
    "  p-value is like a positive flag. What is the \"base rate\" for true effects?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-prior-likelihood-posterior-header",
   "metadata": {},
   "source": [
    "<a id=\"prior-likelihood-and-posterior\"></a>\n",
    "## Prior, Likelihood, and Posterior\n",
    "\n",
    "### Goal\n",
    "Visualize Bayesian updating: how a prior belief is combined with data (likelihood)\n",
    "to produce a posterior belief.\n",
    "\n",
    "### Why this matters\n",
    "This is the core mechanism of Bayesian inference. Understanding it helps you see\n",
    "statistics not as a one-shot calculation, but as a *learning process*: you start\n",
    "with a belief, observe data, and update. This is how rational agents should behave\n",
    "under uncertainty \u2014 and it mirrors how economists think about expectations.\n",
    "\n",
    "---\n",
    "\n",
    "### The Beta-Binomial model: a beautiful toy example\n",
    "\n",
    "Suppose you want to estimate the probability $\\theta$ that a recession indicator\n",
    "gives a correct signal. This is a proportion, so $\\theta \\in [0, 1]$.\n",
    "\n",
    "**The setup:**\n",
    "- **Prior:** Before seeing data, your belief about $\\theta$ is described by a\n",
    "  Beta distribution: $\\theta \\sim \\text{Beta}(\\alpha, \\beta)$.\n",
    "  - $\\text{Beta}(1, 1)$ = uniform (flat) prior: \"I have no idea.\"\n",
    "  - $\\text{Beta}(10, 10)$ = mildly informative: \"I think it is around 0.5.\"\n",
    "- **Data:** You observe $n$ trials with $k$ successes (e.g., correct signals).\n",
    "- **Posterior:** The updated belief is also a Beta distribution:\n",
    "  $\\theta \\mid \\text{data} \\sim \\text{Beta}(\\alpha + k, \\beta + n - k)$.\n",
    "\n",
    "This is called a **conjugate prior**: the posterior has the same distributional\n",
    "form as the prior, which makes the math tractable.\n",
    "\n",
    "### Key intuition\n",
    "\n",
    "$$\\text{Posterior} \\propto \\text{Likelihood} \\times \\text{Prior}$$\n",
    "\n",
    "The posterior is a *compromise* between the prior and the data. With little data,\n",
    "the prior dominates. With lots of data, the data dominate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-prior-likelihood-posterior-turn",
   "metadata": {},
   "source": [
    "### Your Turn\n",
    "\n",
    "Visualize how the posterior evolves as you observe more coin flips (or more\n",
    "recession indicator signals). Start with a flat prior and accumulate data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-prior-likelihood-posterior-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Visualize Bayesian updating with the Beta-Binomial model.\n",
    "#\n",
    "# Scenario: You are evaluating whether a recession indicator has a true\n",
    "# accuracy rate theta. You start with a flat prior Beta(1,1) and observe\n",
    "# results one at a time.\n",
    "#\n",
    "# Suppose you observe the following sequence of correct/incorrect signals:\n",
    "# 1 = correct, 0 = incorrect\n",
    "\n",
    "np.random.seed(42)\n",
    "observations = [1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1]\n",
    "# 16 correct out of 20 => sample proportion = 0.80\n",
    "\n",
    "# Prior parameters\n",
    "alpha_prior = 1  # Beta(1,1) = flat/uniform prior\n",
    "beta_prior = 1\n",
    "\n",
    "theta = np.linspace(0, 1, 500)\n",
    "\n",
    "# Snapshots: plot the posterior after 0, 1, 3, 5, 10, 20 observations\n",
    "snapshots = [0, 1, 3, 5, 10, 20]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(14, 8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, n_obs in enumerate(snapshots):\n",
    "    data_so_far = observations[:n_obs]\n",
    "    k = sum(data_so_far)        # number of successes\n",
    "    n = len(data_so_far)        # number of trials\n",
    "\n",
    "    # TODO: compute posterior parameters\n",
    "    alpha_post = ...\n",
    "    beta_post = ...\n",
    "\n",
    "    # TODO: compute the posterior PDF using stats.beta.pdf(theta, alpha_post, beta_post)\n",
    "    posterior_pdf = ...\n",
    "\n",
    "    axes[i].plot(theta, posterior_pdf, 'b-', linewidth=2)\n",
    "    axes[i].fill_between(theta, posterior_pdf, alpha=0.2)\n",
    "    axes[i].axvline(x=0.80, color='red', linestyle='--', alpha=0.6,\n",
    "                    label='True rate (0.80)')\n",
    "    axes[i].set_title(f'After {n_obs} obs (k={k})', fontsize=11)\n",
    "    axes[i].set_xlabel(r'$\\theta$')\n",
    "    axes[i].set_ylabel('Density')\n",
    "    axes[i].legend(fontsize=8)\n",
    "\n",
    "fig.suptitle('Bayesian Updating: Posterior Concentrates as Data Accumulates',\n",
    "             fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-prior-likelihood-posterior-components",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: For the case of n=20, plot the prior, likelihood, and posterior\n",
    "# on the same axes to see how the posterior is a compromise.\n",
    "#\n",
    "# Hint:\n",
    "#   - Prior: stats.beta.pdf(theta, alpha_prior, beta_prior)\n",
    "#   - Likelihood: stats.binom.pmf(k, n, theta)  (treat theta as the variable)\n",
    "#     Note: for visualization, normalize the likelihood to have a similar scale.\n",
    "#   - Posterior: stats.beta.pdf(theta, alpha_prior + k, beta_prior + n - k)\n",
    "\n",
    "k_total = sum(observations)    # total successes\n",
    "n_total = len(observations)    # total trials\n",
    "\n",
    "prior_pdf = ...\n",
    "likelihood = ...               # use stats.binom.pmf(k_total, n_total, theta)\n",
    "# Normalize likelihood for visual comparison\n",
    "likelihood_scaled = likelihood / likelihood.max() * stats.beta.pdf(\n",
    "    theta, alpha_prior + k_total, beta_prior + n_total - k_total).max()\n",
    "posterior_pdf = ...\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 5))\n",
    "ax.plot(theta, prior_pdf, 'g--', linewidth=2, label='Prior: Beta(1, 1)')\n",
    "ax.plot(theta, likelihood_scaled, 'orange', linewidth=2,\n",
    "        label=f'Likelihood (scaled): {k_total}/{n_total} correct', linestyle=':')\n",
    "ax.plot(theta, posterior_pdf, 'b-', linewidth=2,\n",
    "        label=f'Posterior: Beta({alpha_prior + k_total}, {beta_prior + n_total - k_total})')\n",
    "ax.axvline(x=0.80, color='red', linestyle='--', alpha=0.5, label='True rate')\n",
    "ax.set_xlabel(r'$\\theta$ (accuracy rate)', fontsize=12)\n",
    "ax.set_ylabel('Density', fontsize=12)\n",
    "ax.set_title('Prior x Likelihood = Posterior', fontsize=13)\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-prior-likelihood-posterior-interp",
   "metadata": {},
   "source": [
    "**Interpretation prompt** (write 2\u20134 sentences below):\n",
    "- How does the posterior change shape as more data arrives?\n",
    "- After 20 observations, is the posterior centered near the true value (0.80)?\n",
    "- In the prior-likelihood-posterior plot, where does the posterior \"sit\" relative\n",
    "  to the prior and likelihood? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-intervals-header",
   "metadata": {},
   "source": [
    "<a id=\"bayesian-vs-frequentist-intervals\"></a>\n",
    "## Bayesian vs Frequentist Intervals\n",
    "\n",
    "### Goal\n",
    "Compute both a frequentist confidence interval and a Bayesian credible interval\n",
    "for the same parameter, and understand how their *interpretations* differ.\n",
    "\n",
    "### Why this matters\n",
    "This is arguably the most important conceptual distinction in applied statistics.\n",
    "Most practitioners (including many economists) *think* they are making Bayesian\n",
    "statements when they report frequentist confidence intervals. Understanding the\n",
    "difference will make your statistical communication more precise.\n",
    "\n",
    "---\n",
    "\n",
    "### The two interpretations\n",
    "\n",
    "**Frequentist 95% confidence interval:**\n",
    "> \"If I repeated this experiment many times and computed a CI each time,\n",
    "> 95% of those intervals would contain the true parameter value.\"\n",
    ">\n",
    "> The *interval* is random (it varies across samples). The *parameter* is fixed.\n",
    "> You *cannot* say: \"there is a 95% probability the parameter is in this interval.\"\n",
    "\n",
    "**Bayesian 95% credible interval:**\n",
    "> \"Given the data and my prior, there is a 95% probability the parameter\n",
    "> lies in this interval.\"\n",
    ">\n",
    "> The *parameter* is random (it has a posterior distribution). The *data* are fixed.\n",
    "> This is the statement people *think* a CI makes.\n",
    "\n",
    "### Why the distinction matters in practice\n",
    "\n",
    "When an economist reports a 95% CI for the effect of minimum wage on employment\n",
    "as [&minus;0.3, &minus;0.1], the temptation is to say \"there is a 95% probability the\n",
    "true effect is between &minus;0.3 and &minus;0.1.\" That is a Bayesian statement. The\n",
    "correct frequentist statement is: \"this procedure produces intervals that cover\n",
    "the true effect 95% of the time.\" Subtle but important."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-intervals-turn",
   "metadata": {},
   "source": [
    "### Your Turn\n",
    "\n",
    "Compute both intervals for a simple proportion estimation problem using\n",
    "the recession indicator data from the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-intervals-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute a frequentist CI and a Bayesian credible interval for\n",
    "# the same proportion theta.\n",
    "#\n",
    "# Data: 16 correct signals out of 20 observations.\n",
    "\n",
    "k = 16   # successes\n",
    "n = 20   # trials\n",
    "p_hat = k / n\n",
    "\n",
    "# --- Frequentist: Wald 95% confidence interval ---\n",
    "# p_hat +/- z * sqrt(p_hat * (1 - p_hat) / n)\n",
    "z = stats.norm.ppf(0.975)  # ~1.96\n",
    "\n",
    "# TODO: compute the standard error and confidence interval bounds\n",
    "se = ...\n",
    "freq_ci_lower = ...\n",
    "freq_ci_upper = ...\n",
    "\n",
    "print('--- Frequentist 95% Confidence Interval (Wald) ---')\n",
    "print(f'Point estimate: {p_hat:.4f}')\n",
    "print(f'95% CI: [{freq_ci_lower:.4f}, {freq_ci_upper:.4f}]')\n",
    "print(f'Interpretation: If we repeated this experiment many times,')\n",
    "print(f'95% of such intervals would contain the true theta.\\n')\n",
    "\n",
    "# --- Bayesian: 95% credible interval from the posterior ---\n",
    "# Posterior: Beta(1 + k, 1 + n - k) with flat prior Beta(1,1)\n",
    "alpha_post = 1 + k\n",
    "beta_post = 1 + n - k\n",
    "\n",
    "# TODO: compute the 95% credible interval using stats.beta.ppf()\n",
    "# (the percent point function / inverse CDF)\n",
    "bayes_ci_lower = ...\n",
    "bayes_ci_upper = ...\n",
    "posterior_mean = ...\n",
    "\n",
    "print('--- Bayesian 95% Credible Interval (flat prior) ---')\n",
    "print(f'Posterior mean: {posterior_mean:.4f}')\n",
    "print(f'95% credible interval: [{bayes_ci_lower:.4f}, {bayes_ci_upper:.4f}]')\n",
    "print(f'Interpretation: Given the data and our prior, there is a 95%')\n",
    "print(f'probability that theta lies in this interval.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-intervals-visual",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Visualize both intervals on the same plot.\n",
    "\n",
    "theta = np.linspace(0, 1, 500)\n",
    "posterior_pdf = stats.beta.pdf(theta, alpha_post, beta_post)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 5))\n",
    "\n",
    "# Plot the posterior\n",
    "ax.plot(theta, posterior_pdf, 'b-', linewidth=2, label='Posterior')\n",
    "ax.fill_between(theta, posterior_pdf,\n",
    "                where=(theta >= bayes_ci_lower) & (theta <= bayes_ci_upper),\n",
    "                alpha=0.2, color='blue', label='95% Bayesian credible interval')\n",
    "\n",
    "# Overlay the frequentist CI as horizontal bar\n",
    "y_bar = posterior_pdf.max() * 1.05\n",
    "ax.plot([freq_ci_lower, freq_ci_upper], [y_bar, y_bar], 'r-', linewidth=3,\n",
    "        label='95% Frequentist CI')\n",
    "ax.plot(p_hat, y_bar, 'ro', markersize=8)\n",
    "\n",
    "ax.set_xlabel(r'$\\theta$', fontsize=12)\n",
    "ax.set_ylabel('Density', fontsize=12)\n",
    "ax.set_title('Frequentist CI vs Bayesian Credible Interval', fontsize=13)\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-intervals-interp",
   "metadata": {},
   "source": [
    "**Interpretation prompt** (write 2\u20134 sentences below):\n",
    "- Are the two intervals similar numerically? Why or why not?\n",
    "- How do their *interpretations* differ, even if the numbers are close?\n",
    "- Which interpretation do you find more intuitive? Which is technically correct\n",
    "  for the standard econometrics tools you have learned so far?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-prior-matters-header",
   "metadata": {},
   "source": [
    "<a id=\"when-does-the-prior-matter\"></a>\n",
    "## When Does the Prior Matter?\n",
    "\n",
    "### Goal\n",
    "Demonstrate that with enough data, the prior gets \"washed out\" and Bayesian\n",
    "and frequentist answers converge.\n",
    "\n",
    "### Why this matters\n",
    "A common objection to Bayesian methods is: \"But the prior is subjective!\"\n",
    "The reassuring answer: with enough data, the prior barely matters. Different\n",
    "people starting with different beliefs will converge to the same posterior.\n",
    "This is a deep result called **Bayesian consistency** (or \"the data overwhelm\n",
    "the prior\").\n",
    "\n",
    "But with *little* data, the prior matters a *lot*. This is both a feature\n",
    "(you can incorporate expert knowledge) and a bug (you can bias results).\n",
    "\n",
    "---\n",
    "\n",
    "### The experiment\n",
    "\n",
    "Three analysts have different priors about a recession indicator's accuracy $\\theta$:\n",
    "- **Analyst A (optimist):** Beta(9, 1) \u2014 strongly believes the model is great ($\\theta \\approx 0.9$).\n",
    "- **Analyst B (skeptic):** Beta(1, 9) \u2014 strongly believes the model is bad ($\\theta \\approx 0.1$).\n",
    "- **Analyst C (agnostic):** Beta(1, 1) \u2014 flat prior, no opinion.\n",
    "\n",
    "They all observe the same data. How quickly do their posteriors converge?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-prior-matters-turn",
   "metadata": {},
   "source": [
    "### Your Turn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-prior-matters-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Simulate data and show posterior convergence across different priors.\n",
    "#\n",
    "# True theta = 0.75. Generate data from Binomial(n, 0.75).\n",
    "# Show posteriors for n = 5, 20, 50, 500.\n",
    "\n",
    "np.random.seed(123)\n",
    "true_theta = 0.75\n",
    "\n",
    "# Three different priors\n",
    "priors = {\n",
    "    'Optimist: Beta(9,1)': (9, 1),\n",
    "    'Skeptic: Beta(1,9)':  (1, 9),\n",
    "    'Agnostic: Beta(1,1)': (1, 1),\n",
    "}\n",
    "\n",
    "sample_sizes = [5, 20, 50, 500]\n",
    "theta = np.linspace(0, 1, 500)\n",
    "\n",
    "# Generate a large dataset, then take subsets\n",
    "all_data = np.random.binomial(1, true_theta, size=500)\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(18, 4))\n",
    "\n",
    "for j, n_obs in enumerate(sample_sizes):\n",
    "    data_subset = all_data[:n_obs]\n",
    "    k = data_subset.sum()\n",
    "\n",
    "    for label, (a0, b0) in priors.items():\n",
    "        # TODO: compute posterior parameters and plot\n",
    "        a_post = ...\n",
    "        b_post = ...\n",
    "        pdf = ...\n",
    "        axes[j].plot(theta, pdf, linewidth=2, label=label)\n",
    "\n",
    "    axes[j].axvline(x=true_theta, color='black', linestyle='--', alpha=0.5,\n",
    "                    label=f'True \\u03b8 = {true_theta}')\n",
    "    axes[j].set_title(f'n = {n_obs} (k = {k})', fontsize=11)\n",
    "    axes[j].set_xlabel(r'$\\theta$')\n",
    "    if j == 0:\n",
    "        axes[j].set_ylabel('Density')\n",
    "    axes[j].legend(fontsize=7)\n",
    "\n",
    "fig.suptitle('Prior Influence Vanishes as Data Accumulates', fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-prior-matters-quantify",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Quantify the convergence.\n",
    "# Compute the posterior mean for each prior at each sample size.\n",
    "# Show that the gap between posterior means shrinks with n.\n",
    "\n",
    "print(f'{\"n\":>6s}  {\"Optimist\":>10s}  {\"Skeptic\":>10s}  {\"Agnostic\":>10s}  {\"Max Gap\":>10s}')\n",
    "print('-' * 55)\n",
    "\n",
    "for n_obs in [5, 10, 20, 50, 100, 200, 500]:\n",
    "    data_subset = all_data[:n_obs]\n",
    "    k = data_subset.sum()\n",
    "\n",
    "    means = []\n",
    "    for label, (a0, b0) in priors.items():\n",
    "        # TODO: compute posterior mean = (a0 + k) / (a0 + b0 + n_obs)\n",
    "        post_mean = ...\n",
    "        means.append(post_mean)\n",
    "\n",
    "    gap = max(means) - min(means)\n",
    "    print(f'{n_obs:>6d}  {means[0]:>10.4f}  {means[1]:>10.4f}  {means[2]:>10.4f}  {gap:>10.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-prior-matters-interp",
   "metadata": {},
   "source": [
    "**Interpretation prompt** (write 2\u20134 sentences below):\n",
    "- At n=5, how different are the three posteriors? What does this mean practically?\n",
    "- At n=500, do the priors still matter?\n",
    "- In what situation would you *want* the prior to have strong influence?\n",
    "  (Hint: think about small samples and expert knowledge.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-practical-header",
   "metadata": {},
   "source": [
    "<a id=\"practical-implications-for-this-project\"></a>\n",
    "## Practical Implications for This Project\n",
    "\n",
    "### Goal\n",
    "Connect the Bayesian/frequentist distinction to the econometric tools you will\n",
    "use throughout the rest of this project.\n",
    "\n",
    "### Why this matters\n",
    "You now have the conceptual framework to understand *why* certain techniques work\n",
    "the way they do, even though we will primarily use frequentist implementations.\n",
    "\n",
    "---\n",
    "\n",
    "### This project uses frequentist methods\n",
    "\n",
    "Throughout the rest of this project, you will use:\n",
    "- **OLS regression** \u2014 finds coefficients by minimizing squared residuals (a\n",
    "  frequentist optimization).\n",
    "- **p-values** \u2014 \"probability of data this extreme or more, if the null hypothesis\n",
    "  is true\" (a purely frequentist concept).\n",
    "- **Confidence intervals** \u2014 coverage guarantees over repeated sampling.\n",
    "- **Hypothesis tests** \u2014 reject/fail to reject at a significance level.\n",
    "\n",
    "These are the standard tools of econometrics, and they work well for the\n",
    "problems in this project.\n",
    "\n",
    "### But Bayesian thinking enriches your work\n",
    "\n",
    "Even within a frequentist workflow, Bayesian intuition helps:\n",
    "\n",
    "1. **Interpreting results more carefully:**\n",
    "   A significant p-value does *not* mean \"the effect is real with 95%\n",
    "   probability.\" Bayesian thinking reminds you that the posterior probability\n",
    "   of an effect depends on the prior (base rate) as well.\n",
    "\n",
    "2. **Understanding regularization:**\n",
    "   Ridge regression (which you will encounter in `02_regression/05`) adds a\n",
    "   penalty $\\lambda \\|\\beta\\|^2$ to the OLS objective. This is *exactly*\n",
    "   equivalent to Bayesian MAP (Maximum A Posteriori) estimation with a\n",
    "   Gaussian prior on the coefficients: $\\beta \\sim N(0, \\sigma^2/\\lambda)$.\n",
    "   The penalty \"shrinks\" coefficients toward zero \u2014 just as a prior centered\n",
    "   at zero would.\n",
    "\n",
    "   Similarly, LASSO (L1 penalty) corresponds to a Laplace prior.\n",
    "\n",
    "3. **Thinking about model uncertainty:**\n",
    "   Frequentist tools give you uncertainty about parameters *within* a model.\n",
    "   Bayesian thinking encourages you to also think about uncertainty *across*\n",
    "   models (Bayesian model averaging). This perspective is valuable even if\n",
    "   you implement it informally.\n",
    "\n",
    "4. **Understanding stationarity priors:**\n",
    "   In time series econometrics (`07_time_series_econ`), unit root tests check\n",
    "   whether a series is stationary. The Bayesian perspective is: what is your\n",
    "   prior belief about stationarity? This matters because unit root tests have\n",
    "   low power \u2014 they often fail to reject the null of a unit root even when\n",
    "   the series is stationary.\n",
    "\n",
    "### Summary\n",
    "\n",
    "| You will use (frequentist) | Bayesian counterpart | Connection |\n",
    "|---|---|---|\n",
    "| OLS | Bayesian regression | OLS = Bayesian posterior mode with flat prior |\n",
    "| Ridge regression | MAP with Gaussian prior | Penalty = prior precision |\n",
    "| p-value | Posterior probability | Related but NOT the same |\n",
    "| Confidence interval | Credible interval | Same numbers, different meaning |\n",
    "| AIC/BIC model selection | Bayesian model averaging | BIC approximates Bayes factor |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-practical-interp",
   "metadata": {},
   "source": [
    "**Interpretation prompt** (write 2\u20134 sentences below):\n",
    "- In your own words, why does understanding ridge regression as Bayesian help?\n",
    "- Have you ever heard someone say \"there is a 95% probability the true value is in\n",
    "  this confidence interval\"? Is that statement technically correct for a frequentist CI?\n",
    "- When might you actually want to use Bayesian methods instead of frequentist ones\n",
    "  in an economics context?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-later-header",
   "metadata": {},
   "source": [
    "## Where This Shows Up Later\n",
    "\n",
    "- **Ridge regression is Bayesian MAP estimation with a Gaussian prior**\n",
    "  (`02_regression/05`). You will see that the penalty parameter lambda\n",
    "  controls the \"strength\" of the prior belief that coefficients are near zero.\n",
    "- **Prior beliefs about stationarity inform unit root testing**\n",
    "  (`07_time_series_econ`). Whether you believe a series has a unit root\n",
    "  before testing affects how you interpret test results with low power.\n",
    "- **Model uncertainty quantification in the capstone.** The capstone project\n",
    "  requires you to think about uncertainty at multiple levels \u2014 a Bayesian habit\n",
    "  of mind, even if the implementation is frequentist."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-checkpoint-header",
   "metadata": {},
   "source": [
    "<a id=\"checkpoint-self-check\"></a>\n",
    "## Checkpoint (Self-Check)\n",
    "Run these asserts to verify your work. If any fail, go back and fix the corresponding section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-checkpoint",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Bayes' theorem checks ----\n",
    "assert isinstance(p_flag, float), 'p_flag should be a float'\n",
    "assert 0 < p_flag < 1, f'p_flag={p_flag} should be a probability'\n",
    "assert isinstance(p_recession_given_flag, float), 'p_recession_given_flag should be a float'\n",
    "assert 0 < p_recession_given_flag < 1, 'Posterior should be a probability'\n",
    "# The answer should be around 0.585\n",
    "assert abs(p_recession_given_flag - 0.585) < 0.01, (\n",
    "    f'Expected ~0.585, got {p_recession_given_flag:.4f}. Check your Bayes calculation.'\n",
    ")\n",
    "\n",
    "# ---- Interval checks ----\n",
    "assert freq_ci_lower < freq_ci_upper, 'CI lower bound should be < upper bound'\n",
    "assert bayes_ci_lower < bayes_ci_upper, 'Credible interval should be ordered'\n",
    "assert 0 < freq_ci_lower < 1, 'Frequentist CI should be in (0, 1)'\n",
    "assert 0 < bayes_ci_lower < 1, 'Bayesian CI should be in (0, 1)'\n",
    "\n",
    "# ---- Posterior parameter checks ----\n",
    "assert alpha_post == 1 + 16, f'alpha_post should be 17, got {alpha_post}'\n",
    "assert beta_post == 1 + 4, f'beta_post should be 5, got {beta_post}'\n",
    "\n",
    "print('All checkpoint assertions passed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-extensions",
   "metadata": {},
   "source": [
    "## Extensions (Optional)\n",
    "- **Bayesian A/B testing:** Suppose two economic policies are tested in different regions.\n",
    "  Region A shows 60/100 positive outcomes, Region B shows 55/100. Use the Beta-Binomial\n",
    "  model to compute P(policy A is better than policy B) by simulation.\n",
    "- **Conjugate priors for the normal mean:** If you observe data from a normal distribution\n",
    "  with known variance, the conjugate prior for the mean is also normal. Work through\n",
    "  the math and plot the prior-to-posterior update.\n",
    "- **Explore `PyMC` or `ArviZ`:** These Python libraries implement full Bayesian inference.\n",
    "  Try fitting a simple linear regression with `PyMC` and compare the posterior\n",
    "  distribution of coefficients to the OLS point estimates.\n",
    "- **The Jeffreys-Lindley paradox:** With a large sample size, a Bayesian test and a\n",
    "  frequentist test can give opposite conclusions. Research this paradox and think about\n",
    "  what it means for applied work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-reflection",
   "metadata": {},
   "source": [
    "## Reflection\n",
    "- Before this notebook, which \"camp\" did you lean toward \u2014 frequentist or Bayesian?\n",
    "  Has your view changed?\n",
    "- Think of a specific economic question you care about. Would you rather have a\n",
    "  confidence interval or a credible interval for the answer? Why?\n",
    "- How does the base-rate fallacy from the Bayes' theorem section connect to the\n",
    "  replication crisis in social science research?\n",
    "- In what ways does Bayesian thinking make you a more careful consumer of\n",
    "  frequentist results?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-solutions",
   "metadata": {},
   "source": [
    "<a id=\"solutions-reference\"></a>\n",
    "## Solutions (Reference)\n",
    "\n",
    "Try the TODOs first. Use these only to unblock yourself or to compare approaches.\n",
    "\n",
    "<details><summary>Solution: Bayes' theorem (recession indicator)</summary>\n",
    "\n",
    "_One possible approach. Your variable names may differ; align them with the notebook._\n",
    "\n",
    "```python\n",
    "# Reference solution for 08 \u2014 Bayes' theorem\n",
    "sensitivity = 0.80\n",
    "false_positive_rate = 0.10\n",
    "base_rate = 0.15\n",
    "\n",
    "# P(flag) via law of total probability\n",
    "p_flag = sensitivity * base_rate + false_positive_rate * (1 - base_rate)\n",
    "\n",
    "# Bayes' theorem\n",
    "p_recession_given_flag = sensitivity * base_rate / p_flag\n",
    "\n",
    "print(f'P(flag)             = {p_flag:.4f}')       # 0.2050\n",
    "print(f'P(recession | flag) = {p_recession_given_flag:.4f}')  # ~0.5854\n",
    "\n",
    "# Base rate exploration\n",
    "base_rates = np.linspace(0.01, 0.50, 100)\n",
    "posteriors = (sensitivity * base_rates) / (\n",
    "    sensitivity * base_rates + false_positive_rate * (1 - base_rates)\n",
    ")\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<details><summary>Solution: Prior, likelihood, and posterior (Bayesian updating)</summary>\n",
    "\n",
    "_One possible approach. Your variable names may differ; align them with the notebook._\n",
    "\n",
    "```python\n",
    "# Reference solution for 08 \u2014 Bayesian updating\n",
    "observations = [1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1]\n",
    "alpha_prior, beta_prior = 1, 1\n",
    "theta = np.linspace(0, 1, 500)\n",
    "snapshots = [0, 1, 3, 5, 10, 20]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(14, 8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, n_obs in enumerate(snapshots):\n",
    "    data_so_far = observations[:n_obs]\n",
    "    k = sum(data_so_far)\n",
    "    n = len(data_so_far)\n",
    "\n",
    "    alpha_post = alpha_prior + k\n",
    "    beta_post = beta_prior + n - k\n",
    "    posterior_pdf = stats.beta.pdf(theta, alpha_post, beta_post)\n",
    "\n",
    "    axes[i].plot(theta, posterior_pdf, 'b-', linewidth=2)\n",
    "    axes[i].fill_between(theta, posterior_pdf, alpha=0.2)\n",
    "    axes[i].axvline(x=0.80, color='red', linestyle='--', alpha=0.6,\n",
    "                    label='True rate (0.80)')\n",
    "    axes[i].set_title(f'After {n_obs} obs (k={k})', fontsize=11)\n",
    "    axes[i].set_xlabel(r'$\\theta$')\n",
    "    axes[i].set_ylabel('Density')\n",
    "    axes[i].legend(fontsize=8)\n",
    "\n",
    "plt.suptitle('Bayesian Updating', fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Prior-Likelihood-Posterior plot\n",
    "k_total = sum(observations)\n",
    "n_total = len(observations)\n",
    "\n",
    "prior_pdf = stats.beta.pdf(theta, alpha_prior, beta_prior)\n",
    "likelihood = stats.binom.pmf(k_total, n_total, theta)\n",
    "likelihood_scaled = likelihood / likelihood.max() * stats.beta.pdf(\n",
    "    theta, alpha_prior + k_total, beta_prior + n_total - k_total).max()\n",
    "posterior_pdf = stats.beta.pdf(theta, alpha_prior + k_total,\n",
    "                               beta_prior + n_total - k_total)\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<details><summary>Solution: Bayesian vs frequentist intervals</summary>\n",
    "\n",
    "_One possible approach. Your variable names may differ; align them with the notebook._\n",
    "\n",
    "```python\n",
    "# Reference solution for 08 \u2014 Intervals\n",
    "k, n = 16, 20\n",
    "p_hat = k / n  # 0.80\n",
    "\n",
    "# Frequentist Wald CI\n",
    "z = stats.norm.ppf(0.975)\n",
    "se = np.sqrt(p_hat * (1 - p_hat) / n)\n",
    "freq_ci_lower = p_hat - z * se\n",
    "freq_ci_upper = p_hat + z * se\n",
    "\n",
    "# Bayesian credible interval (flat prior)\n",
    "alpha_post = 1 + k   # 17\n",
    "beta_post = 1 + n - k  # 5\n",
    "bayes_ci_lower = stats.beta.ppf(0.025, alpha_post, beta_post)\n",
    "bayes_ci_upper = stats.beta.ppf(0.975, alpha_post, beta_post)\n",
    "posterior_mean = alpha_post / (alpha_post + beta_post)\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<details><summary>Solution: When does the prior matter?</summary>\n",
    "\n",
    "_One possible approach. Your variable names may differ; align them with the notebook._\n",
    "\n",
    "```python\n",
    "# Reference solution for 08 \u2014 Prior convergence\n",
    "np.random.seed(123)\n",
    "true_theta = 0.75\n",
    "all_data = np.random.binomial(1, true_theta, size=500)\n",
    "\n",
    "priors = {\n",
    "    'Optimist: Beta(9,1)': (9, 1),\n",
    "    'Skeptic: Beta(1,9)':  (1, 9),\n",
    "    'Agnostic: Beta(1,1)': (1, 1),\n",
    "}\n",
    "\n",
    "sample_sizes = [5, 20, 50, 500]\n",
    "theta = np.linspace(0, 1, 500)\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(18, 4))\n",
    "for j, n_obs in enumerate(sample_sizes):\n",
    "    data_subset = all_data[:n_obs]\n",
    "    k = data_subset.sum()\n",
    "    for label, (a0, b0) in priors.items():\n",
    "        a_post = a0 + k\n",
    "        b_post = b0 + n_obs - k\n",
    "        pdf = stats.beta.pdf(theta, a_post, b_post)\n",
    "        axes[j].plot(theta, pdf, linewidth=2, label=label)\n",
    "    axes[j].axvline(x=true_theta, color='black', linestyle='--', alpha=0.5)\n",
    "    axes[j].set_title(f'n = {n_obs} (k = {k})')\n",
    "    axes[j].legend(fontsize=7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Quantify convergence\n",
    "for n_obs in [5, 10, 20, 50, 100, 200, 500]:\n",
    "    k = all_data[:n_obs].sum()\n",
    "    means = [(a0 + k) / (a0 + b0 + n_obs) for (a0, b0) in priors.values()]\n",
    "    gap = max(means) - min(means)\n",
    "    print(f'n={n_obs:>4d}  gap={gap:.4f}')\n",
    "```\n",
    "\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 5,
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}