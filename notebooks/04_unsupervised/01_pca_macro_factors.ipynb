{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 01 PCA Macro Factors\n",
        "\n",
        "Extract factors and interpret loadings.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Table of Contents\n",
        "- [Standardize](#standardize)\n",
        "- [Fit PCA](#fit-pca)\n",
        "- [Interpret loadings](#interpret-loadings)\n",
        "- [Checkpoint (Self-Check)](#checkpoint-self-check)\n",
        "- [Solutions (Reference)](#solutions-reference)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Why This Notebook Matters\n",
        "Unsupervised notebooks help you understand macro structure:\n",
        "- latent factors (PCA),\n",
        "- regimes (clustering),\n",
        "- anomalies/crises.\n",
        "\n",
        "\n",
        "## Prerequisites (Quick Self-Check)\n",
        "- Completed Part 01 (macro panel) or equivalent.\n",
        "- Comfort with standardization and basic linear algebra intuition (variance, distance).\n",
        "\n",
        "## What You Will Produce\n",
        "- (no file output; learning/analysis notebook)\n",
        "\n",
        "## Success Criteria\n",
        "- You can explain what you built and why each step exists.\n",
        "- You can run your work end-to-end without undefined variables.\n",
        "\n",
        "## Common Pitfalls\n",
        "- Running cells top-to-bottom without reading the instructions.\n",
        "- Leaving `...` placeholders in code cells.\n",
        "- Forgetting to standardize features before PCA/clustering.\n",
        "- Over-interpreting clusters as causal regimes.\n",
        "\n",
        "## Quick Fixes (When You Get Stuck)\n",
        "- If you see `ModuleNotFoundError`, re-run the bootstrap cell and restart the kernel; make sure `PROJECT_ROOT` is the repo root.\n",
        "- If a `data/processed/*` file is missing, either run the matching build script (see guide) or use the notebook\u2019s `data/sample/*` fallback.\n",
        "- If results look \u201ctoo good,\u201d suspect leakage; re-check shifts, rolling windows, and time splits.\n",
        "- If a model errors, check dtypes (`astype(float)`) and missingness (`dropna()` on required columns).\n",
        "\n",
        "## Matching Guide\n",
        "- `docs/guides/04_unsupervised/01_pca_macro_factors.md`\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## How To Use This Notebook\n",
        "- Work section-by-section; don\u2019t skip the markdown.\n",
        "- Most code cells are incomplete on purpose: replace TODOs and `...`, then run.\n",
        "- After each section, write 2\u20134 sentences answering the interpretation prompts (what changed, why it matters).\n",
        "- Prefer `data/processed/*` if you have built the real datasets; otherwise use the bundled `data/sample/*` fallbacks.\n",
        "- Use the **Checkpoint (Self-Check)** section to catch mistakes early.\n",
        "- Use **Solutions (Reference)** only to unblock yourself; then re-implement without looking.\n",
        "- Use the matching guide (`docs/guides/04_unsupervised/01_pca_macro_factors.md`) for the math, assumptions, and deeper context.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"environment-bootstrap\"></a>\n",
        "## Environment Bootstrap\n",
        "Run this cell first. It makes the repo importable and defines common directories.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "\n",
        "def find_repo_root(start: Path) -> Path:\n",
        "    p = start\n",
        "    for _ in range(8):\n",
        "        if (p / 'src').exists() and (p / 'docs').exists():\n",
        "            return p\n",
        "        p = p.parent\n",
        "    raise RuntimeError('Could not find repo root. Start Jupyter from the repo root.')\n",
        "\n",
        "\n",
        "PROJECT_ROOT = find_repo_root(Path.cwd())\n",
        "if str(PROJECT_ROOT) not in sys.path:\n",
        "    sys.path.append(str(PROJECT_ROOT))\n",
        "\n",
        "DATA_DIR = PROJECT_ROOT / 'data'\n",
        "RAW_DIR = DATA_DIR / 'raw'\n",
        "PROCESSED_DIR = DATA_DIR / 'processed'\n",
        "SAMPLE_DIR = DATA_DIR / 'sample'\n",
        "\n",
        "PROJECT_ROOT\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Goal\n",
        "Use PCA to extract a small number of macro factors from many indicators.\n",
        "\n",
        "Why PCA is useful here:\n",
        "- macro series are correlated\n",
        "- PCA creates orthogonal (uncorrelated) components\n",
        "- components can act like \"macro factors\" (growth, inflation, rates, etc.)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Primer: pandas time series essentials (indexing, resampling, lags)\n",
        "\n",
        "Most \u201cmysterious bugs\u201d in time series work come from index and alignment mistakes. This primer gives you the minimum patterns to avoid them.\n",
        "\n",
        "### 1) DatetimeIndex (the first thing to verify)\n",
        "\n",
        "Most time-series operations assume a `DatetimeIndex`:\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "df = df.copy()\n",
        "df.index = pd.to_datetime(df.index)\n",
        "df = df.sort_index()\n",
        "assert isinstance(df.index, pd.DatetimeIndex)\n",
        "```\n",
        "\n",
        "**Expected output / sanity checks**\n",
        "- `df.index.min(), df.index.max()` look reasonable\n",
        "- `df.index.is_monotonic_increasing` is `True`\n",
        "\n",
        "### 2) Resampling (frequency alignment)\n",
        "\n",
        "Resampling converts one frequency to another. Choose the aggregation rule intentionally.\n",
        "\n",
        "```python\n",
        "# month-end last value (end-of-period)\n",
        "df_me_last = df.resample(\"ME\").last()\n",
        "\n",
        "# month-end mean (average-of-period)\n",
        "df_me_mean = df.resample(\"ME\").mean()\n",
        "\n",
        "# quarter-end mean\n",
        "df_q_mean = df.resample(\"QE\").mean()\n",
        "```\n",
        "\n",
        "**Interpretation matters**\n",
        "- `.last()` treats end-of-period value as \u201cthe period\u2019s value.\u201d\n",
        "- `.mean()` treats the period average as \u201cthe period\u2019s value.\u201d\n",
        "\n",
        "### 3) Alignment and merging\n",
        "\n",
        "When joining series, always check missingness after the join:\n",
        "\n",
        "```python\n",
        "merged = df1.join(df2, how=\"outer\").sort_index()\n",
        "print(merged.isna().sum().sort_values(ascending=False).head(10))\n",
        "```\n",
        "\n",
        "### 4) Lags and rolling windows (watch for leakage!)\n",
        "\n",
        "```python\n",
        "# lag 1 period (past-only)\n",
        "df[\"x_lag1\"] = df[\"x\"].shift(1)\n",
        "\n",
        "# rolling mean using past values ending at t\n",
        "df[\"x_roll12\"] = df[\"x\"].rolling(12).mean()\n",
        "```\n",
        "\n",
        "**Leakage pitfalls**\n",
        "- `shift(-1)` uses the future.\n",
        "- `rolling(..., center=True)` uses the future.\n",
        "\n",
        "### 5) A quick workflow you should repeat\n",
        "\n",
        "1) Set and verify DatetimeIndex.\n",
        "2) Resample intentionally (mean vs last).\n",
        "3) Join and inspect missingness.\n",
        "4) Add lags/rolls (past-only).\n",
        "5) `dropna()` to build a clean modeling table.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"standardize\"></a>\n",
        "## Standardize\n",
        "\n",
        "### Goal\n",
        "Load a monthly panel and standardize features (mean 0, std 1).\n",
        "\n",
        "PCA is sensitive to scale: if one variable has larger units, it can dominate the components.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Your Turn (1): Load panel_monthly.csv (or sample)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "path = PROCESSED_DIR / 'panel_monthly.csv'\n",
        "if path.exists():\n",
        "    df = pd.read_csv(path, index_col=0, parse_dates=True)\n",
        "else:\n",
        "    df = pd.read_csv(SAMPLE_DIR / 'panel_monthly_sample.csv', index_col=0, parse_dates=True)\n",
        "\n",
        "df.head()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Your Turn (2): Build X and standardize\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "x_cols = df.columns.tolist()\n",
        "X = df[x_cols].dropna().copy()\n",
        "\n",
        "sc = StandardScaler().fit(X)\n",
        "X_s = sc.transform(X)\n",
        "\n",
        "# TODO: Validate standardization\n",
        "...\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"fit-pca\"></a>\n",
        "## Fit PCA\n",
        "\n",
        "### Goal\n",
        "Fit PCA and inspect explained variance.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Your Turn (1): Fit PCA\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=3).fit(X_s)\n",
        "\n",
        "evr = pd.Series(pca.explained_variance_ratio_, index=[f'PC{i+1}' for i in range(pca.n_components_)])\n",
        "evr\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Your Turn (2): Scree plot\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# TODO: Plot explained variance ratio by component.\n",
        "...\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"interpret-loadings\"></a>\n",
        "## Interpret loadings\n",
        "\n",
        "### Goal\n",
        "Interpret what each component represents in economic terms.\n",
        "\n",
        "Loadings tell you which original variables contribute most to each component.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Your Turn (1): Build a loadings table\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "loadings = pd.DataFrame(\n",
        "    pca.components_.T,\n",
        "    index=x_cols,\n",
        "    columns=[f'PC{i+1}' for i in range(pca.n_components_)],\n",
        ")\n",
        "\n",
        "# TODO: For each PC, list the top + and top - loadings.\n",
        "...\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Your Turn (2): Name the components\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# TODO: Write a name/interpretation for PC1 and PC2.\n",
        "notes = \"\"\"\n",
        "PC1: ...\n",
        "PC2: ...\n",
        "\"\"\"\n",
        "print(notes)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"checkpoint-self-check\"></a>\n",
        "## Checkpoint (Self-Check)\n",
        "Run a few asserts and write 2-3 sentences summarizing what you verified.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# TODO: Confirm your feature matrix is standardized (or justify why not).\n",
        "# Example:\n",
        "# assert abs(X_scaled.mean(axis=0)).max() < 1e-6\n",
        "# assert abs(X_scaled.std(axis=0) - 1).max() < 1e-6\n",
        "...\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extensions (Optional)\n",
        "- Try one additional variant beyond the main path (different features, different split, different model).\n",
        "- Write down what improved, what got worse, and your hypothesis for why.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reflection\n",
        "- What did you assume implicitly (about timing, availability, stationarity, or costs)?\n",
        "- If you had to ship this model, what would you monitor?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"solutions-reference\"></a>\n",
        "## Solutions (Reference)\n",
        "\n",
        "Try the TODOs first. Use these only to unblock yourself or to compare approaches.\n",
        "\n",
        "<details><summary>Solution: Standardize</summary>\n",
        "\n",
        "_One possible approach. Your variable names may differ; align them with the notebook._\n",
        "\n",
        "```python\n",
        "# Reference solution for 01_pca_macro_factors \u2014 Standardize\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "panel = pd.read_csv(SAMPLE_DIR / 'panel_monthly_sample.csv', index_col=0, parse_dates=True).dropna()\n",
        "X = StandardScaler().fit_transform(panel)\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "<details><summary>Solution: Fit PCA</summary>\n",
        "\n",
        "_One possible approach. Your variable names may differ; align them with the notebook._\n",
        "\n",
        "```python\n",
        "# Reference solution for 01_pca_macro_factors \u2014 Fit PCA\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=3).fit(X)\n",
        "pca.explained_variance_ratio_\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "<details><summary>Solution: Interpret loadings</summary>\n",
        "\n",
        "_One possible approach. Your variable names may differ; align them with the notebook._\n",
        "\n",
        "```python\n",
        "# Reference solution for 01_pca_macro_factors \u2014 Interpret loadings\n",
        "import pandas as pd\n",
        "loadings = pd.DataFrame(pca.components_.T, index=panel.columns, columns=[f'PC{i+1}' for i in range(pca.n_components_)])\n",
        "loadings.sort_values('PC1', key=abs, ascending=False).head(10)\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "python3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}