{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 03 Instrumental Variables (2SLS)\n",
        "\n",
        "Endogeneity, instruments, and two-stage least squares (2SLS).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Table of Contents\n",
        "- [Simulate endogeneity](#simulate-endogeneity)\n",
        "- [OLS vs 2SLS](#ols-vs-2sls)\n",
        "- [First-stage + weak IV checks](#first-stage-weak-iv-checks)\n",
        "- [Interpretation + limitations](#interpretation-limitations)\n",
        "- [Checkpoint (Self-Check)](#checkpoint-self-check)\n",
        "- [Solutions (Reference)](#solutions-reference)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Why This Notebook Matters\n",
        "Causal notebooks focus on **identification**: what would have to be true for a coefficient to represent a causal effect.\n",
        "You will practice:\n",
        "- building a county-year panel,\n",
        "- fixed effects (TWFE),\n",
        "- clustered standard errors,\n",
        "- DiD + event studies,\n",
        "- IV/2SLS.\n",
        "\n",
        "\n",
        "## Prerequisites (Quick Self-Check)\n",
        "- Completed Part 02 (regression + robust SE).\n",
        "- Basic familiarity with panels (same unit over time) and the idea of identification assumptions.\n",
        "\n",
        "## What You Will Produce\n",
        "- (no file output; learning/analysis notebook)\n",
        "\n",
        "## Success Criteria\n",
        "- You can explain what you built and why each step exists.\n",
        "- You can run your work end-to-end without undefined variables.\n",
        "\n",
        "## Common Pitfalls\n",
        "- Running cells top-to-bottom without reading the instructions.\n",
        "- Leaving `...` placeholders in code cells.\n",
        "- Treating regression output as causal without stating identification assumptions.\n",
        "- Using non-clustered SE when shocks are correlated within groups (e.g., states).\n",
        "\n",
        "## Quick Fixes (When You Get Stuck)\n",
        "- If you see `ModuleNotFoundError`, re-run the bootstrap cell and restart the kernel; make sure `PROJECT_ROOT` is the repo root.\n",
        "- If a `data/processed/*` file is missing, either run the matching build script (see guide) or use the notebook\u2019s `data/sample/*` fallback.\n",
        "- If results look \u201ctoo good,\u201d suspect leakage; re-check shifts, rolling windows, and time splits.\n",
        "- If a model errors, check dtypes (`astype(float)`) and missingness (`dropna()` on required columns).\n",
        "\n",
        "## Matching Guide\n",
        "- `docs/guides/06_causal/03_instrumental_variables_2sls.md`\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## How To Use This Notebook\n",
        "- Work section-by-section; don\u2019t skip the markdown.\n",
        "- Most code cells are incomplete on purpose: replace TODOs and `...`, then run.\n",
        "- After each section, write 2\u20134 sentences answering the interpretation prompts (what changed, why it matters).\n",
        "- Prefer `data/processed/*` if you have built the real datasets; otherwise use the bundled `data/sample/*` fallbacks.\n",
        "- Use the **Checkpoint (Self-Check)** section to catch mistakes early.\n",
        "- Use **Solutions (Reference)** only to unblock yourself; then re-implement without looking.\n",
        "- Use the matching guide (`docs/guides/06_causal/03_instrumental_variables_2sls.md`) for the math, assumptions, and deeper context.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"environment-bootstrap\"></a>\n",
        "## Environment Bootstrap\n",
        "Run this cell first. It makes the repo importable and defines common directories.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "\n",
        "def find_repo_root(start: Path) -> Path:\n",
        "    p = start\n",
        "    for _ in range(8):\n",
        "        if (p / 'src').exists() and (p / 'docs').exists():\n",
        "            return p\n",
        "        p = p.parent\n",
        "    raise RuntimeError('Could not find repo root. Start Jupyter from the repo root.')\n",
        "\n",
        "\n",
        "PROJECT_ROOT = find_repo_root(Path.cwd())\n",
        "if str(PROJECT_ROOT) not in sys.path:\n",
        "    sys.path.append(str(PROJECT_ROOT))\n",
        "\n",
        "DATA_DIR = PROJECT_ROOT / 'data'\n",
        "RAW_DIR = DATA_DIR / 'raw'\n",
        "PROCESSED_DIR = DATA_DIR / 'processed'\n",
        "SAMPLE_DIR = DATA_DIR / 'sample'\n",
        "\n",
        "PROJECT_ROOT\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Goal\n",
        "Practice IV/2SLS by simulating a classic endogeneity problem.\n",
        "\n",
        "We do this synthetically so you can see the bias and how IV can fix it under assumptions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Primer: Panel + IV regression with `linearmodels` (FE, clustered SE, 2SLS)\n",
        "\n",
        "This repo uses:\n",
        "- `statsmodels` for classic OLS inference patterns, and\n",
        "- `linearmodels` for **panel fixed effects** and **instrumental variables** (IV/2SLS).\n",
        "\n",
        "The goal of this primer is to make you productive quickly (with the *minimum* theory needed to use the tools correctly). Deep math lives in the guides.\n",
        "\n",
        "### Why `linearmodels`?\n",
        "\n",
        "`linearmodels` provides clean APIs for:\n",
        "- `PanelOLS`: fixed effects / TWFE\n",
        "- `IV2SLS`: two-stage least squares\n",
        "\n",
        "and it handles some panel-specific details (like absorbing FE) more naturally than `statsmodels`.\n",
        "\n",
        "### Panel data shape (the #1 requirement)\n",
        "\n",
        "Most panel estimators expect a **MultiIndex**:\n",
        "- level 0: entity (e.g., county `fips`)\n",
        "- level 1: time (e.g., `year`)\n",
        "\n",
        "```python\n",
        "# df has columns: fips, year, y, x1, x2, state, ...\n",
        "df = df.copy()\n",
        "df[\"fips\"] = df[\"fips\"].astype(str)\n",
        "df[\"year\"] = df[\"year\"].astype(int)\n",
        "df = df.set_index([\"fips\", \"year\"]).sort_index()\n",
        "```\n",
        "\n",
        "**Expected output / sanity check**\n",
        "- `df.index.nlevels == 2`\n",
        "- `df.index.is_monotonic_increasing` is `True`\n",
        "- no duplicate index pairs: `df.index.duplicated().any()` is `False`\n",
        "\n",
        "### TWFE model (PanelOLS)\n",
        "\n",
        "Econometric form:\n",
        "\n",
        "$$\n",
        "Y_{it} = X_{it}'\\\\beta + \\\\alpha_i + \\\\gamma_t + \\\\varepsilon_{it}\n",
        "$$\n",
        "\n",
        "In code:\n",
        "\n",
        "```python\n",
        "from linearmodels.panel import PanelOLS\n",
        "import statsmodels.api as sm\n",
        "\n",
        "y = df[\"y\"].astype(float)\n",
        "X = df[[\"x1\", \"x2\"]].astype(float)\n",
        "X = sm.add_constant(X, has_constant=\"add\")\n",
        "\n",
        "res = PanelOLS(y, X, entity_effects=True, time_effects=True).fit(cov_type=\"robust\")\n",
        "print(res.summary)\n",
        "```\n",
        "\n",
        "### Clustered SE (common in applied panel/DiD work)\n",
        "\n",
        "If errors are correlated within clusters (e.g., state-level shocks), use clustered SE:\n",
        "\n",
        "```python\n",
        "clusters = df[\"state\"]  # must align row-for-row with y/X index\n",
        "\n",
        "res_cl = PanelOLS(y, X, entity_effects=True, time_effects=True).fit(\n",
        "  cov_type=\"clustered\",\n",
        "  clusters=clusters,\n",
        ")\n",
        "```\n",
        "\n",
        "**Expected output / sanity check**\n",
        "- clustered SE are often larger than robust SE (not guaranteed, but common)\n",
        "- always report the number of clusters: `clusters.nunique()`\n",
        "\n",
        "### IV / 2SLS (IV2SLS)\n",
        "\n",
        "Structural equation (endogeneity motivation):\n",
        "$$\n",
        "Y = \\\\beta X + W'\\\\delta + u, \\\\quad \\\\mathrm{Cov}(X,u)\\\\neq 0\n",
        "$$\n",
        "\n",
        "In code (one endogenous regressor):\n",
        "\n",
        "```python\n",
        "from linearmodels.iv import IV2SLS\n",
        "import statsmodels.api as sm\n",
        "\n",
        "y = df[\"y\"].astype(float)\n",
        "endog = df[[\"x_endog\"]].astype(float)\n",
        "exog = sm.add_constant(df[[\"x_exog1\", \"x_exog2\"]].astype(float), has_constant=\"add\")\n",
        "instr = df[[\"z1\", \"z2\"]].astype(float)\n",
        "\n",
        "res_iv = IV2SLS(y, exog, endog, instr).fit(cov_type=\"robust\")\n",
        "print(res_iv.summary)\n",
        "```\n",
        "\n",
        "**Expected output / sanity check**\n",
        "- `res_iv.params` contains coefficients for exog + endogenous variables\n",
        "- `res_iv.first_stage` (if printed) shows instrument relevance diagnostics\n",
        "\n",
        "### Common pitfalls (and quick fixes)\n",
        "\n",
        "- **MultiIndex mismatch:** if `clusters` is not aligned to the same index as `y/X`, you\u2019ll get errors or wrong results.\n",
        "  - Fix: construct clusters from the same `df` after indexing/sorting.\n",
        "- **Non-numeric dtypes:** strings in `X` silently break models.\n",
        "  - Fix: `astype(float)` on model columns.\n",
        "- **Missing data:** panels often have missing rows after merges/transforms.\n",
        "  - Fix: build a modeling table with `.dropna()` for required columns.\n",
        "- **Too few clusters:** cluster-robust inference is fragile with very small cluster counts.\n",
        "  - Fix: treat p-values as fragile; report cluster count; consider alternative designs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"simulate-endogeneity\"></a>\n",
        "## Simulate endogeneity\n",
        "\n",
        "### Background\n",
        "Endogeneity means your regressor $x$ is correlated with the error term.\n",
        "That breaks the core OLS condition $E[u\\mid X]=0$ and typically biases OLS.\n",
        "\n",
        "We simulate endogeneity by constructing a hidden confounder $u$ that affects both $x$ and $y$.\n",
        "We then construct an instrument $z$ that shifts $x$ but (by design) does not directly shift $y$.\n",
        "\n",
        "### What you should see\n",
        "- `x` is correlated with the confounder-driven error component.\n",
        "- `z` is correlated with `x` (relevance).\n",
        "- `z` is not directly in the structural equation for `y` (exclusion in this synthetic setup).\n",
        "\n",
        "### Interpretation prompts\n",
        "- In one sentence, explain why OLS is biased here.\n",
        "- Write the relevance and exclusion conditions in words for this simulation.\n",
        "\n",
        "### Goal\n",
        "Create data where:\n",
        "- x is correlated with the error term (endogenous)\n",
        "- z shifts x but not y directly (instrument)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Your Turn: Simulate (y, x, z)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "rng = np.random.default_rng(0)\n",
        "n = 2000\n",
        "\n",
        "# Instrument\n",
        "z = rng.normal(size=n)\n",
        "\n",
        "# Hidden confounder\n",
        "u = rng.normal(size=n)\n",
        "\n",
        "# Endogenous regressor: depends on z and u\n",
        "x = 0.8*z + 0.8*u + rng.normal(size=n)\n",
        "\n",
        "# Error term correlated with u\n",
        "eps = 0.8*u + rng.normal(size=n)\n",
        "\n",
        "beta_true = 1.5\n",
        "y = beta_true * x + eps\n",
        "\n",
        "df = pd.DataFrame({'y': y, 'x': x, 'z': z})\n",
        "df.head()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"ols-vs-2sls\"></a>\n",
        "## OLS vs 2SLS\n",
        "\n",
        "### Background\n",
        "OLS uses all variation in $x$, including the endogenous part correlated with the error.\n",
        "2SLS replaces $x$ with the part predicted by $z$ (instrumented variation).\n",
        "\n",
        "### What you should see\n",
        "- OLS estimate differs from `beta_true` (bias).\n",
        "- IV/2SLS estimate is closer to `beta_true` (in this synthetic world).\n",
        "\n",
        "### Interpretation prompts\n",
        "- Which direction is the OLS bias and why (link it to how you constructed the confounder)?\n",
        "- Why does IV move the estimate toward the truth in this setup?\n",
        "\n",
        "### Goal\n",
        "Compare naive OLS (biased) to IV/2SLS.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Your Turn: Fit OLS and 2SLS\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import statsmodels.api as sm\n",
        "from src.causal import fit_iv_2sls\n",
        "\n",
        "# OLS\n",
        "ols = sm.OLS(df['y'], sm.add_constant(df[['x']], has_constant='add')).fit()\n",
        "print('OLS beta:', float(ols.params['x']))\n",
        "\n",
        "# 2SLS\n",
        "iv = fit_iv_2sls(df, y_col='y', x_endog='x', x_exog=[], z_cols=['z'])\n",
        "print('IV beta :', float(iv.params['x']))\n",
        "\n",
        "iv.summary\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"first-stage-weak-iv-checks\"></a>\n",
        "## First-stage + weak IV checks\n",
        "\n",
        "### Background\n",
        "A valid instrument must be relevant.\n",
        "If $z$ barely predicts $x$, 2SLS can be unstable and misleading (weak instruments).\n",
        "\n",
        "### What you should see\n",
        "- a first-stage relationship where `z` helps explain `x`.\n",
        "- a discussion of instrument strength (even informally).\n",
        "\n",
        "### Interpretation prompts\n",
        "- What would happen to 2SLS if `z` were only weakly related to `x`?\n",
        "- Which parts of IV validity are testable from the data, and which are not?\n",
        "\n",
        "### Goal\n",
        "Inspect the first stage and discuss instrument strength.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Your Turn: Inspect first stage\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# TODO: Explore first-stage outputs.\n",
        "# Hint: `iv.first_stage` is usually informative.\n",
        "iv.first_stage\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"interpretation-limitations\"></a>\n",
        "## Interpretation + limitations\n",
        "\n",
        "Write 5-8 sentences on:\n",
        "- relevance and exclusion in this synthetic setup\n",
        "- what would break IV in real data\n",
        "- why IV identifies a local effect when effects are heterogeneous (LATE intuition)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"checkpoint-self-check\"></a>\n",
        "## Checkpoint (Self-Check)\n",
        "Run a few asserts and write 2-3 sentences summarizing what you verified.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Expected output: (see notebook front matter)\n",
        "# TODO: If you created a panel DataFrame, verify the indexing + core columns.\n",
        "# Example (adjust variable names):\n",
        "# assert isinstance(panel.index, pd.MultiIndex)\n",
        "# assert panel.index.names[:2] == ['fips', 'year']\n",
        "# assert panel['year'].astype(int).between(1900, 2100).all()\n",
        "# assert panel['fips'].astype(str).str.len().eq(5).all()\n",
        "#\n",
        "# TODO: Write 2-3 sentences:\n",
        "# - What is the identification assumption for your causal estimate?\n",
        "# - What diagnostic/falsification did you run?\n",
        "...\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extensions (Optional)\n",
        "- Try one additional variant beyond the main path (different features, different split, different model).\n",
        "- Write down what improved, what got worse, and your hypothesis for why.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reflection\n",
        "- What did you assume implicitly (about timing, availability, stationarity, or costs)?\n",
        "- If you had to ship this model, what would you monitor?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"solutions-reference\"></a>\n",
        "## Solutions (Reference)\n",
        "\n",
        "Try the TODOs first. Use these only to unblock yourself or to compare approaches.\n",
        "\n",
        "<details><summary>Solution: Simulate endogeneity</summary>\n",
        "\n",
        "_One possible approach. Your variable names may differ; align them with the notebook._\n",
        "\n",
        "```python\n",
        "# Reference solution for 03_instrumental_variables_2sls \u2014 Simulate endogeneity\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "rng = np.random.default_rng(0)\n",
        "n = 2000\n",
        "z = rng.normal(size=n)          # instrument\n",
        "u = rng.normal(size=n)          # unobserved confounder\n",
        "\n",
        "x = 0.8*z + 0.8*u + rng.normal(size=n)  # endogenous regressor\n",
        "eps = 0.8*u + rng.normal(size=n)        # error correlated with x\n",
        "\n",
        "beta_true = 1.5\n",
        "y = beta_true * x + eps\n",
        "\n",
        "df = pd.DataFrame({'y': y, 'x': x, 'z': z})\n",
        "df.head()\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "<details><summary>Solution: OLS vs 2SLS</summary>\n",
        "\n",
        "_One possible approach. Your variable names may differ; align them with the notebook._\n",
        "\n",
        "```python\n",
        "# Reference solution for 03_instrumental_variables_2sls \u2014 OLS vs 2SLS\n",
        "import statsmodels.api as sm\n",
        "from src.causal import fit_iv_2sls\n",
        "\n",
        "ols = sm.OLS(df['y'], sm.add_constant(df[['x']], has_constant='add')).fit()\n",
        "print('OLS beta:', float(ols.params['x']))\n",
        "\n",
        "iv = fit_iv_2sls(df, y_col='y', x_endog='x', x_exog=[], z_cols=['z'])\n",
        "print('IV beta :', float(iv.params['x']))\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "<details><summary>Solution: First-stage + weak IV checks</summary>\n",
        "\n",
        "_One possible approach. Your variable names may differ; align them with the notebook._\n",
        "\n",
        "```python\n",
        "# Reference solution for 03_instrumental_variables_2sls \u2014 First-stage + weak IV checks\n",
        "# Inspect first stage output (instrument strength):\n",
        "# iv.first_stage\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "<details><summary>Solution: Interpretation + limitations</summary>\n",
        "\n",
        "_One possible approach. Your variable names may differ; align them with the notebook._\n",
        "\n",
        "```python\n",
        "# Reference solution for 03_instrumental_variables_2sls \u2014 Interpretation + limitations\n",
        "# Write 3-5 sentences on:\n",
        "# - relevance + exclusion in your simulated setup\n",
        "# - why IV can fix endogeneity here\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "python3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}