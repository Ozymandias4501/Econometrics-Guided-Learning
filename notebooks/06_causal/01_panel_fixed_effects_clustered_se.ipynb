{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 01 Panel Fixed Effects + Clustered SE\n",
        "\n",
        "Pooled vs two-way fixed effects and clustered standard errors.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Table of Contents\n",
        "- [Load panel and define variables](#load-panel-and-define-variables)\n",
        "- [Pooled OLS baseline](#pooled-ols-baseline)\n",
        "- [Two-way fixed effects](#two-way-fixed-effects)\n",
        "- [Clustered standard errors](#clustered-standard-errors)\n",
        "- [Checkpoint (Self-Check)](#checkpoint-self-check)\n",
        "- [Solutions (Reference)](#solutions-reference)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Why This Notebook Matters\n",
        "Causal notebooks focus on **identification**: what would have to be true for a coefficient to represent a causal effect.\n",
        "You will practice:\n",
        "- building a county-year panel,\n",
        "- fixed effects (TWFE),\n",
        "- clustered standard errors,\n",
        "- DiD + event studies,\n",
        "- IV/2SLS.\n",
        "\n",
        "\n",
        "## Prerequisites (Quick Self-Check)\n",
        "- Completed Part 02 (regression + robust SE).\n",
        "- Basic familiarity with panels (same unit over time) and the idea of identification assumptions.\n",
        "\n",
        "## What You Will Produce\n",
        "- (no file output; learning/analysis notebook)\n",
        "\n",
        "## Success Criteria\n",
        "- You can explain what you built and why each step exists.\n",
        "- You can run your work end-to-end without undefined variables.\n",
        "\n",
        "## Common Pitfalls\n",
        "- Running cells top-to-bottom without reading the instructions.\n",
        "- Leaving `...` placeholders in code cells.\n",
        "- Treating regression output as causal without stating identification assumptions.\n",
        "- Using non-clustered SE when shocks are correlated within groups (e.g., states).\n",
        "\n",
        "## Quick Fixes (When You Get Stuck)\n",
        "- If you see `ModuleNotFoundError`, re-run the bootstrap cell and restart the kernel; make sure `PROJECT_ROOT` is the repo root.\n",
        "- If a `data/processed/*` file is missing, either run the matching build script (see guide) or use the notebook\u2019s `data/sample/*` fallback.\n",
        "- If results look \u201ctoo good,\u201d suspect leakage; re-check shifts, rolling windows, and time splits.\n",
        "- If a model errors, check dtypes (`astype(float)`) and missingness (`dropna()` on required columns).\n",
        "\n",
        "## Matching Guide\n",
        "- `docs/guides/06_causal/01_panel_fixed_effects_clustered_se.md`\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## How To Use This Notebook\n",
        "- Work section-by-section; don\u2019t skip the markdown.\n",
        "- Most code cells are incomplete on purpose: replace TODOs and `...`, then run.\n",
        "- After each section, write 2\u20134 sentences answering the interpretation prompts (what changed, why it matters).\n",
        "- Prefer `data/processed/*` if you have built the real datasets; otherwise use the bundled `data/sample/*` fallbacks.\n",
        "- Use the **Checkpoint (Self-Check)** section to catch mistakes early.\n",
        "- Use **Solutions (Reference)** only to unblock yourself; then re-implement without looking.\n",
        "- Use the matching guide (`docs/guides/06_causal/01_panel_fixed_effects_clustered_se.md`) for the math, assumptions, and deeper context.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"environment-bootstrap\"></a>\n",
        "## Environment Bootstrap\n",
        "Run this cell first. It makes the repo importable and defines common directories.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "\n",
        "def find_repo_root(start: Path) -> Path:\n",
        "    p = start\n",
        "    for _ in range(8):\n",
        "        if (p / 'src').exists() and (p / 'docs').exists():\n",
        "            return p\n",
        "        p = p.parent\n",
        "    raise RuntimeError('Could not find repo root. Start Jupyter from the repo root.')\n",
        "\n",
        "\n",
        "PROJECT_ROOT = find_repo_root(Path.cwd())\n",
        "if str(PROJECT_ROOT) not in sys.path:\n",
        "    sys.path.append(str(PROJECT_ROOT))\n",
        "\n",
        "DATA_DIR = PROJECT_ROOT / 'data'\n",
        "RAW_DIR = DATA_DIR / 'raw'\n",
        "PROCESSED_DIR = DATA_DIR / 'processed'\n",
        "SAMPLE_DIR = DATA_DIR / 'sample'\n",
        "\n",
        "PROJECT_ROOT\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Goal\n",
        "Compare:\n",
        "- pooled OLS (ignores panel structure)\n",
        "- two-way fixed effects (county FE + year FE)\n",
        "- robust vs clustered standard errors\n",
        "\n",
        "This is still not causal by default. FE helps control time-invariant confounding, not everything.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Primer: Panel + IV regression with `linearmodels` (FE, clustered SE, 2SLS)\n",
        "\n",
        "This repo uses:\n",
        "- `statsmodels` for classic OLS inference patterns, and\n",
        "- `linearmodels` for **panel fixed effects** and **instrumental variables** (IV/2SLS).\n",
        "\n",
        "The goal of this primer is to make you productive quickly (with the *minimum* theory needed to use the tools correctly). Deep math lives in the guides.\n",
        "\n",
        "### Why `linearmodels`?\n",
        "\n",
        "`linearmodels` provides clean APIs for:\n",
        "- `PanelOLS`: fixed effects / TWFE\n",
        "- `IV2SLS`: two-stage least squares\n",
        "\n",
        "and it handles some panel-specific details (like absorbing FE) more naturally than `statsmodels`.\n",
        "\n",
        "### Panel data shape (the #1 requirement)\n",
        "\n",
        "Most panel estimators expect a **MultiIndex**:\n",
        "- level 0: entity (e.g., county `fips`)\n",
        "- level 1: time (e.g., `year`)\n",
        "\n",
        "```python\n",
        "# df has columns: fips, year, y, x1, x2, state, ...\n",
        "df = df.copy()\n",
        "df[\"fips\"] = df[\"fips\"].astype(str)\n",
        "df[\"year\"] = df[\"year\"].astype(int)\n",
        "df = df.set_index([\"fips\", \"year\"]).sort_index()\n",
        "```\n",
        "\n",
        "**Expected output / sanity check**\n",
        "- `df.index.nlevels == 2`\n",
        "- `df.index.is_monotonic_increasing` is `True`\n",
        "- no duplicate index pairs: `df.index.duplicated().any()` is `False`\n",
        "\n",
        "### TWFE model (PanelOLS)\n",
        "\n",
        "Econometric form:\n",
        "\n",
        "$$\n",
        "Y_{it} = X_{it}'\\\\beta + \\\\alpha_i + \\\\gamma_t + \\\\varepsilon_{it}\n",
        "$$\n",
        "\n",
        "In code:\n",
        "\n",
        "```python\n",
        "from linearmodels.panel import PanelOLS\n",
        "import statsmodels.api as sm\n",
        "\n",
        "y = df[\"y\"].astype(float)\n",
        "X = df[[\"x1\", \"x2\"]].astype(float)\n",
        "X = sm.add_constant(X, has_constant=\"add\")\n",
        "\n",
        "res = PanelOLS(y, X, entity_effects=True, time_effects=True).fit(cov_type=\"robust\")\n",
        "print(res.summary)\n",
        "```\n",
        "\n",
        "### Clustered SE (common in applied panel/DiD work)\n",
        "\n",
        "If errors are correlated within clusters (e.g., state-level shocks), use clustered SE:\n",
        "\n",
        "```python\n",
        "clusters = df[\"state\"]  # must align row-for-row with y/X index\n",
        "\n",
        "res_cl = PanelOLS(y, X, entity_effects=True, time_effects=True).fit(\n",
        "  cov_type=\"clustered\",\n",
        "  clusters=clusters,\n",
        ")\n",
        "```\n",
        "\n",
        "**Expected output / sanity check**\n",
        "- clustered SE are often larger than robust SE (not guaranteed, but common)\n",
        "- always report the number of clusters: `clusters.nunique()`\n",
        "\n",
        "### IV / 2SLS (IV2SLS)\n",
        "\n",
        "Structural equation (endogeneity motivation):\n",
        "$$\n",
        "Y = \\\\beta X + W'\\\\delta + u, \\\\quad \\\\mathrm{Cov}(X,u)\\\\neq 0\n",
        "$$\n",
        "\n",
        "In code (one endogenous regressor):\n",
        "\n",
        "```python\n",
        "from linearmodels.iv import IV2SLS\n",
        "import statsmodels.api as sm\n",
        "\n",
        "y = df[\"y\"].astype(float)\n",
        "endog = df[[\"x_endog\"]].astype(float)\n",
        "exog = sm.add_constant(df[[\"x_exog1\", \"x_exog2\"]].astype(float), has_constant=\"add\")\n",
        "instr = df[[\"z1\", \"z2\"]].astype(float)\n",
        "\n",
        "res_iv = IV2SLS(y, exog, endog, instr).fit(cov_type=\"robust\")\n",
        "print(res_iv.summary)\n",
        "```\n",
        "\n",
        "**Expected output / sanity check**\n",
        "- `res_iv.params` contains coefficients for exog + endogenous variables\n",
        "- `res_iv.first_stage` (if printed) shows instrument relevance diagnostics\n",
        "\n",
        "### Common pitfalls (and quick fixes)\n",
        "\n",
        "- **MultiIndex mismatch:** if `clusters` is not aligned to the same index as `y/X`, you\u2019ll get errors or wrong results.\n",
        "  - Fix: construct clusters from the same `df` after indexing/sorting.\n",
        "- **Non-numeric dtypes:** strings in `X` silently break models.\n",
        "  - Fix: `astype(float)` on model columns.\n",
        "- **Missing data:** panels often have missing rows after merges/transforms.\n",
        "  - Fix: build a modeling table with `.dropna()` for required columns.\n",
        "- **Too few clusters:** cluster-robust inference is fragile with very small cluster counts.\n",
        "  - Fix: treat p-values as fragile; report cluster count; consider alternative designs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"load-panel-and-define-variables\"></a>\n",
        "## Load panel and define variables\n",
        "\n",
        "### Background\n",
        "Panel regressions expect a clear unit index (county) and time index (year).\n",
        "Before modeling, we build a **small, typed modeling table**:\n",
        "- confirm `fips` and `year`,\n",
        "- set a MultiIndex,\n",
        "- create a few interpretable transforms (like logs).\n",
        "\n",
        "Log transforms are common for heavy-tailed variables (income, rent) because they reduce scale and make multiplicative differences more linear.\n",
        "\n",
        "### What you should see\n",
        "- the DataFrame is indexed by `('fips','year')`.\n",
        "- `log_income` and `log_rent` are finite (no -inf/inf).\n",
        "- summary stats look plausible (rates roughly in [0,1]).\n",
        "\n",
        "### Interpretation prompts\n",
        "- Why might `log_income` be easier to interpret than raw income?\n",
        "- What does a 0.01 change in `poverty_rate` represent?\n",
        "\n",
        "### Goal\n",
        "Load the county-year panel and build a small modeling table.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Your Turn: Load panel (processed or sample)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "path = PROCESSED_DIR / 'census_county_panel.csv'\n",
        "if path.exists():\n",
        "    df = pd.read_csv(path)\n",
        "else:\n",
        "    df = pd.read_csv(SAMPLE_DIR / 'census_county_panel_sample.csv')\n",
        "\n",
        "# TODO: Ensure fips/year exist and build a MultiIndex\n",
        "df['fips'] = df['fips'].astype(str)\n",
        "df['year'] = df['year'].astype(int)\n",
        "df = df.set_index(['fips', 'year'], drop=False).sort_index()\n",
        "\n",
        "# Starter transforms\n",
        "df['log_income'] = np.log(df['B19013_001E'].astype(float))\n",
        "df['log_rent'] = np.log(df['B25064_001E'].astype(float))\n",
        "\n",
        "df[['poverty_rate', 'unemployment_rate', 'log_income', 'log_rent']].describe()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"pooled-ols-baseline\"></a>\n",
        "## Pooled OLS baseline\n",
        "\n",
        "### Background\n",
        "Pooled OLS treats each row as an independent observation and ignores that rows come from the same county over time.\n",
        "It is a useful baseline, but it can be misleading when counties differ in unobserved, time-invariant ways (baseline poverty, institutions, geography).\n",
        "\n",
        "### What you should see\n",
        "- a `statsmodels` summary table.\n",
        "- coefficients with HC3 robust SE.\n",
        "\n",
        "### Interpretation prompts\n",
        "- Interpret the sign and units of one coefficient in 2\u20134 sentences.\n",
        "- List one plausible omitted variable that differs across counties and could confound this pooled relationship.\n",
        "\n",
        "### Goal\n",
        "Fit a pooled model that ignores FE.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Your Turn: Fit pooled OLS\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import statsmodels.api as sm\n",
        "\n",
        "y_col = 'poverty_rate'\n",
        "x_cols = ['log_income', 'unemployment_rate']\n",
        "\n",
        "tmp = df[[y_col] + x_cols].dropna().copy()\n",
        "y = tmp[y_col].astype(float)\n",
        "X = sm.add_constant(tmp[x_cols].astype(float), has_constant='add')\n",
        "\n",
        "# TODO: Fit and print a summary (HC3 as a baseline)\n",
        "res_pool = sm.OLS(y, X).fit(cov_type='HC3')\n",
        "print(res_pool.summary())\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"two-way-fixed-effects\"></a>\n",
        "## Two-way fixed effects\n",
        "\n",
        "### Background\n",
        "Two-way fixed effects (TWFE) compares counties to themselves over time (county FE) while removing year-wide shocks (year FE).\n",
        "This can reduce bias from time-invariant county differences.\n",
        "\n",
        "### What you should see\n",
        "- a `PanelOLS` summary.\n",
        "- coefficients that may differ from pooled OLS (because identification uses within-county changes).\n",
        "\n",
        "### Interpretation prompts\n",
        "- Compare pooled vs TWFE: did the coefficient move? What story could explain the change?\n",
        "- What variation identifies $\\beta$ in TWFE (within county, across time)?\n",
        "\n",
        "### Goal\n",
        "Estimate a TWFE model:\n",
        "- county FE (entity)\n",
        "- year FE (time)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Your Turn: Fit TWFE with PanelOLS\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from src.causal import fit_twfe_panel_ols\n",
        "\n",
        "# TODO: Fit TWFE (robust SE)\n",
        "res_twfe = fit_twfe_panel_ols(\n",
        "    df,\n",
        "    y_col=y_col,\n",
        "    x_cols=x_cols,\n",
        "    entity_effects=True,\n",
        "    time_effects=True,\n",
        ")\n",
        "print(res_twfe.summary)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"clustered-standard-errors\"></a>\n",
        "## Clustered standard errors\n",
        "\n",
        "### Background\n",
        "Even with TWFE, inference can be too optimistic if errors are correlated within groups.\n",
        "A common choice here is clustering by state because counties in the same state share policies and shocks.\n",
        "\n",
        "### What you should see\n",
        "- a table comparing robust vs clustered standard errors.\n",
        "- clustered SE are often larger (not guaranteed, but common).\n",
        "\n",
        "### Interpretation prompts\n",
        "- Which SE would you report for a state-level policy story and why?\n",
        "- How many clusters do you have (unique states), and why does that matter?\n",
        "\n",
        "### Goal\n",
        "Re-fit TWFE with clustered SE.\n",
        "\n",
        "Typical clustering choice here:\n",
        "- by state (shared shocks/policies)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Your Turn: Cluster by state and compare SE\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from src.causal import fit_twfe_panel_ols\n",
        "\n",
        "# TODO: Compare robust vs clustered SE\n",
        "res_cluster = fit_twfe_panel_ols(\n",
        "    df,\n",
        "    y_col=y_col,\n",
        "    x_cols=x_cols,\n",
        "    entity_effects=True,\n",
        "    time_effects=True,\n",
        "    cluster_col='state',\n",
        ")\n",
        "\n",
        "pd.DataFrame({'robust_se': res_twfe.std_errors, 'cluster_se': res_cluster.std_errors})\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"checkpoint-self-check\"></a>\n",
        "## Checkpoint (Self-Check)\n",
        "Run a few asserts and write 2-3 sentences summarizing what you verified.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Expected output: (see notebook front matter)\n",
        "# TODO: If you created a panel DataFrame, verify the indexing + core columns.\n",
        "# Example (adjust variable names):\n",
        "# assert isinstance(panel.index, pd.MultiIndex)\n",
        "# assert panel.index.names[:2] == ['fips', 'year']\n",
        "# assert panel['year'].astype(int).between(1900, 2100).all()\n",
        "# assert panel['fips'].astype(str).str.len().eq(5).all()\n",
        "#\n",
        "# TODO: Write 2-3 sentences:\n",
        "# - What is the identification assumption for your causal estimate?\n",
        "# - What diagnostic/falsification did you run?\n",
        "...\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extensions (Optional)\n",
        "- Try one additional variant beyond the main path (different features, different split, different model).\n",
        "- Write down what improved, what got worse, and your hypothesis for why.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reflection\n",
        "- What did you assume implicitly (about timing, availability, stationarity, or costs)?\n",
        "- If you had to ship this model, what would you monitor?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"solutions-reference\"></a>\n",
        "## Solutions (Reference)\n",
        "\n",
        "Try the TODOs first. Use these only to unblock yourself or to compare approaches.\n",
        "\n",
        "<details><summary>Solution: Load panel and define variables</summary>\n",
        "\n",
        "_One possible approach. Your variable names may differ; align them with the notebook._\n",
        "\n",
        "```python\n",
        "# Reference solution for 01_panel_fixed_effects_clustered_se \u2014 Load panel and define variables\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "path = PROCESSED_DIR / 'census_county_panel.csv'\n",
        "if path.exists():\n",
        "    df = pd.read_csv(path)\n",
        "else:\n",
        "    df = pd.read_csv(SAMPLE_DIR / 'census_county_panel_sample.csv')\n",
        "\n",
        "df['fips'] = df['fips'].astype(str)\n",
        "df['year'] = df['year'].astype(int)\n",
        "df = df.set_index(['fips', 'year'], drop=False).sort_index()\n",
        "\n",
        "df['log_income'] = np.log(df['B19013_001E'].astype(float))\n",
        "df['log_rent'] = np.log(df['B25064_001E'].astype(float))\n",
        "df[['poverty_rate', 'log_income', 'unemployment_rate']].describe()\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "<details><summary>Solution: Pooled OLS baseline</summary>\n",
        "\n",
        "_One possible approach. Your variable names may differ; align them with the notebook._\n",
        "\n",
        "```python\n",
        "# Reference solution for 01_panel_fixed_effects_clustered_se \u2014 Pooled OLS baseline\n",
        "import statsmodels.api as sm\n",
        "\n",
        "tmp = df[['poverty_rate', 'log_income', 'unemployment_rate']].dropna().copy()\n",
        "y = tmp['poverty_rate'].astype(float)\n",
        "X = sm.add_constant(tmp[['log_income', 'unemployment_rate']], has_constant='add')\n",
        "res = sm.OLS(y, X).fit(cov_type='HC3')\n",
        "print(res.summary())\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "<details><summary>Solution: Two-way fixed effects</summary>\n",
        "\n",
        "_One possible approach. Your variable names may differ; align them with the notebook._\n",
        "\n",
        "```python\n",
        "# Reference solution for 01_panel_fixed_effects_clustered_se \u2014 Two-way fixed effects\n",
        "from src.causal import fit_twfe_panel_ols\n",
        "\n",
        "res_twfe = fit_twfe_panel_ols(\n",
        "    df,\n",
        "    y_col='poverty_rate',\n",
        "    x_cols=['log_income', 'unemployment_rate'],\n",
        "    entity_effects=True,\n",
        "    time_effects=True,\n",
        ")\n",
        "print(res_twfe.summary)\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "<details><summary>Solution: Clustered standard errors</summary>\n",
        "\n",
        "_One possible approach. Your variable names may differ; align them with the notebook._\n",
        "\n",
        "```python\n",
        "# Reference solution for 01_panel_fixed_effects_clustered_se \u2014 Clustered standard errors\n",
        "from src.causal import fit_twfe_panel_ols\n",
        "\n",
        "res_cluster = fit_twfe_panel_ols(\n",
        "    df,\n",
        "    y_col='poverty_rate',\n",
        "    x_cols=['log_income', 'unemployment_rate'],\n",
        "    entity_effects=True,\n",
        "    time_effects=True,\n",
        "    cluster_col='state',\n",
        ")\n",
        "\n",
        "pd.DataFrame({'robust_se': res_twfe.std_errors, 'cluster_se': res_cluster.std_errors})\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "python3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}