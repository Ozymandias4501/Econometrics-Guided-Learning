{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 01 Reproducible Pipeline Design\n",
        "\n",
        "Configs, run IDs, dataset hashes, and artifact layout.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Table of Contents\n",
        "- [Configs](#configs)\n",
        "- [Outputs](#outputs)\n",
        "- [Reproducibility](#reproducibility)\n",
        "- [Checkpoint (Self-Check)](#checkpoint-self-check)\n",
        "- [Solutions (Reference)](#solutions-reference)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Why This Notebook Matters\n",
        "Model ops notebooks turn your work into reproducible runs with saved artifacts.\n",
        "The goal is: someone else can run your pipeline and see the same metrics.\n",
        "\n",
        "\n",
        "## What You Will Produce\n",
        "- (no file output; learning/analysis notebook)\n",
        "\n",
        "## Success Criteria\n",
        "- You can explain what you built and why each step exists.\n",
        "- You can run your work end-to-end without undefined variables.\n",
        "\n",
        "## Common Pitfalls\n",
        "- Running cells top-to-bottom without reading the instructions.\n",
        "- Leaving `...` placeholders in code cells.\n",
        "- Not recording which dataset/config a model was trained on.\n",
        "- Overwriting artifacts without run IDs.\n",
        "\n",
        "## Matching Guide\n",
        "- `docs/guides/05_model_ops/01_reproducible_pipeline_design.md`\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## How To Use This Notebook\n",
        "- This notebook is hands-on. Most code cells are incomplete on purpose.\n",
        "- Complete each TODO, then run the cell.\n",
        "- Use the matching guide (`docs/guides/05_model_ops/01_reproducible_pipeline_design.md`) for deep explanations and alternative examples.\n",
        "- Write short interpretation notes as you go (what changed, why it matters).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"environment-bootstrap\"></a>\n",
        "## Environment Bootstrap\n",
        "Run this cell first. It makes the repo importable and defines common directories.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "\n",
        "def find_repo_root(start: Path) -> Path:\n",
        "    p = start\n",
        "    for _ in range(8):\n",
        "        if (p / 'src').exists() and (p / 'docs').exists():\n",
        "            return p\n",
        "        p = p.parent\n",
        "    raise RuntimeError('Could not find repo root. Start Jupyter from the repo root.')\n",
        "\n",
        "\n",
        "PROJECT_ROOT = find_repo_root(Path.cwd())\n",
        "if str(PROJECT_ROOT) not in sys.path:\n",
        "    sys.path.append(str(PROJECT_ROOT))\n",
        "\n",
        "DATA_DIR = PROJECT_ROOT / 'data'\n",
        "RAW_DIR = DATA_DIR / 'raw'\n",
        "PROCESSED_DIR = DATA_DIR / 'processed'\n",
        "SAMPLE_DIR = DATA_DIR / 'sample'\n",
        "\n",
        "PROJECT_ROOT\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Goal\n",
        "Understand how to turn notebooks into reproducible runs with configs + artifacts.\n",
        "\n",
        "A model is not \"done\" when you get a good plot.\n",
        "A model is done when you can re-run it and reproduce:\n",
        "- the dataset used\n",
        "- the features used\n",
        "- the metrics\n",
        "- the predictions\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Primer: Paths, Files, and Environment Variables\n",
        "\n",
        "You will see a few patterns repeatedly in this repo.\n",
        "\n",
        "### Environment variables\n",
        "> **What this is:** Environment variables are key/value settings provided by your shell to your Python process.\n",
        "\n",
        "We use them for API keys and configuration defaults.\n",
        "\n",
        "```python\n",
        "import os\n",
        "\n",
        "# Reads an environment variable or returns None\n",
        "fred_key = os.getenv('FRED_API_KEY')\n",
        "print('FRED key set?', fred_key is not None)\n",
        "```\n",
        "\n",
        "If you're running from a terminal, you can set a key like this:\n",
        "\n",
        "```bash\n",
        "export FRED_API_KEY=\"your_key_here\"\n",
        "```\n",
        "\n",
        "Then restart the Jupyter kernel (so Python picks up the new env var).\n",
        "\n",
        "### Paths (why `pathlib.Path`)\n",
        "> **What this is:** A Path is a safe way to build file paths without worrying about OS-specific separators.\n",
        "\n",
        "```python\n",
        "from pathlib import Path\n",
        "\n",
        "p = Path('data') / 'sample' / 'macro_quarterly_sample.csv'\n",
        "print(p)\n",
        "print('exists?', p.exists())\n",
        "```\n",
        "\n",
        "In these notebooks, the bootstrap cell defines:\n",
        "- `PROJECT_ROOT` (repo root)\n",
        "- `DATA_DIR`, `RAW_DIR`, `PROCESSED_DIR`, `SAMPLE_DIR`\n",
        "\n",
        "Prefer those over hard-coding paths.\n",
        "\n",
        "### Reading and writing CSV files\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "# Read\n",
        "# df = pd.read_csv(p, index_col=0, parse_dates=True)\n",
        "\n",
        "# Write\n",
        "# out = Path('data') / 'processed' / 'my_dataset.csv'\n",
        "# out.parent.mkdir(parents=True, exist_ok=True)\n",
        "# df.to_csv(out)\n",
        "```\n",
        "\n",
        "### Tip\n",
        "If you get a \"file not found\" error:\n",
        "- `print(path)` to confirm you're reading what you think you're reading\n",
        "- `print(path.exists())` to confirm the file exists\n",
        "- if you're using a relative path, confirm your current working directory: `import os; print(os.getcwd())`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"configs\"></a>\n",
        "## Configs\n",
        "\n",
        "### Goal\n",
        "Inspect a YAML config and understand what it controls.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Your Turn (1): Load and inspect configs/recession.yaml\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import yaml\n",
        "from pathlib import Path\n",
        "\n",
        "cfg_path = PROJECT_ROOT / 'configs' / 'recession.yaml'\n",
        "cfg = yaml.safe_load(cfg_path.read_text())\n",
        "\n",
        "# TODO: Print top-level keys and explain what each one controls.\n",
        "cfg.keys()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Your Turn (2): Find where config values are used\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# TODO: Open scripts/train_recession.py and scripts/build_datasets.py.\n",
        "# Find how 'series', 'feature settings', and 'split rules' are used.\n",
        "# Write a short list of 'hard-coded' vs 'configurable'.\n",
        "...\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"outputs\"></a>\n",
        "## Outputs\n",
        "\n",
        "### Goal\n",
        "Run a pipeline and inspect the artifact bundle under `outputs/<run_id>/`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Your Turn: Run the pipeline from your terminal\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run these commands in terminal (from repo root):\n",
        "- `python scripts/build_datasets.py --recession-config configs/recession.yaml --census-config configs/census.yaml`\n",
        "- `python scripts/train_recession.py --config configs/recession.yaml`\n",
        "\n",
        "Then come back here and inspect the generated `outputs/<run_id>/` folder.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Your Turn (2): Inspect outputs/ in Python\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# TODO: List run folders under outputs/\n",
        "out_dir = PROJECT_ROOT / 'outputs'\n",
        "runs = sorted([p for p in out_dir.glob('*') if p.is_dir()])\n",
        "runs[-3:]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"reproducibility\"></a>\n",
        "## Reproducibility\n",
        "\n",
        "### Goal\n",
        "Verify that a run is self-describing (you can tell what it did).\n",
        "\n",
        "Minimum expected artifacts:\n",
        "- `model.joblib`\n",
        "- `metrics.json`\n",
        "- `predictions.csv`\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Your Turn: Check artifact bundle completeness\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# TODO: Pick the newest run folder and check expected files exist.\n",
        "if not runs:\n",
        "    raise RuntimeError('No runs found. Did you run the training script?')\n",
        "\n",
        "run = runs[-1]\n",
        "expected = ['model.joblib', 'metrics.json', 'predictions.csv']\n",
        "for name in expected:\n",
        "    print(name, (run / name).exists())\n",
        "...\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"checkpoint-self-check\"></a>\n",
        "## Checkpoint (Self-Check)\n",
        "Run a few asserts and write 2-3 sentences summarizing what you verified.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# TODO: Run one script end-to-end and confirm an artifact bundle exists.\n",
        "# Example:\n",
        "# - list outputs/ and pick the newest run_id\n",
        "# - assert model.joblib and metrics.json exist\n",
        "...\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extensions (Optional)\n",
        "- Try one additional variant beyond the main path (different features, different split, different model).\n",
        "- Write down what improved, what got worse, and your hypothesis for why.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reflection\n",
        "- What did you assume implicitly (about timing, availability, stationarity, or costs)?\n",
        "- If you had to ship this model, what would you monitor?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"solutions-reference\"></a>\n",
        "## Solutions (Reference)\n",
        "\n",
        "Try the TODOs first. Use these only to unblock yourself or to compare approaches.\n",
        "\n",
        "<details><summary>Solution: Configs</summary>\n",
        "\n",
        "_One possible approach. Your variable names may differ; align them with the notebook._\n",
        "\n",
        "```python\n",
        "# Reference solution for 01_reproducible_pipeline_design \u2014 Configs\n",
        "# Open configs/recession.yaml and configs/census.yaml and explain each field.\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "<details><summary>Solution: Outputs</summary>\n",
        "\n",
        "_One possible approach. Your variable names may differ; align them with the notebook._\n",
        "\n",
        "```python\n",
        "# Reference solution for 01_reproducible_pipeline_design \u2014 Outputs\n",
        "# Run:\n",
        "#   python scripts/build_datasets.py --recession-config configs/recession.yaml --census-config configs/census.yaml\n",
        "#   python scripts/train_recession.py --config configs/recession.yaml\n",
        "# Then inspect outputs/<run_id>/\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "<details><summary>Solution: Reproducibility</summary>\n",
        "\n",
        "_One possible approach. Your variable names may differ; align them with the notebook._\n",
        "\n",
        "```python\n",
        "# Reference solution for 01_reproducible_pipeline_design \u2014 Reproducibility\n",
        "# Confirm run_metadata.json includes dataset hash and feature list.\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "python3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}