{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 Reproducible Pipeline Design\n",
    "\n",
    "Configs, run IDs, dataset hashes, and artifact layout.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [Configs](#configs)\n",
    "- [Outputs](#outputs)\n",
    "- [Reproducibility](#reproducibility)\n",
    "- [Checkpoint (Self-Check)](#checkpoint-self-check)\n",
    "- [Solutions (Reference)](#solutions-reference)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why This Notebook Matters\n",
    "Model ops notebooks turn your work into reproducible runs with saved artifacts.\n",
    "The goal is: someone else can run your pipeline and see the same metrics.\n",
    "\n",
    "\n",
    "## Prerequisites (Quick Self-Check)\n",
    "- Completed earlier modeling notebooks (regression/classification).\n",
    "- Comfort running scripts and inspecting files under `outputs/`.\n",
    "\n",
    "## What You Will Produce\n",
    "- (no file output; learning/analysis notebook)\n",
    "\n",
    "## Success Criteria\n",
    "- You can explain what you built and why each step exists.\n",
    "- You can run your work end-to-end without undefined variables.\n",
    "\n",
    "## Common Pitfalls\n",
    "- Running cells top-to-bottom without reading the instructions.\n",
    "- Leaving `...` placeholders in code cells.\n",
    "- Not recording which dataset/config a model was trained on.\n",
    "- Overwriting artifacts without run IDs.\n",
    "\n",
    "## Quick Fixes (When You Get Stuck)\n",
    "- If you see `ModuleNotFoundError`, re-run the bootstrap cell and restart the kernel; make sure `PROJECT_ROOT` is the repo root.\n",
    "- If a `data/processed/*` file is missing, either run the matching build script (see guide) or use the notebook’s `data/sample/*` fallback.\n",
    "- If results look “too good,” suspect leakage; re-check shifts, rolling windows, and time splits.\n",
    "- If a model errors, check dtypes (`astype(float)`) and missingness (`dropna()` on required columns).\n",
    "\n",
    "## Matching Guide\n",
    "- `docs/guides/05_model_ops/01_reproducible_pipeline_design.md`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How To Use This Notebook\n",
    "- Work section-by-section; don’t skip the markdown.\n",
    "- Most code cells are incomplete on purpose: replace TODOs and `...`, then run.\n",
    "- After each section, write 2–4 sentences answering the interpretation prompts (what changed, why it matters).\n",
    "- Prefer `data/processed/*` if you have built the real datasets; otherwise use the bundled `data/sample/*` fallbacks.\n",
    "- Use the **Checkpoint (Self-Check)** section to catch mistakes early.\n",
    "- Use **Solutions (Reference)** only to unblock yourself; then re-implement without looking.\n",
    "- Use the matching guide (`docs/guides/05_model_ops/01_reproducible_pipeline_design.md`) for the math, assumptions, and deeper context.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"environment-bootstrap\"></a>\n",
    "## Environment Bootstrap\n",
    "Run this cell first. It makes the repo importable and defines common directories.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "\n",
    "def find_repo_root(start: Path) -> Path:\n",
    "    p = start\n",
    "    for _ in range(8):\n",
    "        if (p / 'src').exists() and (p / 'docs').exists():\n",
    "            return p\n",
    "        p = p.parent\n",
    "    raise RuntimeError('Could not find repo root. Start Jupyter from the repo root.')\n",
    "\n",
    "\n",
    "PROJECT_ROOT = find_repo_root(Path.cwd())\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "RAW_DIR = DATA_DIR / 'raw'\n",
    "PROCESSED_DIR = DATA_DIR / 'processed'\n",
    "SAMPLE_DIR = DATA_DIR / 'sample'\n",
    "\n",
    "PROJECT_ROOT\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal\n",
    "Understand how to turn notebooks into reproducible runs with configs + artifacts.\n",
    "\n",
    "A model is not \"done\" when you get a good plot.\n",
    "A model is done when you can re-run it and reproduce:\n",
    "- the dataset used\n",
    "- the features used\n",
    "- the metrics\n",
    "- the predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Primer: Configs, run IDs, and artifact management\n\nModel ops is about making experiments reproducible. These are the building blocks.\n\n### Configs (YAML files that record your choices)\n\nA config file captures every decision that affects a model run:\n\n```yaml\n# configs/recession.yaml\nseries:           # which FRED series to fetch\n  - T10Y2Y\n  - UNRATE\n  - FEDFUNDS\nfeatures:\n  lags: [1, 2, 4]\n  rolling_windows: [4, 8]\nsplit:\n  test_size: 0.2\nmodel:\n  type: logistic\n  C: 1.0\n```\n\n**Why it matters:** Without a config, your notebook is a black box. Six months later, you won't remember which features you used or how you split the data.\n\n### Run IDs (unique identifiers for each experiment)\n\nEach training run saves artifacts under `outputs/<run_id>/`:\n\n```python\nfrom datetime import datetime\n\nrun_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\nrun_dir = Path(\"outputs\") / run_id\nrun_dir.mkdir(parents=True, exist_ok=True)\n```\n\n### Artifacts (what a complete run should save)\n\n| File | Purpose |\n|---|---|\n| `model.joblib` | Serialized trained model |\n| `metrics.json` | Evaluation metrics (ROC-AUC, PR-AUC, etc.) |\n| `predictions.csv` | Out-of-sample predictions for inspection |\n| `run_metadata.json` | Config used, dataset hash, timestamp |\n\n```python\nimport joblib, json\n\njoblib.dump(model, run_dir / \"model.joblib\")\njson.dump(metrics, open(run_dir / \"metrics.json\", \"w\"), indent=2)\npredictions.to_csv(run_dir / \"predictions.csv\")\n```\n\n### Verifying reproducibility\n\nA run is reproducible if you can re-run it from the same config and get the same outputs:\n- Fixed random seeds (`np.random.default_rng(seed)`)\n- Fixed data version (dataset hash saved in metadata)\n- Fixed code version (git commit hash in metadata)\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"configs\"></a>\n",
    "## Configs\n",
    "\n",
    "### Goal\n",
    "Inspect a YAML config and understand what it controls.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Turn (1): Load and inspect configs/recession.yaml\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "cfg_path = PROJECT_ROOT / 'configs' / 'recession.yaml'\n",
    "cfg = yaml.safe_load(cfg_path.read_text())\n",
    "\n",
    "# TODO: Print top-level keys and explain what each one controls.\n",
    "cfg.keys()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Turn (2): Find where config values are used\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: Open scripts/train_recession.py and scripts/build_datasets.py.\n",
    "# Find how 'series', 'feature settings', and 'split rules' are used.\n",
    "# Write a short list of 'hard-coded' vs 'configurable'.\n",
    "...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"outputs\"></a>\n",
    "## Outputs\n",
    "\n",
    "### Goal\n",
    "Run a pipeline and inspect the artifact bundle under `outputs/<run_id>/`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Turn: Run the pipeline from your terminal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run these commands in terminal (from repo root):\n",
    "- `python scripts/build_datasets.py --recession-config configs/recession.yaml --census-config configs/census.yaml`\n",
    "- `python scripts/train_recession.py --config configs/recession.yaml`\n",
    "\n",
    "Then come back here and inspect the generated `outputs/<run_id>/` folder.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Turn (2): Inspect outputs/ in Python\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# TODO: List run folders under outputs/\n",
    "out_dir = PROJECT_ROOT / 'outputs'\n",
    "runs = sorted([p for p in out_dir.glob('*') if p.is_dir()])\n",
    "runs[-3:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"reproducibility\"></a>\n",
    "## Reproducibility\n",
    "\n",
    "### Goal\n",
    "Verify that a run is self-describing (you can tell what it did).\n",
    "\n",
    "Minimum expected artifacts:\n",
    "- `model.joblib`\n",
    "- `metrics.json`\n",
    "- `predictions.csv`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Turn: Check artifact bundle completeness\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: Pick the newest run folder and check expected files exist.\n",
    "if not runs:\n",
    "    raise RuntimeError('No runs found. Did you run the training script?')\n",
    "\n",
    "run = runs[-1]\n",
    "expected = ['model.joblib', 'metrics.json', 'predictions.csv']\n",
    "for name in expected:\n",
    "    print(name, (run / name).exists())\n",
    "...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"checkpoint-self-check\"></a>\n",
    "## Checkpoint (Self-Check)\n",
    "Run a few asserts and write 2-3 sentences summarizing what you verified.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: Run one script end-to-end and confirm an artifact bundle exists.\n",
    "# Example:\n",
    "# - list outputs/ and pick the newest run_id\n",
    "# - assert model.joblib and metrics.json exist\n",
    "...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions (Optional)\n",
    "- Try one additional variant beyond the main path (different features, different split, different model).\n",
    "- Write down what improved, what got worse, and your hypothesis for why.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection\n",
    "- What did you assume implicitly (about timing, availability, stationarity, or costs)?\n",
    "- If you had to ship this model, what would you monitor?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"solutions-reference\"></a>\n",
    "## Solutions (Reference)\n",
    "\n",
    "Try the TODOs first. Use these only to unblock yourself or to compare approaches.\n",
    "\n",
    "<details><summary>Solution: Configs</summary>\n",
    "\n",
    "_One possible approach. Your variable names may differ; align them with the notebook._\n",
    "\n",
    "```python\n",
    "# Reference solution for 01_reproducible_pipeline_design — Configs\n",
    "# Open configs/recession.yaml and configs/census.yaml and explain each field.\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<details><summary>Solution: Outputs</summary>\n",
    "\n",
    "_One possible approach. Your variable names may differ; align them with the notebook._\n",
    "\n",
    "```python\n",
    "# Reference solution for 01_reproducible_pipeline_design — Outputs\n",
    "# Run:\n",
    "#   python scripts/build_datasets.py --recession-config configs/recession.yaml --census-config configs/census.yaml\n",
    "#   python scripts/train_recession.py --config configs/recession.yaml\n",
    "# Then inspect outputs/<run_id>/\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<details><summary>Solution: Reproducibility</summary>\n",
    "\n",
    "_One possible approach. Your variable names may differ; align them with the notebook._\n",
    "\n",
    "```python\n",
    "# Reference solution for 01_reproducible_pipeline_design — Reproducibility\n",
    "# Confirm run_metadata.json includes dataset hash and feature list.\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}