{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 Model Cards and Reporting\n",
    "\n",
    "Document your model: intended use, risks, limitations, monitoring.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [Model card](#model-card)\n",
    "- [Reporting](#reporting)\n",
    "- [Limitations](#limitations)\n",
    "- [Checkpoint (Self-Check)](#checkpoint-self-check)\n",
    "- [Solutions (Reference)](#solutions-reference)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why This Notebook Matters\n",
    "Model ops notebooks turn your work into reproducible runs with saved artifacts.\n",
    "The goal is: someone else can run your pipeline and see the same metrics.\n",
    "\n",
    "\n",
    "## Prerequisites (Quick Self-Check)\n",
    "- Completed earlier modeling notebooks (regression/classification).\n",
    "- Comfort running scripts and inspecting files under `outputs/`.\n",
    "\n",
    "## What You Will Produce\n",
    "- (no file output; learning/analysis notebook)\n",
    "\n",
    "## Success Criteria\n",
    "- You can explain what you built and why each step exists.\n",
    "- You can run your work end-to-end without undefined variables.\n",
    "\n",
    "## Common Pitfalls\n",
    "- Running cells top-to-bottom without reading the instructions.\n",
    "- Leaving `...` placeholders in code cells.\n",
    "- Not recording which dataset/config a model was trained on.\n",
    "- Overwriting artifacts without run IDs.\n",
    "\n",
    "## Quick Fixes (When You Get Stuck)\n",
    "- If you see `ModuleNotFoundError`, re-run the bootstrap cell and restart the kernel; make sure `PROJECT_ROOT` is the repo root.\n",
    "- If a `data/processed/*` file is missing, either run the matching build script (see guide) or use the notebook’s `data/sample/*` fallback.\n",
    "- If results look “too good,” suspect leakage; re-check shifts, rolling windows, and time splits.\n",
    "- If a model errors, check dtypes (`astype(float)`) and missingness (`dropna()` on required columns).\n",
    "\n",
    "## Matching Guide\n",
    "- `docs/guides/05_model_ops/03_model_cards_and_reporting.md`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How To Use This Notebook\n",
    "- Work section-by-section; don’t skip the markdown.\n",
    "- Most code cells are incomplete on purpose: replace TODOs and `...`, then run.\n",
    "- After each section, write 2–4 sentences answering the interpretation prompts (what changed, why it matters).\n",
    "- Prefer `data/processed/*` if you have built the real datasets; otherwise use the bundled `data/sample/*` fallbacks.\n",
    "- Use the **Checkpoint (Self-Check)** section to catch mistakes early.\n",
    "- Use **Solutions (Reference)** only to unblock yourself; then re-implement without looking.\n",
    "- Use the matching guide (`docs/guides/05_model_ops/03_model_cards_and_reporting.md`) for the math, assumptions, and deeper context.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"environment-bootstrap\"></a>\n",
    "## Environment Bootstrap\n",
    "Run this cell first. It makes the repo importable and defines common directories.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "\n",
    "def find_repo_root(start: Path) -> Path:\n",
    "    p = start\n",
    "    for _ in range(8):\n",
    "        if (p / 'src').exists() and (p / 'docs').exists():\n",
    "            return p\n",
    "        p = p.parent\n",
    "    raise RuntimeError('Could not find repo root. Start Jupyter from the repo root.')\n",
    "\n",
    "\n",
    "PROJECT_ROOT = find_repo_root(Path.cwd())\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "RAW_DIR = DATA_DIR / 'raw'\n",
    "PROCESSED_DIR = DATA_DIR / 'processed'\n",
    "SAMPLE_DIR = DATA_DIR / 'sample'\n",
    "\n",
    "PROJECT_ROOT\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal\n",
    "Write a \"model card\"-style document for one training run.\n",
    "\n",
    "A model card forces you to answer:\n",
    "- What is this model for?\n",
    "- What data did it use?\n",
    "- How was it evaluated?\n",
    "- What are the limitations and risks?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Primer: Model cards (documenting what a model does and doesn't do)\n\nA **model card** is a short, structured document that accompanies a trained model. It forces you to answer the questions a reviewer, stakeholder, or future-you would ask.\n\n### Why model cards matter\n\nWithout documentation, a model file is a black box. Six months later, you won't remember:\n- what data it was trained on,\n- which features were included,\n- what its failure modes are,\n- whether it's safe to use on a different population.\n\n### Standard model card sections\n\n| Section | What to write |\n|---|---|\n| **Model details** | Type (logistic, random forest), hyperparameters, training date |\n| **Intended use** | What decisions does this model inform? Who are the users? |\n| **Training data** | Source, date range, sample size, key features |\n| **Evaluation** | Metrics (ROC-AUC, PR-AUC, F1), train/test split strategy, results |\n| **Limitations** | Known failure modes, data gaps, populations where it may not generalize |\n| **Ethical considerations** | Potential harms if the model is wrong, fairness across subgroups |\n\n### Example (recession classifier)\n\n```markdown\n## Model Card: Recession Classifier v1\n\n**Model type:** Logistic regression (L2, C=1.0)\n**Training data:** FRED macro quarterly, 1970Q1–2019Q4\n**Test data:** 2020Q1–2024Q4\n**Features:** T10Y2Y_lag1, UNRATE_lag1, FEDFUNDS_lag1, INDPRO_lag1\n\n**Performance:**\n- ROC-AUC: 0.87\n- PR-AUC: 0.52\n- Brier score: 0.06\n\n**Limitations:**\n- Only 7 recessions in the training period — limited positive examples\n- COVID recession (2020Q2) is structurally different from prior recessions\n- Model assumes feature distributions are stationary\n```\n\n### Common pitfalls\n\n- Writing a model card after the fact (write it during development when details are fresh).\n- Listing only good metrics — a useful model card also documents where the model fails.\n- Omitting the data version — model performance depends on the exact dataset used.\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"model-card\"></a>\n",
    "## Model card\n",
    "\n",
    "### Goal\n",
    "Fill out the provided report template for one run.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Turn (1): Pick a run folder\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "out_dir = PROJECT_ROOT / 'outputs'\n",
    "runs = sorted([p for p in out_dir.glob('*') if p.is_dir()])\n",
    "runs[-3:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Turn (2): Load metrics.json and predictions.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# TODO: Pick a run folder (e.g., the newest)\n",
    "run = runs[-1]\n",
    "\n",
    "metrics = json.loads((run / 'metrics.json').read_text())\n",
    "preds = pd.read_csv(run / 'predictions.csv')\n",
    "\n",
    "metrics, preds.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"reporting\"></a>\n",
    "## Reporting\n",
    "\n",
    "### Goal\n",
    "Connect artifacts to a written narrative.\n",
    "\n",
    "Write a report that includes:\n",
    "- what you predicted (label definition)\n",
    "- dataset + time range\n",
    "- train/test or walk-forward evaluation summary\n",
    "- key plots/metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Turn: Fill reports/capstone_report.md for this run\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: Open reports/capstone_report.md and fill it using the artifacts above.\n",
    "# - insert metrics\n",
    "# - insert a short interpretation narrative\n",
    "...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"limitations\"></a>\n",
    "## Limitations\n",
    "\n",
    "### Goal\n",
    "Write a high-quality limitations section.\n",
    "\n",
    "Prompts:\n",
    "- data limitations (timing, revisions, coverage)\n",
    "- label limitations (technical recession proxy)\n",
    "- evaluation limitations (few recessions, era changes)\n",
    "- deployment limitations (stale model, monitoring)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Turn: Write limitations as bullet points\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "limitations = [\n",
    "    '...'\n",
    "]\n",
    "limitations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"checkpoint-self-check\"></a>\n",
    "## Checkpoint (Self-Check)\n",
    "Run a few asserts and write 2-3 sentences summarizing what you verified.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: Run one script end-to-end and confirm an artifact bundle exists.\n",
    "# Example:\n",
    "# - list outputs/ and pick the newest run_id\n",
    "# - assert model.joblib and metrics.json exist\n",
    "...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions (Optional)\n",
    "- Try one additional variant beyond the main path (different features, different split, different model).\n",
    "- Write down what improved, what got worse, and your hypothesis for why.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection\n",
    "- What did you assume implicitly (about timing, availability, stationarity, or costs)?\n",
    "- If you had to ship this model, what would you monitor?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"solutions-reference\"></a>\n",
    "## Solutions (Reference)\n",
    "\n",
    "Try the TODOs first. Use these only to unblock yourself or to compare approaches.\n",
    "\n",
    "<details><summary>Solution: Model card</summary>\n",
    "\n",
    "_One possible approach. Your variable names may differ; align them with the notebook._\n",
    "\n",
    "```python\n",
    "# Reference solution for 03_model_cards_and_reporting — Model card\n",
    "# Use reports/capstone_report.md as a template.\n",
    "# Fill it using one specific outputs/<run_id>.\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<details><summary>Solution: Reporting</summary>\n",
    "\n",
    "_One possible approach. Your variable names may differ; align them with the notebook._\n",
    "\n",
    "```python\n",
    "# Reference solution for 03_model_cards_and_reporting — Reporting\n",
    "# Include: metrics, calibration, top drivers, error analysis by date.\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<details><summary>Solution: Limitations</summary>\n",
    "\n",
    "_One possible approach. Your variable names may differ; align them with the notebook._\n",
    "\n",
    "```python\n",
    "# Reference solution for 03_model_cards_and_reporting — Limitations\n",
    "# Include: GDP revisions, structural breaks, leakage risks, regime shifts.\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}