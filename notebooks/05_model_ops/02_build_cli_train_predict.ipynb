{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 Build CLI Train/Predict\n",
    "\n",
    "Extend the CLI to control features/models and generate artifacts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [Training CLI](#training-cli)\n",
    "- [Prediction CLI](#prediction-cli)\n",
    "- [Artifacts](#artifacts)\n",
    "- [Checkpoint (Self-Check)](#checkpoint-self-check)\n",
    "- [Solutions (Reference)](#solutions-reference)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why This Notebook Matters\n",
    "Model ops notebooks turn your work into reproducible runs with saved artifacts.\n",
    "The goal is: someone else can run your pipeline and see the same metrics.\n",
    "\n",
    "\n",
    "## Prerequisites (Quick Self-Check)\n",
    "- Completed earlier modeling notebooks (regression/classification).\n",
    "- Comfort running scripts and inspecting files under `outputs/`.\n",
    "\n",
    "## What You Will Produce\n",
    "- outputs/<run_id>/model.joblib\n",
    "- outputs/<run_id>/metrics.json\n",
    "- outputs/<run_id>/predictions.csv\n",
    "\n",
    "## Success Criteria\n",
    "- You can explain what you built and why each step exists.\n",
    "- You can run your work end-to-end without undefined variables.\n",
    "- You can point to the concrete deliverable(s) listed below and explain how they were produced.\n",
    "\n",
    "## Common Pitfalls\n",
    "- Running cells top-to-bottom without reading the instructions.\n",
    "- Leaving `...` placeholders in code cells.\n",
    "- Not recording which dataset/config a model was trained on.\n",
    "- Overwriting artifacts without run IDs.\n",
    "\n",
    "## Quick Fixes (When You Get Stuck)\n",
    "- If you see `ModuleNotFoundError`, re-run the bootstrap cell and restart the kernel; make sure `PROJECT_ROOT` is the repo root.\n",
    "- If a `data/processed/*` file is missing, either run the matching build script (see guide) or use the notebook’s `data/sample/*` fallback.\n",
    "- If results look “too good,” suspect leakage; re-check shifts, rolling windows, and time splits.\n",
    "- If a model errors, check dtypes (`astype(float)`) and missingness (`dropna()` on required columns).\n",
    "\n",
    "## Matching Guide\n",
    "- `docs/guides/05_model_ops/02_build_cli_train_predict.md`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How To Use This Notebook\n",
    "- Work section-by-section; don’t skip the markdown.\n",
    "- Most code cells are incomplete on purpose: replace TODOs and `...`, then run.\n",
    "- After each section, write 2–4 sentences answering the interpretation prompts (what changed, why it matters).\n",
    "- Prefer `data/processed/*` if you have built the real datasets; otherwise use the bundled `data/sample/*` fallbacks.\n",
    "- Use the **Checkpoint (Self-Check)** section to catch mistakes early.\n",
    "- Use **Solutions (Reference)** only to unblock yourself; then re-implement without looking.\n",
    "- Use the matching guide (`docs/guides/05_model_ops/02_build_cli_train_predict.md`) for the math, assumptions, and deeper context.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"environment-bootstrap\"></a>\n",
    "## Environment Bootstrap\n",
    "Run this cell first. It makes the repo importable and defines common directories.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "\n",
    "def find_repo_root(start: Path) -> Path:\n",
    "    p = start\n",
    "    for _ in range(8):\n",
    "        if (p / 'src').exists() and (p / 'docs').exists():\n",
    "            return p\n",
    "        p = p.parent\n",
    "    raise RuntimeError('Could not find repo root. Start Jupyter from the repo root.')\n",
    "\n",
    "\n",
    "PROJECT_ROOT = find_repo_root(Path.cwd())\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "RAW_DIR = DATA_DIR / 'raw'\n",
    "PROCESSED_DIR = DATA_DIR / 'processed'\n",
    "SAMPLE_DIR = DATA_DIR / 'sample'\n",
    "\n",
    "PROJECT_ROOT\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal\n",
    "Practice model ops by extending the CLI scripts:\n",
    "- add flags/config controls\n",
    "- generate artifact bundles\n",
    "- run predict to produce new outputs\n",
    "\n",
    "This notebook is hands-on *engineering*, not just analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Primer: CLI design with `argparse` (making scripts configurable)\n\nScripts in this project use Python's `argparse` module to accept command-line arguments. This makes experiments controllable without editing code.\n\n### Minimal argparse pattern\n\n```python\nimport argparse\n\nparser = argparse.ArgumentParser(description=\"Train recession classifier\")\nparser.add_argument(\"--config\", type=str, required=True, help=\"Path to YAML config\")\nparser.add_argument(\"--model\", type=str, default=\"logistic\", choices=[\"logistic\", \"random_forest\"])\nparser.add_argument(\"--test-size\", type=float, default=0.2)\nargs = parser.parse_args()\n\nprint(args.config, args.model, args.test_size)\n```\n\nUsage from the terminal:\n```bash\npython scripts/train_recession.py --config configs/recession.yaml --model random_forest --test-size 0.25\n```\n\n### Loading YAML configs\n\n```python\nimport yaml\n\ncfg = yaml.safe_load(open(args.config))\nseries = cfg[\"series\"]       # which FRED series to use\nfeatures = cfg[\"features\"]   # lag/rolling settings\n```\n\nThe config + CLI flags together define a complete experiment specification.\n\n### Saving run metadata\n\nEvery training run should save enough metadata to reproduce itself:\n\n```python\nimport json\nfrom datetime import datetime\n\nrun_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\nmetadata = {\n    \"run_id\": run_id,\n    \"config_path\": str(args.config),\n    \"model_type\": args.model,\n    \"test_size\": args.test_size,\n    \"timestamp\": run_id,\n}\njson.dump(metadata, open(f\"outputs/{run_id}/run_metadata.json\", \"w\"), indent=2)\n```\n\n### Common pitfalls\n\n- Hard-coding values that should be flags (e.g., feature lists, split sizes).\n- Not saving the config used for a run — making it impossible to reproduce.\n- Using `sys.argv` directly instead of `argparse` (fragile, no help text, no validation).\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"training-cli\"></a>\n",
    "## Training CLI\n",
    "\n",
    "### Goal\n",
    "Extend `scripts/train_recession.py` so you can control key behavior from the command line.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Turn (1): Inspect the current CLI\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: Open scripts/train_recession.py and find:\n",
    "# - how argparse is set up\n",
    "# - what args exist today\n",
    "# - what config fields are read\n",
    "...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Turn (2): Add a meaningful flag\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement at least one of these:\n",
    "- `--model logistic|random_forest`\n",
    "- `--include-gdp-features true|false`\n",
    "- `--test-size 0.2`\n",
    "\n",
    "Constraints:\n",
    "- default behavior should remain unchanged\n",
    "- the selected option must be written to the run folder (as JSON/YAML)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: After implementing, re-run training and confirm it works.\n",
    "# In terminal: python scripts/train_recession.py --config configs/recession.yaml --model random_forest\n",
    "...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"prediction-cli\"></a>\n",
    "## Prediction CLI\n",
    "\n",
    "### Goal\n",
    "Extend `scripts/predict_recession.py` to support useful output controls.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Turn: Add a filter option\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement one option:\n",
    "- `--last-n 20` (only write last N predictions)\n",
    "- `--from-date 2010-01-01` (filter by date)\n",
    "\n",
    "Make sure the filter is applied to the output CSV.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: After implementing, run prediction on your latest run.\n",
    "# In terminal: python scripts/predict_recession.py --run-id <run_id> --last-n 20\n",
    "...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"artifacts\"></a>\n",
    "## Artifacts\n",
    "\n",
    "### Goal\n",
    "Verify your artifact bundle is complete and interpretable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Turn: Inspect the newest run artifacts\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "out_dir = PROJECT_ROOT / 'outputs'\n",
    "runs = sorted([p for p in out_dir.glob('*') if p.is_dir()])\n",
    "if not runs:\n",
    "    raise RuntimeError('No runs found. Run training first.')\n",
    "\n",
    "run = runs[-1]\n",
    "print('run:', run.name)\n",
    "print('files:', [p.name for p in run.iterdir()])\n",
    "...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"checkpoint-self-check\"></a>\n",
    "## Checkpoint (Self-Check)\n",
    "Run a few asserts and write 2-3 sentences summarizing what you verified.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: Run one script end-to-end and confirm an artifact bundle exists.\n",
    "# Example:\n",
    "# - list outputs/ and pick the newest run_id\n",
    "# - assert model.joblib and metrics.json exist\n",
    "...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions (Optional)\n",
    "- Try one additional variant beyond the main path (different features, different split, different model).\n",
    "- Write down what improved, what got worse, and your hypothesis for why.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection\n",
    "- What did you assume implicitly (about timing, availability, stationarity, or costs)?\n",
    "- If you had to ship this model, what would you monitor?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"solutions-reference\"></a>\n",
    "## Solutions (Reference)\n",
    "\n",
    "Try the TODOs first. Use these only to unblock yourself or to compare approaches.\n",
    "\n",
    "<details><summary>Solution: Training CLI</summary>\n",
    "\n",
    "_One possible approach. Your variable names may differ; align them with the notebook._\n",
    "\n",
    "```python\n",
    "# Reference solution for 02_build_cli_train_predict — Training CLI\n",
    "# Reference idea: add argparse flags in scripts/train_recession.py\n",
    "# - --include-gdp-features true/false\n",
    "# - --model logistic|rf\n",
    "# Then branch logic when selecting feature_cols and choosing estimator.\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<details><summary>Solution: Prediction CLI</summary>\n",
    "\n",
    "_One possible approach. Your variable names may differ; align them with the notebook._\n",
    "\n",
    "```python\n",
    "# Reference solution for 02_build_cli_train_predict — Prediction CLI\n",
    "# Reference idea: add --last-n option to scripts/predict_recession.py\n",
    "# Slice the output dataframe before writing.\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<details><summary>Solution: Artifacts</summary>\n",
    "\n",
    "_One possible approach. Your variable names may differ; align them with the notebook._\n",
    "\n",
    "```python\n",
    "# Reference solution for 02_build_cli_train_predict — Artifacts\n",
    "# Ensure each run writes:\n",
    "# - model.joblib\n",
    "# - metrics.json\n",
    "# - predictions.csv\n",
    "# - run_metadata.json\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}