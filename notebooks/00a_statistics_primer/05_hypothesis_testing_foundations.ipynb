{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# 05 Hypothesis Testing Foundations\n",
    "\n",
    "The logic of statistical testing: null hypotheses, p-values, significance, errors, and power."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "toc",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [The logic of hypothesis testing](#the-logic-of-hypothesis-testing)\n",
    "- [Null and alternative hypotheses](#null-and-alternative-hypotheses)\n",
    "- [Test statistics and p-values](#test-statistics-and-p-values)\n",
    "- [What p-values are NOT](#what-p-values-are-not)\n",
    "- [Type I and Type II errors](#type-i-and-type-ii-errors)\n",
    "- [Statistical power](#statistical-power)\n",
    "- [Significance levels and multiple testing](#significance-levels-and-multiple-testing)\n",
    "- [Connecting to regression output](#connecting-to-regression-output)\n",
    "- [Checkpoint (Self-Check)](#checkpoint-self-check)\n",
    "- [Solutions (Reference)](#solutions-reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "why-matters",
   "metadata": {},
   "source": [
    "## Why This Notebook Matters\n",
    "Hypothesis testing is the backbone of empirical economics. Every regression coefficient\n",
    "comes with a p-value. Every policy evaluation involves a test. Understanding the logic\n",
    "correctly — especially the common misinterpretations — is essential for reading and\n",
    "producing credible empirical work. This notebook builds that understanding from scratch.\n",
    "\n",
    "## Prerequisites (Quick Self-Check)\n",
    "- Completed notebooks 00-04 (descriptive stats through confidence intervals).\n",
    "- Understanding of sampling distributions and the CLT.\n",
    "- Comfort with the t-distribution.\n",
    "\n",
    "## What You Will Produce\n",
    "- (no file output; learning/analysis notebook)\n",
    "\n",
    "## Success Criteria\n",
    "- You can state null and alternative hypotheses for common economic questions.\n",
    "- You can correctly interpret (and state the limits of) a p-value.\n",
    "- You can explain Type I error, Type II error, and statistical power.\n",
    "- You can simulate hypothesis tests to verify their properties.\n",
    "\n",
    "## Common Pitfalls\n",
    "- Interpreting p-value as \"probability that H0 is true.\"\n",
    "- Treating \"not significant\" as \"no effect\" (could be low power).\n",
    "- Running many tests and reporting only the significant ones (p-hacking).\n",
    "- Confusing statistical significance with economic/practical significance.\n",
    "- Using one-sided tests to get smaller p-values without theoretical justification.\n",
    "\n",
    "## Quick Fixes (When You Get Stuck)\n",
    "- `scipy.stats.ttest_1samp(x, popmean)` for one-sample t-test.\n",
    "- `scipy.stats.ttest_ind(x, y)` for two-sample t-test.\n",
    "- In statsmodels, p-values are in `res.pvalues`.\n",
    "- If you see `ModuleNotFoundError`, re-run the bootstrap cell.\n",
    "\n",
    "## Matching Guide\n",
    "- `docs/guides/00a_statistics_primer/05_hypothesis_testing_foundations.md`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "how-to-use",
   "metadata": {},
   "source": [
    "## How To Use This Notebook\n",
    "- Work section-by-section; don't skip the markdown.\n",
    "- Most code cells are incomplete on purpose: replace TODOs and `...`, then run.\n",
    "- After each section, write 2-4 sentences answering the interpretation prompts (what changed, why it matters).\n",
    "- Prefer `data/processed/*` if you have built the real datasets; otherwise use the bundled `data/sample/*` fallbacks.\n",
    "- Use the **Checkpoint (Self-Check)** section to catch mistakes early.\n",
    "- Use **Solutions (Reference)** only to unblock yourself; then re-implement without looking.\n",
    "- Use the matching guide (`docs/guides/00a_statistics_primer/05_hypothesis_testing_foundations.md`) for the math, assumptions, and deeper context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bootstrap-label",
   "metadata": {},
   "source": [
    "<a id=\"environment-bootstrap\"></a>\n",
    "## Environment Bootstrap\n",
    "Run this cell first. It makes the repo importable and defines common directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bootstrap",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "\n",
    "def find_repo_root(start: Path) -> Path:\n",
    "    p = start\n",
    "    for _ in range(8):\n",
    "        if (p / 'src').exists() and (p / 'docs').exists():\n",
    "            return p\n",
    "        p = p.parent\n",
    "    raise RuntimeError('Could not find repo root. Start Jupyter from the repo root.')\n",
    "\n",
    "\n",
    "PROJECT_ROOT = find_repo_root(Path.cwd())\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "RAW_DIR = DATA_DIR / 'raw'\n",
    "PROCESSED_DIR = DATA_DIR / 'processed'\n",
    "SAMPLE_DIR = DATA_DIR / 'sample'\n",
    "\n",
    "PROJECT_ROOT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "logic-header",
   "metadata": {},
   "source": [
    "<a id=\"the-logic-of-hypothesis-testing\"></a>\n",
    "## The logic of hypothesis testing\n",
    "\n",
    "### Goal\n",
    "Understand the framework of hypothesis testing before computing anything.\n",
    "\n",
    "### Why this matters in economics\n",
    "Every empirical economics paper makes claims like \"this policy raised wages\" or \"monetary\n",
    "tightening slowed GDP growth.\" Hypothesis testing is the formal framework for deciding\n",
    "whether data support such claims. Getting the logic wrong leads to false conclusions\n",
    "about which policies work and which do not.\n",
    "\n",
    "### The courtroom analogy\n",
    "\n",
    "Hypothesis testing works like a trial:\n",
    "\n",
    "| Courtroom | Hypothesis test |\n",
    "|---|---|\n",
    "| Defendant is **innocent until proven guilty** | The null hypothesis $H_0$ is assumed true until evidence says otherwise |\n",
    "| Prosecution presents evidence | We compute a test statistic from the data |\n",
    "| Jury asks: \"Is this evidence strong enough?\" | We compare the test statistic to a threshold (or compute a p-value) |\n",
    "| Verdict: **guilty** or **not guilty** | We **reject** $H_0$ or **fail to reject** $H_0$ |\n",
    "| \"Not guilty\" $\\neq$ \"innocent\" | \"Fail to reject\" $\\neq$ \"$H_0$ is true\" |\n",
    "\n",
    "The null hypothesis is the **status quo** or the **default** claim. In economics, it is\n",
    "typically \"no effect\":\n",
    "\n",
    "- *Does a minimum wage increase reduce employment?*  $\\rightarrow$  $H_0$: no effect on employment.\n",
    "- *Does this training program raise earnings?*  $\\rightarrow$  $H_0$: earnings are unchanged.\n",
    "- *Is GDP growth different from 2%?*  $\\rightarrow$  $H_0$: mean GDP growth = 2%.\n",
    "\n",
    "We **never** \"accept\" $H_0$. We either find enough evidence to reject it, or we don't.\n",
    "Absence of evidence is not evidence of absence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hypotheses-header",
   "metadata": {},
   "source": [
    "<a id=\"null-and-alternative-hypotheses\"></a>\n",
    "## Null and alternative hypotheses\n",
    "\n",
    "### Goal\n",
    "Practice writing $H_0$ and $H_1$ for various economic scenarios. Distinguish one-sided\n",
    "from two-sided tests.\n",
    "\n",
    "### Why this matters in economics\n",
    "Stating hypotheses precisely forces you to articulate what you are testing before you\n",
    "look at the data. Sloppy hypotheses lead to sloppy conclusions. One-sided vs. two-sided\n",
    "matters: using a one-sided test to halve the p-value without prior theoretical\n",
    "justification is a form of p-hacking.\n",
    "\n",
    "### Key concepts\n",
    "\n",
    "**Two-sided test** (most common):\n",
    "- $H_0: \\mu = \\mu_0$ (e.g., mean GDP growth = 2%)\n",
    "- $H_1: \\mu \\neq \\mu_0$\n",
    "\n",
    "**One-sided test** (requires strong theoretical motivation):\n",
    "- $H_0: \\mu \\leq \\mu_0$ vs. $H_1: \\mu > \\mu_0$, or\n",
    "- $H_0: \\mu \\geq \\mu_0$ vs. $H_1: \\mu < \\mu_0$\n",
    "\n",
    "In regression, the default test for each coefficient is:\n",
    "- $H_0: \\beta_j = 0$ (\"this variable has no linear effect\")\n",
    "- $H_1: \\beta_j \\neq 0$ (two-sided)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hypotheses-turn",
   "metadata": {},
   "source": [
    "### Your Turn (1): Write hypotheses for economic questions\n",
    "\n",
    "For each scenario below, write $H_0$ and $H_1$ in the markdown cell. State whether\n",
    "the test should be one-sided or two-sided, and justify your choice.\n",
    "\n",
    "1. **Mean GDP growth equals 2%.** You want to know if the long-run average quarterly\n",
    "   (annualized) GDP growth rate differs from 2%.\n",
    "\n",
    "2. **Unemployment is higher in recession quarters.** You believe recessions push\n",
    "   unemployment above its non-recession average.\n",
    "\n",
    "3. **The coefficient on education is positive.** In a wage regression, economic theory\n",
    "   predicts that more education leads to higher wages.\n",
    "\n",
    "4. **Federal funds rate has no effect on industrial production.** You are agnostic about\n",
    "   the direction.\n",
    "\n",
    "Write your answers below (double-click to edit):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hypotheses-answers",
   "metadata": {},
   "source": [
    "**Your answers:**\n",
    "\n",
    "1. $H_0$: ... &nbsp; $H_1$: ... &nbsp; (one-sided / two-sided because ...)\n",
    "\n",
    "2. $H_0$: ... &nbsp; $H_1$: ... &nbsp; (one-sided / two-sided because ...)\n",
    "\n",
    "3. $H_0$: ... &nbsp; $H_1$: ... &nbsp; (one-sided / two-sided because ...)\n",
    "\n",
    "4. $H_0$: ... &nbsp; $H_1$: ... &nbsp; (one-sided / two-sided because ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hypotheses-turn2",
   "metadata": {},
   "source": [
    "### Your Turn (2): One-sample t-test on GDP growth\n",
    "\n",
    "Load the macro sample data and test whether mean annualized GDP growth equals 2%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hypotheses-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "df = pd.read_csv(SAMPLE_DIR / 'macro_quarterly_sample.csv', index_col=0, parse_dates=True)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hypotheses-test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Extract the annualized GDP growth column\n",
    "gdp_growth = df['gdp_growth_qoq_annualized'].dropna()\n",
    "\n",
    "# TODO: Test H0: mean GDP growth = 2% (two-sided)\n",
    "t_stat, p_val = ...  # Hint: stats.ttest_1samp(gdp_growth, popmean=2.0)\n",
    "\n",
    "print(f'Sample mean: {gdp_growth.mean():.3f}')\n",
    "print(f't-statistic: {t_stat:.3f}')\n",
    "print(f'p-value:     {p_val:.4f}')\n",
    "print(f'n =          {len(gdp_growth)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hypotheses-interp",
   "metadata": {},
   "source": [
    "**Interpretation prompt:**\n",
    "- Can you reject $H_0: \\mu = 2\\%$ at the 5% level? At the 1% level?\n",
    "- What does it mean economically if you fail to reject?\n",
    "- Would you conclude that GDP growth IS exactly 2%? Why or why not?\n",
    "\n",
    "*Write 2-4 sentences here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test-stats-header",
   "metadata": {},
   "source": [
    "<a id=\"test-statistics-and-p-values\"></a>\n",
    "## Test statistics and p-values\n",
    "\n",
    "### Goal\n",
    "Build intuition for test statistics and p-values by simulating data under the null\n",
    "hypothesis and seeing how the test statistic behaves.\n",
    "\n",
    "### Why this matters in economics\n",
    "Understanding what a p-value actually measures (and what it does not) is the single\n",
    "most important statistical skill for any empirical economist. When you read that a\n",
    "coefficient has $p = 0.03$, you need to know exactly what that means.\n",
    "\n",
    "### The test statistic\n",
    "\n",
    "A test statistic measures: *how far is our estimate from the null hypothesis value,\n",
    "measured in standard errors?*\n",
    "\n",
    "$$\n",
    "t = \\frac{\\hat{\\theta} - \\theta_0}{\\widehat{SE}(\\hat{\\theta})}\n",
    "$$\n",
    "\n",
    "- $\\hat{\\theta}$: your estimate (e.g., sample mean, regression coefficient)\n",
    "- $\\theta_0$: the value under $H_0$ (often 0)\n",
    "- $\\widehat{SE}$: estimated standard error\n",
    "\n",
    "### The p-value\n",
    "\n",
    "The p-value is the probability of seeing a test statistic **this extreme or more extreme**,\n",
    "**if $H_0$ is true**.\n",
    "\n",
    "Small p-value $\\Rightarrow$ the data are unlikely under $H_0$ $\\Rightarrow$ evidence against $H_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test-stats-turn1",
   "metadata": {},
   "source": [
    "### Your Turn (1): Simulate the null distribution of a t-statistic\n",
    "\n",
    "Generate many samples where $H_0$ is true (true mean = 0), compute the t-statistic\n",
    "each time, and plot the distribution. Then shade the tails to visualize the p-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-stats-sim",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "n_obs = 30          # sample size per experiment\n",
    "n_sims = 10_000     # number of simulated experiments\n",
    "true_mean = 0.0     # H0 is true: population mean is 0\n",
    "\n",
    "t_stats = np.empty(n_sims)\n",
    "\n",
    "for i in range(n_sims):\n",
    "    # TODO: Generate a sample of size n_obs from N(true_mean, 1)\n",
    "    sample = ...\n",
    "    # TODO: Compute the t-statistic for H0: mu = 0\n",
    "    # t = (sample_mean - 0) / (sample_std / sqrt(n))\n",
    "    t_stats[i] = ...\n",
    "\n",
    "print(f'Mean of t-stats: {t_stats.mean():.3f} (should be ~0)')\n",
    "print(f'Std of t-stats:  {t_stats.std():.3f} (should be ~1)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-stats-plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot a histogram of t_stats and overlay the theoretical t-distribution\n",
    "fig, ax = plt.subplots(figsize=(9, 5))\n",
    "\n",
    "# Histogram of simulated t-statistics\n",
    "ax.hist(t_stats, bins=60, density=True, alpha=0.6, color='steelblue',\n",
    "        label='Simulated t-stats under $H_0$')\n",
    "\n",
    "# Overlay theoretical t-distribution with n-1 degrees of freedom\n",
    "x_grid = np.linspace(-4, 4, 300)\n",
    "ax.plot(x_grid, stats.t.pdf(x_grid, df=n_obs - 1), 'k-', lw=2,\n",
    "        label=f't({n_obs - 1}) theoretical')\n",
    "\n",
    "# TODO: Shade the rejection region for alpha = 0.05 (two-sided)\n",
    "# Hint: the critical value is stats.t.ppf(0.975, df=n_obs-1)\n",
    "t_crit = ...  # stats.t.ppf(0.975, df=n_obs - 1)\n",
    "ax.axvline(t_crit, color='red', ls='--', label=f'Critical value = +/- {t_crit:.2f}')\n",
    "ax.axvline(-t_crit, color='red', ls='--')\n",
    "\n",
    "# Shade tails\n",
    "x_right = x_grid[x_grid >= t_crit]\n",
    "x_left = x_grid[x_grid <= -t_crit]\n",
    "ax.fill_between(x_right, stats.t.pdf(x_right, df=n_obs - 1), alpha=0.3, color='red')\n",
    "ax.fill_between(x_left, stats.t.pdf(x_left, df=n_obs - 1), alpha=0.3, color='red')\n",
    "\n",
    "ax.set_xlabel('t-statistic')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('Distribution of t-statistics under $H_0$ (true mean = 0)')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test-stats-turn2",
   "metadata": {},
   "source": [
    "### Your Turn (2): Compute the p-value for one specific sample\n",
    "\n",
    "Draw one sample and manually compute the p-value two ways: (1) from the simulation\n",
    "and (2) from `scipy.stats`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-stats-pval",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw one sample (with a small real effect so the t-stat is interesting)\n",
    "rng2 = np.random.default_rng(99)\n",
    "one_sample = rng2.normal(loc=0.4, scale=1.0, size=n_obs)\n",
    "one_t = (one_sample.mean() - 0) / (one_sample.std(ddof=1) / np.sqrt(n_obs))\n",
    "print(f'Observed t-statistic: {one_t:.3f}')\n",
    "\n",
    "# TODO: Method 1 -- Simulation-based p-value\n",
    "# What fraction of the null t-stats are more extreme than one_t?\n",
    "p_sim = ...  # np.mean(np.abs(t_stats) >= np.abs(one_t))\n",
    "\n",
    "# TODO: Method 2 -- Exact p-value from scipy\n",
    "p_exact = ...  # 2 * stats.t.sf(np.abs(one_t), df=n_obs - 1)\n",
    "\n",
    "print(f'Simulation p-value: {p_sim:.4f}')\n",
    "print(f'Exact p-value:      {p_exact:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test-stats-interp",
   "metadata": {},
   "source": [
    "**Interpretation prompt:**\n",
    "- In your own words, what does the p-value you computed mean?\n",
    "- Why are the simulation and exact p-values close but not identical?\n",
    "- If $H_0$ is true, what fraction of t-statistics should fall in the red-shaded tails?\n",
    "\n",
    "*Write 2-4 sentences here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pval-not-header",
   "metadata": {},
   "source": [
    "<a id=\"what-p-values-are-not\"></a>\n",
    "## What p-values are NOT\n",
    "\n",
    "### Goal\n",
    "Confront and correct the most common misinterpretations of p-values.\n",
    "\n",
    "### Why this matters in economics\n",
    "Empirical economics papers are full of p-values. Misinterpreting them leads to\n",
    "over-confident policy recommendations, spurious \"significant\" findings, and a failure\n",
    "to distinguish statistical significance from economic significance. Getting this right\n",
    "is not pedantic -- it is essential.\n",
    "\n",
    "### Three things a p-value is NOT\n",
    "\n",
    "1. **NOT the probability that $H_0$ is true.**\n",
    "   The p-value is $P(\\text{data} \\mid H_0)$, not $P(H_0 \\mid \\text{data})$. Confusing\n",
    "   these is the \"prosecutor's fallacy.\"\n",
    "\n",
    "2. **NOT the probability the result is \"due to chance.\"**\n",
    "   The p-value is computed *assuming* $H_0$ is true. It does not tell you the probability\n",
    "   that randomness alone produced the result.\n",
    "\n",
    "3. **NOT a measure of effect size.**\n",
    "   A tiny, economically meaningless effect can have $p < 0.001$ with a large enough\n",
    "   sample. A large, important effect can have $p > 0.10$ with a small sample."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pval-not-turn1",
   "metadata": {},
   "source": [
    "### Your Turn (1): Tiny effect, small p-value (large n)\n",
    "\n",
    "Show that with enough data, even a trivially small effect becomes \"statistically\n",
    "significant.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pval-not-tiny",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(10)\n",
    "\n",
    "# A tiny true effect: mean = 0.01 (practically zero in most economic contexts)\n",
    "tiny_effect = 0.01\n",
    "\n",
    "# TODO: Generate a very large sample (n = 100_000) with this tiny mean\n",
    "large_sample = ...  # rng.normal(loc=tiny_effect, scale=1.0, size=100_000)\n",
    "\n",
    "# TODO: Run a t-test for H0: mu = 0\n",
    "t_stat, p_val = ...  # stats.ttest_1samp(large_sample, popmean=0.0)\n",
    "\n",
    "print(f'True effect:      {tiny_effect}')\n",
    "print(f'Sample mean:      {large_sample.mean():.5f}')\n",
    "print(f'Sample size:      {len(large_sample):,}')\n",
    "print(f't-statistic:      {t_stat:.2f}')\n",
    "print(f'p-value:          {p_val:.6f}')\n",
    "print(f'\"Significant\" at 5%? {p_val < 0.05}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pval-not-turn2",
   "metadata": {},
   "source": [
    "### Your Turn (2): Large effect, large p-value (small n)\n",
    "\n",
    "Show that with too little data, even a meaningful effect can fail to reach significance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pval-not-large",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(11)\n",
    "\n",
    "# A large true effect: mean = 0.8 (economically meaningful)\n",
    "large_effect = 0.8\n",
    "\n",
    "# TODO: Generate a small sample (n = 8) with this large mean\n",
    "small_sample = ...  # rng.normal(loc=large_effect, scale=2.0, size=8)\n",
    "\n",
    "# TODO: Run a t-test for H0: mu = 0\n",
    "t_stat, p_val = ...  # stats.ttest_1samp(small_sample, popmean=0.0)\n",
    "\n",
    "print(f'True effect:      {large_effect}')\n",
    "print(f'Sample mean:      {small_sample.mean():.3f}')\n",
    "print(f'Sample size:      {len(small_sample)}')\n",
    "print(f't-statistic:      {t_stat:.2f}')\n",
    "print(f'p-value:          {p_val:.4f}')\n",
    "print(f'\"Significant\" at 5%? {p_val < 0.05}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pval-not-interp",
   "metadata": {},
   "source": [
    "**Interpretation prompt:**\n",
    "- In scenario 1, the effect is tiny but \"significant.\" Would you advise a policymaker\n",
    "  to act on this finding? Why or why not?\n",
    "- In scenario 2, the effect is large but \"not significant.\" Does that mean there is no\n",
    "  effect? What is actually going on?\n",
    "- What additional information (beyond p-value) would you want before drawing conclusions?\n",
    "\n",
    "*Write 2-4 sentences here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "errors-header",
   "metadata": {},
   "source": [
    "<a id=\"type-i-and-type-ii-errors\"></a>\n",
    "## Type I and Type II errors\n",
    "\n",
    "### Goal\n",
    "Simulate Type I errors (false positives) and Type II errors (false negatives) to make\n",
    "the concepts concrete.\n",
    "\n",
    "### Why this matters in economics\n",
    "- **Type I error** (false positive): You conclude a policy works when it does not.\n",
    "  Wasted resources, misguided policy.\n",
    "- **Type II error** (false negative): You conclude a policy has no effect when it does.\n",
    "  A beneficial policy gets abandoned.\n",
    "\n",
    "The cost of each error depends on context. A drug safety test should be very cautious\n",
    "about Type I (approving a harmful drug). A preliminary program evaluation might worry\n",
    "more about Type II (killing a helpful program).\n",
    "\n",
    "### Decision table\n",
    "\n",
    "|  | $H_0$ is actually true | $H_0$ is actually false |\n",
    "|---|---|---|\n",
    "| **Reject $H_0$** | Type I error ($\\alpha$) | Correct (Power = $1 - \\beta$) |\n",
    "| **Fail to reject $H_0$** | Correct ($1 - \\alpha$) | Type II error ($\\beta$) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "errors-turn1",
   "metadata": {},
   "source": [
    "### Your Turn (1): Simulate Type I errors (false positives)\n",
    "\n",
    "Generate data where $H_0$ is true (mean = 0). Run 1,000 hypothesis tests. Count how\n",
    "many reject at $\\alpha = 0.05$. The count should be roughly 50 (5% of 1,000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "errors-type1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(55)\n",
    "\n",
    "n_tests = 1_000\n",
    "n_obs = 50\n",
    "alpha = 0.05\n",
    "rejections = 0\n",
    "\n",
    "for _ in range(n_tests):\n",
    "    # TODO: Generate data under H0: true mean = 0\n",
    "    sample = ...\n",
    "    # TODO: Run a t-test and check if p < alpha\n",
    "    _, p = ...\n",
    "    if p < alpha:\n",
    "        rejections += 1\n",
    "\n",
    "print(f'Rejections out of {n_tests}: {rejections}')\n",
    "print(f'Rejection rate:              {rejections / n_tests:.3f}')\n",
    "print(f'Expected (alpha):            {alpha}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "errors-turn2",
   "metadata": {},
   "source": [
    "### Your Turn (2): Simulate Type II errors (false negatives)\n",
    "\n",
    "Generate data where $H_0$ is false (there IS a real effect, e.g., mean = 0.3). Run\n",
    "1,000 tests. Count how many *fail* to reject. These are Type II errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "errors-type2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(56)\n",
    "\n",
    "n_tests = 1_000\n",
    "n_obs = 50\n",
    "true_effect = 0.3   # H0 is FALSE; the true mean is 0.3\n",
    "alpha = 0.05\n",
    "failures_to_reject = 0\n",
    "\n",
    "for _ in range(n_tests):\n",
    "    # TODO: Generate data with mean = true_effect\n",
    "    sample = ...\n",
    "    # TODO: Test H0: mu = 0\n",
    "    _, p = ...\n",
    "    if p >= alpha:\n",
    "        failures_to_reject += 1\n",
    "\n",
    "print(f'Type II errors out of {n_tests}: {failures_to_reject}')\n",
    "print(f'Type II error rate (beta):      {failures_to_reject / n_tests:.3f}')\n",
    "print(f'Power (1 - beta):               {1 - failures_to_reject / n_tests:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "errors-interp",
   "metadata": {},
   "source": [
    "**Interpretation prompt:**\n",
    "- Did the Type I error rate match $\\alpha = 0.05$? Why or why not?\n",
    "- What was the power against the effect of 0.3? Is that good enough?\n",
    "- If you were evaluating a government program with n=50 observations and a modest true\n",
    "  effect, what is the risk of concluding \"no effect\"?\n",
    "\n",
    "*Write 2-4 sentences here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "power-header",
   "metadata": {},
   "source": [
    "<a id=\"statistical-power\"></a>\n",
    "## Statistical power\n",
    "\n",
    "### Goal\n",
    "Understand what determines statistical power and build a power curve through simulation.\n",
    "\n",
    "### Why this matters in economics\n",
    "Many empirical studies are underpowered: they have too few observations to detect the\n",
    "effects they are looking for. This leads to two problems: (1) real effects go undetected,\n",
    "and (2) the effects that *are* detected tend to be exaggerated (\"winner's curse\").\n",
    "Power analysis should be done *before* collecting data, not after.\n",
    "\n",
    "### What determines power?\n",
    "\n",
    "Power = $P$(reject $H_0$ $\\mid$ $H_0$ is false) = $1 - \\beta$.\n",
    "\n",
    "Power increases when:\n",
    "- **Effect size** is larger (easier to detect)\n",
    "- **Sample size** is larger (more information)\n",
    "- **Noise ($\\sigma$)** is smaller (cleaner signal)\n",
    "- **$\\alpha$** is larger (more lenient threshold -- but more false positives)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "power-turn1",
   "metadata": {},
   "source": [
    "### Your Turn (1): Build a power curve (varying sample size)\n",
    "\n",
    "For a fixed small effect (mean = 0.3, $\\sigma$ = 1), vary $n$ from 10 to 500 and\n",
    "compute the proportion of rejections at each sample size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "power-curve",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(77)\n",
    "\n",
    "true_effect = 0.3\n",
    "sigma = 1.0\n",
    "alpha = 0.05\n",
    "n_sims = 2_000\n",
    "\n",
    "sample_sizes = [10, 20, 30, 50, 75, 100, 150, 200, 300, 500]\n",
    "power_values = []\n",
    "\n",
    "for n in sample_sizes:\n",
    "    rejections = 0\n",
    "    for _ in range(n_sims):\n",
    "        # TODO: Simulate data with the true effect\n",
    "        sample = ...  # rng.normal(loc=true_effect, scale=sigma, size=n)\n",
    "        # TODO: Test H0: mu = 0 and count rejections\n",
    "        _, p = ...  # stats.ttest_1samp(sample, popmean=0.0)\n",
    "        if p < alpha:\n",
    "            rejections += 1\n",
    "    power_values.append(rejections / n_sims)\n",
    "\n",
    "power_df = pd.DataFrame({'n': sample_sizes, 'power': power_values})\n",
    "print(power_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "power-plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot the power curve\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "ax.plot(power_df['n'], power_df['power'], 'o-', color='steelblue', lw=2)\n",
    "ax.axhline(0.80, color='red', ls='--', alpha=0.7, label='Conventional power = 0.80')\n",
    "ax.axhline(alpha, color='gray', ls=':', alpha=0.5, label=f'alpha = {alpha}')\n",
    "\n",
    "ax.set_xlabel('Sample Size (n)')\n",
    "ax.set_ylabel('Power (rejection rate)')\n",
    "ax.set_title(f'Power Curve: detecting effect = {true_effect}, sigma = {sigma}')\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "power-turn2",
   "metadata": {},
   "source": [
    "### Your Turn (2): How much data do you need?\n",
    "\n",
    "Read off the power curve: approximately what sample size do you need to achieve 80%\n",
    "power for this effect size?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "power-required-n",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Find the smallest n in your simulation where power >= 0.80\n",
    "# Hint: filter power_df\n",
    "required_n = ...  # power_df.loc[power_df['power'] >= 0.80, 'n'].iloc[0]\n",
    "print(f'Minimum n for 80% power (from simulation): ~{required_n}')\n",
    "\n",
    "# Cross-check with analytical formula (optional):\n",
    "# For a one-sample z-test: n = ((z_alpha/2 + z_beta) * sigma / delta)^2\n",
    "from scipy.stats import norm\n",
    "z_alpha2 = norm.ppf(0.975)\n",
    "z_beta = norm.ppf(0.80)\n",
    "n_analytical = ((z_alpha2 + z_beta) * sigma / true_effect) ** 2\n",
    "print(f'Analytical approximation:                   ~{n_analytical:.0f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "power-interp",
   "metadata": {},
   "source": [
    "**Interpretation prompt:**\n",
    "- What happens to the power curve if the true effect is even smaller (e.g., 0.1)?\n",
    "- Many macro datasets have ~80-200 quarterly observations. Given your power curve,\n",
    "  what size effects can you reliably detect?\n",
    "- Why should power analysis be done *before* collecting data, not after?\n",
    "\n",
    "*Write 2-4 sentences here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multiple-header",
   "metadata": {},
   "source": [
    "<a id=\"significance-levels-and-multiple-testing\"></a>\n",
    "## Significance levels and multiple testing\n",
    "\n",
    "### Goal\n",
    "Understand why $\\alpha = 0.05$ is a convention (not a law) and see how running many\n",
    "tests inflates false positives.\n",
    "\n",
    "### Why this matters in economics\n",
    "Researchers often estimate many specifications, test many variables, or examine many\n",
    "subgroups. If you run 20 tests at $\\alpha = 0.05$ and all null hypotheses are true,\n",
    "you *expect* one false positive. Reporting only the significant result is p-hacking.\n",
    "This is a major issue in empirical economics and one reason journals increasingly\n",
    "require pre-registration of studies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multiple-turn1",
   "metadata": {},
   "source": [
    "### Your Turn (1): Simulate the multiple testing problem\n",
    "\n",
    "Run 20 independent tests where $H_0$ is true for all of them. Repeat this experiment\n",
    "many times. How often does at least one test reject?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multiple-sim",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(88)\n",
    "\n",
    "n_experiments = 5_000\n",
    "n_tests_per_experiment = 20\n",
    "n_obs = 50\n",
    "alpha = 0.05\n",
    "\n",
    "any_rejection_count = 0\n",
    "total_rejections = 0\n",
    "\n",
    "for _ in range(n_experiments):\n",
    "    p_values = []\n",
    "    for _ in range(n_tests_per_experiment):\n",
    "        # TODO: Generate pure noise (H0 true for every test)\n",
    "        sample = ...\n",
    "        _, p = ...\n",
    "        p_values.append(p)\n",
    "\n",
    "    # TODO: Count how many of the 20 tests rejected\n",
    "    n_rejected = ...  # sum(p < alpha for p in p_values)\n",
    "    total_rejections += n_rejected\n",
    "    if n_rejected > 0:\n",
    "        any_rejection_count += 1\n",
    "\n",
    "print(f'Experiments with at least one false positive: '\n",
    "      f'{any_rejection_count}/{n_experiments} = '\n",
    "      f'{any_rejection_count / n_experiments:.3f}')\n",
    "print(f'Expected: 1 - (1 - {alpha})^{n_tests_per_experiment} = '\n",
    "      f'{1 - (1 - alpha)**n_tests_per_experiment:.3f}')\n",
    "print(f'Average false positives per experiment: '\n",
    "      f'{total_rejections / n_experiments:.2f}')\n",
    "print(f'Expected: {n_tests_per_experiment * alpha:.1f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multiple-turn2",
   "metadata": {},
   "source": [
    "### Your Turn (2): Bonferroni correction\n",
    "\n",
    "The simplest fix: if you run $m$ tests, use $\\alpha / m$ as your significance\n",
    "threshold. Repeat the simulation above with the Bonferroni-corrected threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multiple-bonf",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(89)\n",
    "\n",
    "alpha_bonf = alpha / n_tests_per_experiment\n",
    "print(f'Bonferroni-corrected alpha: {alpha_bonf:.4f}')\n",
    "\n",
    "any_rejection_bonf = 0\n",
    "\n",
    "for _ in range(n_experiments):\n",
    "    p_values = []\n",
    "    for _ in range(n_tests_per_experiment):\n",
    "        # TODO: Generate pure noise, run t-test\n",
    "        sample = ...\n",
    "        _, p = ...\n",
    "        p_values.append(p)\n",
    "\n",
    "    # TODO: Check if any p < alpha_bonf\n",
    "    if ...:\n",
    "        any_rejection_bonf += 1\n",
    "\n",
    "print(f'\\nWith Bonferroni correction:')\n",
    "print(f'Experiments with at least one false positive: '\n",
    "      f'{any_rejection_bonf}/{n_experiments} = '\n",
    "      f'{any_rejection_bonf / n_experiments:.3f}')\n",
    "print(f'Target family-wise error rate: {alpha}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multiple-interp",
   "metadata": {},
   "source": [
    "**Interpretation prompt:**\n",
    "- Without correction, what fraction of experiments had at least one false positive?\n",
    "  How does that compare to the naive expectation of 5%?\n",
    "- After Bonferroni correction, did the family-wise error rate return to ~5%?\n",
    "- What is the downside of Bonferroni correction? (Hint: think about power.)\n",
    "\n",
    "*Write 2-4 sentences here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regression-header",
   "metadata": {},
   "source": [
    "<a id=\"connecting-to-regression-output\"></a>\n",
    "## Connecting to regression output\n",
    "\n",
    "### Goal\n",
    "See that every t-statistic and p-value in a `statsmodels` regression summary is a\n",
    "hypothesis test for $H_0: \\beta_j = 0$.\n",
    "\n",
    "### Why this matters in economics\n",
    "When you run a regression in practice, you do not manually compute t-statistics. The\n",
    "software does it for you. But you need to know *what* is being tested to interpret the\n",
    "output correctly. Every coefficient in the summary table has a t-stat and p-value that\n",
    "answer: \"Is there evidence that this variable has a non-zero linear relationship with\n",
    "the outcome, conditional on the other variables in the model?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regression-turn1",
   "metadata": {},
   "source": [
    "### Your Turn (1): Fit a regression and read the hypothesis tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regression-fit",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# Use the macro quarterly sample\n",
    "df = pd.read_csv(SAMPLE_DIR / 'macro_quarterly_sample.csv', index_col=0, parse_dates=True)\n",
    "\n",
    "# Regression: GDP growth ~ UNRATE + FEDFUNDS + INDPRO growth\n",
    "# We'll create a simple INDPRO growth proxy from the lag\n",
    "df['indpro_growth'] = (df['INDPRO'] - df['INDPRO_lag1']) / df['INDPRO_lag1'] * 100\n",
    "\n",
    "features = ['UNRATE', 'FEDFUNDS', 'indpro_growth']\n",
    "target = 'gdp_growth_qoq_annualized'\n",
    "\n",
    "reg_df = df[features + [target]].dropna()\n",
    "\n",
    "X = sm.add_constant(reg_df[features])\n",
    "y = reg_df[target]\n",
    "\n",
    "res = sm.OLS(y, X).fit()\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regression-turn2",
   "metadata": {},
   "source": [
    "### Your Turn (2): Extract and interpret the hypothesis tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regression-extract",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Extract coefficients, standard errors, t-stats, and p-values\n",
    "coefs = ...      # res.params\n",
    "se = ...         # res.bse\n",
    "t_stats = ...    # res.tvalues\n",
    "p_vals = ...     # res.pvalues\n",
    "\n",
    "# TODO: Create a summary DataFrame\n",
    "summary_df = pd.DataFrame({\n",
    "    'coef': coefs,\n",
    "    'std_err': se,\n",
    "    't_stat': t_stats,\n",
    "    'p_value': p_vals,\n",
    "    'significant_5pct': ...  # p_vals < 0.05\n",
    "})\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regression-verify",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Verify that t = coef / std_err (they should match)\n",
    "manual_t = ...  # coefs / se\n",
    "print('Manual t-stats:')\n",
    "print(manual_t.round(4))\n",
    "print('\\nstatsmodels t-stats:')\n",
    "print(t_stats.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regression-interp",
   "metadata": {},
   "source": [
    "**Interpretation prompt:**\n",
    "- Which coefficients are statistically significant at the 5% level?\n",
    "- For each significant coefficient, write a careful one-sentence interpretation.\n",
    "  (Remember: \"significant\" means we reject $H_0: \\beta_j = 0$, not that the effect\n",
    "  is large or important.)\n",
    "- For any non-significant coefficients: can you conclude the variable has no effect?\n",
    "  Why or why not? (Hint: consider power.)\n",
    "- How would using HAC standard errors change the p-values?\n",
    "\n",
    "*Write 2-4 sentences here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "later-header",
   "metadata": {},
   "source": [
    "## Where This Shows Up Later\n",
    "\n",
    "- **02_regression**: Every coefficient t-test and p-value is the hypothesis test you\n",
    "  learned here. F-tests for joint significance test whether a *group* of coefficients\n",
    "  are all zero.\n",
    "- **02_regression/04a_residual_diagnostics**: Breusch-Pagan and White tests for\n",
    "  heteroskedasticity are hypothesis tests where $H_0$: homoskedastic errors.\n",
    "- **07_time_series_econ/00_stationarity_unit_roots**: The Augmented Dickey-Fuller (ADF)\n",
    "  test has $H_0$: unit root (non-stationary). Rejecting means evidence of stationarity.\n",
    "- **06_causal**: Hausman tests, overidentification tests, and pre-trend tests all follow\n",
    "  the same logic: specify $H_0$, compute a test statistic, evaluate the p-value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "checkpoint-header",
   "metadata": {},
   "source": [
    "<a id=\"checkpoint-self-check\"></a>\n",
    "## Checkpoint (Self-Check)\n",
    "\n",
    "Run the assertions below and answer the conceptual questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "checkpoint",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity checks on your work\n",
    "\n",
    "# 1. Type I error rate should be close to alpha\n",
    "# (from your Type I simulation -- paste your rejection rate here)\n",
    "# assert abs(your_type1_rate - 0.05) < 0.02, 'Type I rate too far from alpha'\n",
    "\n",
    "# 2. Power should increase with sample size\n",
    "# assert power_df['power'].is_monotonic_increasing, 'Power should increase with n'\n",
    "\n",
    "# 3. t-stat = coef / se in the regression\n",
    "# assert np.allclose(manual_t, t_stats, atol=1e-3), 't-stats should match'\n",
    "\n",
    "# TODO: Uncomment and run the asserts above once you have completed the exercises.\n",
    "# Write 2-3 sentences: what does each check verify?\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extensions",
   "metadata": {},
   "source": [
    "## Extensions (Optional)\n",
    "\n",
    "- **Power for different effect sizes**: Re-run the power simulation for effect sizes\n",
    "  of 0.1, 0.2, 0.3, and 0.5 and overlay the power curves on one plot.\n",
    "- **Two-sample test**: Compare mean unemployment in recession vs. non-recession quarters\n",
    "  using `stats.ttest_ind`. What is the effect size? What does the p-value tell you?\n",
    "- **FDR correction**: Instead of Bonferroni, try the Benjamini-Hochberg procedure\n",
    "  (`statsmodels.stats.multitest.multipletests`). How does it compare?\n",
    "- **HAC standard errors**: Re-run the regression with\n",
    "  `res.get_robustcov_results(cov_type='HAC', cov_kwds={'maxlags': 4})` and compare\n",
    "  p-values. Which coefficients change significance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reflection",
   "metadata": {},
   "source": [
    "## Reflection\n",
    "\n",
    "- What implicit assumptions does a t-test make? Which of those assumptions might be\n",
    "  violated in macroeconomic time series data?\n",
    "- If you were reviewing a paper that reports 20 regression specifications and highlights\n",
    "  the one with the smallest p-value, what would you be concerned about?\n",
    "- In your own words, explain the difference between statistical significance and\n",
    "  economic significance. Give an example where they diverge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solutions-header",
   "metadata": {},
   "source": [
    "<a id=\"solutions-reference\"></a>\n",
    "## Solutions (Reference)\n",
    "\n",
    "Try the TODOs first. Use these only to unblock yourself or to compare approaches.\n",
    "\n",
    "<details><summary>Solution: Null and alternative hypotheses (writing exercise)</summary>\n",
    "\n",
    "1. **Mean GDP growth = 2%**: $H_0: \\mu = 2\\%$, $H_1: \\mu \\neq 2\\%$ (two-sided,\n",
    "   because growth could be above or below 2%).\n",
    "\n",
    "2. **Unemployment higher in recessions**: $H_0: \\mu_{\\text{rec}} \\leq \\mu_{\\text{non-rec}}$,\n",
    "   $H_1: \\mu_{\\text{rec}} > \\mu_{\\text{non-rec}}$ (one-sided, because theory and common\n",
    "   sense predict the direction).\n",
    "\n",
    "3. **Coefficient on education is positive**: $H_0: \\beta_{\\text{educ}} \\leq 0$,\n",
    "   $H_1: \\beta_{\\text{educ}} > 0$ (one-sided, justified by human capital theory).\n",
    "\n",
    "4. **Fed funds rate and industrial production**: $H_0: \\beta_{\\text{ff}} = 0$,\n",
    "   $H_1: \\beta_{\\text{ff}} \\neq 0$ (two-sided, because you are agnostic about direction).\n",
    "\n",
    "</details>\n",
    "\n",
    "<details><summary>Solution: Null and alternative hypotheses (t-test code)</summary>\n",
    "\n",
    "```python\n",
    "gdp_growth = df['gdp_growth_qoq_annualized'].dropna()\n",
    "t_stat, p_val = stats.ttest_1samp(gdp_growth, popmean=2.0)\n",
    "print(f'Sample mean: {gdp_growth.mean():.3f}')\n",
    "print(f't-statistic: {t_stat:.3f}')\n",
    "print(f'p-value:     {p_val:.4f}')\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<details><summary>Solution: Test statistics and p-values (simulation)</summary>\n",
    "\n",
    "```python\n",
    "rng = np.random.default_rng(42)\n",
    "n_obs = 30\n",
    "n_sims = 10_000\n",
    "true_mean = 0.0\n",
    "\n",
    "t_stats = np.empty(n_sims)\n",
    "for i in range(n_sims):\n",
    "    sample = rng.normal(loc=true_mean, scale=1.0, size=n_obs)\n",
    "    sample_mean = sample.mean()\n",
    "    sample_se = sample.std(ddof=1) / np.sqrt(n_obs)\n",
    "    t_stats[i] = (sample_mean - 0) / sample_se\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<details><summary>Solution: Test statistics and p-values (plot and p-value computation)</summary>\n",
    "\n",
    "```python\n",
    "t_crit = stats.t.ppf(0.975, df=n_obs - 1)\n",
    "\n",
    "# Simulation-based p-value\n",
    "p_sim = np.mean(np.abs(t_stats) >= np.abs(one_t))\n",
    "\n",
    "# Exact p-value\n",
    "p_exact = 2 * stats.t.sf(np.abs(one_t), df=n_obs - 1)\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<details><summary>Solution: What p-values are NOT</summary>\n",
    "\n",
    "```python\n",
    "# Tiny effect, large n\n",
    "rng = np.random.default_rng(10)\n",
    "large_sample = rng.normal(loc=0.01, scale=1.0, size=100_000)\n",
    "t_stat, p_val = stats.ttest_1samp(large_sample, popmean=0.0)\n",
    "# p-value will be very small despite effect being negligible\n",
    "\n",
    "# Large effect, small n\n",
    "rng = np.random.default_rng(11)\n",
    "small_sample = rng.normal(loc=0.8, scale=2.0, size=8)\n",
    "t_stat, p_val = stats.ttest_1samp(small_sample, popmean=0.0)\n",
    "# p-value may be large despite effect being meaningful\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<details><summary>Solution: Type I and Type II errors</summary>\n",
    "\n",
    "```python\n",
    "# Type I errors (H0 true)\n",
    "rng = np.random.default_rng(55)\n",
    "n_tests, n_obs, alpha = 1_000, 50, 0.05\n",
    "rejections = 0\n",
    "for _ in range(n_tests):\n",
    "    sample = rng.normal(loc=0, scale=1.0, size=n_obs)\n",
    "    _, p = stats.ttest_1samp(sample, popmean=0.0)\n",
    "    if p < alpha:\n",
    "        rejections += 1\n",
    "print(f'Type I rate: {rejections / n_tests:.3f}')  # should be ~0.05\n",
    "\n",
    "# Type II errors (H0 false, true effect = 0.3)\n",
    "rng = np.random.default_rng(56)\n",
    "failures_to_reject = 0\n",
    "for _ in range(n_tests):\n",
    "    sample = rng.normal(loc=0.3, scale=1.0, size=n_obs)\n",
    "    _, p = stats.ttest_1samp(sample, popmean=0.0)\n",
    "    if p >= alpha:\n",
    "        failures_to_reject += 1\n",
    "print(f'Type II rate: {failures_to_reject / n_tests:.3f}')\n",
    "print(f'Power: {1 - failures_to_reject / n_tests:.3f}')\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<details><summary>Solution: Statistical power (power curve)</summary>\n",
    "\n",
    "```python\n",
    "rng = np.random.default_rng(77)\n",
    "true_effect, sigma, alpha, n_sims = 0.3, 1.0, 0.05, 2_000\n",
    "sample_sizes = [10, 20, 30, 50, 75, 100, 150, 200, 300, 500]\n",
    "power_values = []\n",
    "\n",
    "for n in sample_sizes:\n",
    "    rejections = 0\n",
    "    for _ in range(n_sims):\n",
    "        sample = rng.normal(loc=true_effect, scale=sigma, size=n)\n",
    "        _, p = stats.ttest_1samp(sample, popmean=0.0)\n",
    "        if p < alpha:\n",
    "            rejections += 1\n",
    "    power_values.append(rejections / n_sims)\n",
    "\n",
    "# Minimum n for 80% power\n",
    "power_df = pd.DataFrame({'n': sample_sizes, 'power': power_values})\n",
    "required_n = power_df.loc[power_df['power'] >= 0.80, 'n'].iloc[0]\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<details><summary>Solution: Multiple testing</summary>\n",
    "\n",
    "```python\n",
    "rng = np.random.default_rng(88)\n",
    "n_experiments, n_tests_per = 5_000, 20\n",
    "n_obs, alpha = 50, 0.05\n",
    "any_rejection_count = 0\n",
    "\n",
    "for _ in range(n_experiments):\n",
    "    p_values = []\n",
    "    for _ in range(n_tests_per):\n",
    "        sample = rng.normal(loc=0, scale=1.0, size=n_obs)\n",
    "        _, p = stats.ttest_1samp(sample, popmean=0.0)\n",
    "        p_values.append(p)\n",
    "    if any(p < alpha for p in p_values):\n",
    "        any_rejection_count += 1\n",
    "\n",
    "# Bonferroni\n",
    "rng = np.random.default_rng(89)\n",
    "alpha_bonf = alpha / n_tests_per\n",
    "any_rejection_bonf = 0\n",
    "for _ in range(n_experiments):\n",
    "    p_values = []\n",
    "    for _ in range(n_tests_per):\n",
    "        sample = rng.normal(loc=0, scale=1.0, size=n_obs)\n",
    "        _, p = stats.ttest_1samp(sample, popmean=0.0)\n",
    "        p_values.append(p)\n",
    "    if any(p < alpha_bonf for p in p_values):\n",
    "        any_rejection_bonf += 1\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<details><summary>Solution: Connecting to regression output</summary>\n",
    "\n",
    "```python\n",
    "import statsmodels.api as sm\n",
    "\n",
    "df = pd.read_csv(SAMPLE_DIR / 'macro_quarterly_sample.csv', index_col=0, parse_dates=True)\n",
    "df['indpro_growth'] = (df['INDPRO'] - df['INDPRO_lag1']) / df['INDPRO_lag1'] * 100\n",
    "\n",
    "features = ['UNRATE', 'FEDFUNDS', 'indpro_growth']\n",
    "target = 'gdp_growth_qoq_annualized'\n",
    "reg_df = df[features + [target]].dropna()\n",
    "\n",
    "X = sm.add_constant(reg_df[features])\n",
    "y = reg_df[target]\n",
    "res = sm.OLS(y, X).fit()\n",
    "\n",
    "coefs = res.params\n",
    "se = res.bse\n",
    "t_stats = res.tvalues\n",
    "p_vals = res.pvalues\n",
    "\n",
    "summary_df = pd.DataFrame({\n",
    "    'coef': coefs,\n",
    "    'std_err': se,\n",
    "    't_stat': t_stats,\n",
    "    'p_value': p_vals,\n",
    "    'significant_5pct': p_vals < 0.05\n",
    "})\n",
    "\n",
    "# Verify t = coef / se\n",
    "manual_t = coefs / se\n",
    "assert np.allclose(manual_t, t_stats, atol=1e-10)\n",
    "```\n",
    "\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 5,
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}