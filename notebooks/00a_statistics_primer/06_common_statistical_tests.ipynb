{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-title",
   "metadata": {},
   "source": [
    "# 06 Common Statistical Tests\n",
    "\n",
    "Putting hypothesis testing into practice: t-tests, chi-squared tests, F-tests, and knowing which test fits which question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-toc",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [Choosing the right test](#choosing-the-right-test)\n",
    "- [One-sample t-test](#one-sample-t-test)\n",
    "- [Two-sample t-test](#two-sample-t-test)\n",
    "- [Paired t-test](#paired-t-test)\n",
    "- [Chi-squared test of independence](#chi-squared-test-of-independence)\n",
    "- [Chi-squared goodness-of-fit](#chi-squared-goodness-of-fit)\n",
    "- [F-test for equality of variances](#f-test-for-equality-of-variances)\n",
    "- [F-test in regression](#f-test-in-regression)\n",
    "- [Checkpoint (Self-Check)](#checkpoint-self-check)\n",
    "- [Solutions (Reference)](#solutions-reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-why",
   "metadata": {},
   "source": [
    "## Why This Notebook Matters\n",
    "The previous notebook covered the logic of hypothesis testing. This notebook puts that\n",
    "logic into practice with the specific tests you will encounter throughout this project.\n",
    "Each test answers a different kind of question, and choosing the wrong test gives\n",
    "misleading answers. By the end, you will have a practical toolkit for the most common\n",
    "statistical comparisons in economics.\n",
    "\n",
    "## Prerequisites (Quick Self-Check)\n",
    "- Completed notebooks 00-05 (especially hypothesis testing foundations).\n",
    "- Understanding of p-values, Type I/II errors, and test statistics.\n",
    "- Familiarity with the t, chi-squared, and F distributions.\n",
    "\n",
    "## What You Will Produce\n",
    "- (no file output; learning/analysis notebook)\n",
    "\n",
    "## Success Criteria\n",
    "- You can choose the appropriate test for a given question and data type.\n",
    "- You can run and interpret t-tests, chi-squared tests, and F-tests in Python.\n",
    "- You can read the F-statistic from a regression summary and explain what it tests.\n",
    "\n",
    "## Common Pitfalls\n",
    "- Using a two-sample t-test when data is paired (losing power).\n",
    "- Applying chi-squared tests to tables with expected counts below 5.\n",
    "- Confusing the regression F-test (joint significance) with the F-test for equal variances.\n",
    "- Reporting \"significant\" without specifying the test, alpha level, or what was tested.\n",
    "\n",
    "## Quick Fixes (When You Get Stuck)\n",
    "- `scipy.stats.ttest_1samp(x, popmean)` -- one-sample t-test.\n",
    "- `scipy.stats.ttest_ind(x, y, equal_var=False)` -- Welch's two-sample t-test.\n",
    "- `scipy.stats.ttest_rel(x, y)` -- paired t-test.\n",
    "- `scipy.stats.chi2_contingency(table)` -- chi-squared test of independence.\n",
    "- `scipy.stats.levene(x, y)` -- test for equal variances.\n",
    "- If you see `ModuleNotFoundError`, re-run the bootstrap cell.\n",
    "\n",
    "## Matching Guide\n",
    "- `docs/guides/00a_statistics_primer/06_common_statistical_tests.md`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-howto",
   "metadata": {},
   "source": [
    "## How To Use This Notebook\n",
    "- Work section-by-section; don't skip the markdown.\n",
    "- Most code cells are incomplete on purpose: replace TODOs and `...`, then run.\n",
    "- After each section, write 2-4 sentences answering the interpretation prompts (what changed, why it matters).\n",
    "- Prefer `data/processed/*` if you have built the real datasets; otherwise use the bundled `data/sample/*` fallbacks.\n",
    "- Use the **Checkpoint (Self-Check)** section to catch mistakes early.\n",
    "- Use **Solutions (Reference)** only to unblock yourself; then re-implement without looking.\n",
    "- Use the matching guide (`docs/guides/00a_statistics_primer/06_common_statistical_tests.md`) for the math, assumptions, and deeper context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-bootstrap-header",
   "metadata": {},
   "source": [
    "<a id=\"environment-bootstrap\"></a>\n",
    "## Environment Bootstrap\n",
    "Run this cell first. It makes the repo importable and defines common directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-bootstrap",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "\n",
    "def find_repo_root(start: Path) -> Path:\n",
    "    p = start\n",
    "    for _ in range(8):\n",
    "        if (p / 'src').exists() and (p / 'docs').exists():\n",
    "            return p\n",
    "        p = p.parent\n",
    "    raise RuntimeError('Could not find repo root. Start Jupyter from the repo root.')\n",
    "\n",
    "\n",
    "PROJECT_ROOT = find_repo_root(Path.cwd())\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "RAW_DIR = DATA_DIR / 'raw'\n",
    "PROCESSED_DIR = DATA_DIR / 'processed'\n",
    "SAMPLE_DIR = DATA_DIR / 'sample'\n",
    "\n",
    "PROJECT_ROOT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-load-header",
   "metadata": {},
   "source": [
    "## Load the sample data\n",
    "\n",
    "We will use `macro_quarterly_sample.csv` throughout this notebook.\n",
    "This dataset contains quarterly US macroeconomic indicators including GDP growth,\n",
    "unemployment rate, the federal funds rate, CPI, industrial production, and\n",
    "a recession indicator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "df = pd.read_csv(SAMPLE_DIR / 'macro_quarterly_sample.csv', index_col=0, parse_dates=True)\n",
    "print('Shape:', df.shape)\n",
    "print('Columns:', list(df.columns))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-choosing-header",
   "metadata": {},
   "source": [
    "<a id=\"choosing-the-right-test\"></a>\n",
    "## Choosing the Right Test\n",
    "\n",
    "### Goal\n",
    "Develop a mental decision tree for selecting the appropriate statistical test.\n",
    "\n",
    "### Why this matters in economics\n",
    "Economists routinely test hypotheses about means, variances, proportions, and regression\n",
    "coefficients. Each question calls for a specific test with its own assumptions. Picking\n",
    "the wrong test -- for example, using an unpaired t-test for before/after data on the\n",
    "same regions -- wastes statistical power or, worse, gives invalid p-values.\n",
    "\n",
    "### Decision tree\n",
    "\n",
    "Ask yourself three questions:\n",
    "\n",
    "1. **What is your question?** (comparing means, testing proportions, testing variances, testing regression coefficients)\n",
    "2. **What type of data do you have?** (continuous, categorical/counts, paired)\n",
    "3. **How many groups?** (one sample vs a reference value, two samples, more than two)\n",
    "\n",
    "### Reference table\n",
    "\n",
    "| Question | Test | Python function |\n",
    "|---|---|---|\n",
    "| Is a single mean different from a hypothesized value? | **One-sample t-test** | `scipy.stats.ttest_1samp` |\n",
    "| Are two group means different? | **Two-sample t-test** (Welch's) | `scipy.stats.ttest_ind(equal_var=False)` |\n",
    "| Are paired observations different on average? | **Paired t-test** | `scipy.stats.ttest_rel` |\n",
    "| Are proportions / counts independent? | **Chi-squared test of independence** | `scipy.stats.chi2_contingency` |\n",
    "| Does a distribution match a theoretical one? | **Chi-squared goodness-of-fit** | `scipy.stats.chisquare` |\n",
    "| Do two groups have equal variance? | **Levene's test** (robust F-test) | `scipy.stats.levene` |\n",
    "| Are multiple group means all equal? | **ANOVA (F-test)** | `scipy.stats.f_oneway` |\n",
    "| Is a single regression coefficient = 0? | **t-test** (from regression) | `res.summary()` t-stats |\n",
    "| Are several regression coefficients jointly = 0? | **F-test** (from regression) | `res.f_test()` / `res.fvalue` |\n",
    "\n",
    "We will work through each of these in the sections below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-onesample-header",
   "metadata": {},
   "source": [
    "<a id=\"one-sample-t-test\"></a>\n",
    "## One-Sample t-Test\n",
    "\n",
    "### Goal\n",
    "Test whether the mean of a single sample differs from a known or hypothesized value.\n",
    "\n",
    "### Why this matters in economics\n",
    "A central bank might target 2% annual GDP growth. An analyst asks: \"Is the observed\n",
    "mean GDP growth rate significantly different from 2%?\" The one-sample t-test\n",
    "formalizes this comparison. The test statistic is:\n",
    "\n",
    "$$t = \\frac{\\bar{x} - \\mu_0}{s / \\sqrt{n}}$$\n",
    "\n",
    "where $\\bar{x}$ is the sample mean, $\\mu_0$ is the hypothesized value (2.0),\n",
    "$s$ is the sample standard deviation, and $n$ is the sample size.\n",
    "Under $H_0$, $t$ follows a $t$-distribution with $n-1$ degrees of freedom."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-onesample-turn",
   "metadata": {},
   "source": [
    "### Your Turn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-onesample-scipy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test whether mean GDP growth (QoQ) is significantly different from 2%.\n",
    "# 1. Extract the GDP growth series and drop NaNs.\n",
    "# 2. Run scipy.stats.ttest_1samp with popmean=2.0.\n",
    "# 3. Print the t-statistic and p-value.\n",
    "\n",
    "gdp_growth = df['gdp_growth_qoq'].dropna()\n",
    "\n",
    "t_stat_1samp, p_value_1samp = ...\n",
    "\n",
    "print(f'Sample mean:  {gdp_growth.mean():.4f}')\n",
    "print(f't-statistic:  {t_stat_1samp:.4f}')\n",
    "print(f'p-value:      {p_value_1samp:.4f}')\n",
    "print(f'n:            {len(gdp_growth)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-onesample-manual",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute the t-statistic manually to verify.\n",
    "# t = (xbar - mu0) / (s / sqrt(n))\n",
    "\n",
    "mu0 = 2.0\n",
    "xbar = gdp_growth.mean()\n",
    "s = gdp_growth.std(ddof=1)  # sample std (Bessel's correction)\n",
    "n = len(gdp_growth)\n",
    "\n",
    "t_manual = ...\n",
    "\n",
    "print(f'Manual t-stat: {t_manual:.4f}')\n",
    "print(f'Scipy t-stat:  {t_stat_1samp:.4f}')\n",
    "print(f'Match: {np.isclose(t_manual, t_stat_1samp)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-onesample-ci",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Construct a 95% confidence interval for the population mean GDP growth.\n",
    "# CI = xbar +/- t_crit * (s / sqrt(n))\n",
    "# Hint: t_crit = stats.t.ppf(0.975, df=n-1)\n",
    "\n",
    "alpha = 0.05\n",
    "t_crit = ...\n",
    "margin = ...\n",
    "ci_low = ...\n",
    "ci_high = ...\n",
    "\n",
    "print(f'95% CI for mean GDP growth: [{ci_low:.4f}, {ci_high:.4f}]')\n",
    "print(f'Does the CI contain 2.0? {ci_low <= 2.0 <= ci_high}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-onesample-interp",
   "metadata": {},
   "source": [
    "**Interpretation prompt** (write 2-4 sentences below):\n",
    "- Can we reject the null hypothesis that mean GDP growth equals 2% at the 5% level? Why or why not?\n",
    "- Is the confidence interval consistent with the p-value? (If 2.0 is inside the CI, the p-value should be > 0.05.)\n",
    "- What does the sign of the t-statistic tell you about the direction of the difference?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-twosample-header",
   "metadata": {},
   "source": [
    "<a id=\"two-sample-t-test\"></a>\n",
    "## Two-Sample t-Test\n",
    "\n",
    "### Goal\n",
    "Test whether the means of two independent groups differ.\n",
    "\n",
    "### Why this matters in economics\n",
    "\"Is mean GDP growth different during recession vs. non-recession quarters?\" This is\n",
    "the bread-and-butter comparison in empirical economics. The two-sample t-test compares\n",
    "the means of two independent groups. **Welch's t-test** does not assume equal variances\n",
    "in the two groups and is generally preferred, since economic volatility often changes\n",
    "across regimes.\n",
    "\n",
    "**Welch's t-statistic:**\n",
    "$$t = \\frac{\\bar{x}_1 - \\bar{x}_2}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-twosample-turn",
   "metadata": {},
   "source": [
    "### Your Turn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-twosample-split",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Split GDP growth into recession and non-recession quarters.\n",
    "# Then run a two-sample t-test (Welch's).\n",
    "# Hint: use df.loc[df['recession'] == 1, 'gdp_growth_qoq'] to filter.\n",
    "\n",
    "gdp_recession = ...\n",
    "gdp_expansion = ...\n",
    "\n",
    "print(f'Recession quarters:     n={len(gdp_recession)}, mean={gdp_recession.mean():.4f}, std={gdp_recession.std():.4f}')\n",
    "print(f'Non-recession quarters: n={len(gdp_expansion)}, mean={gdp_expansion.mean():.4f}, std={gdp_expansion.std():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-twosample-test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run Welch's two-sample t-test.\n",
    "# Hint: stats.ttest_ind(x, y, equal_var=False)\n",
    "\n",
    "t_stat_2samp, p_value_2samp = ...\n",
    "\n",
    "print(f't-statistic: {t_stat_2samp:.4f}')\n",
    "print(f'p-value:     {p_value_2samp:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-twosample-equalvar",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: For comparison, run the equal-variance version (equal_var=True).\n",
    "# How much does the p-value change?\n",
    "\n",
    "t_eq, p_eq = ...\n",
    "\n",
    "print(f'Equal-variance t-test: t={t_eq:.4f}, p={p_eq:.4f}')\n",
    "print(f'Welch t-test:          t={t_stat_2samp:.4f}, p={p_value_2samp:.4f}')\n",
    "print(f'\\nThe standard deviations are quite different ({gdp_recession.std():.4f} vs {gdp_expansion.std():.4f}),')\n",
    "print('so Welch\\'s test is more appropriate here.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-twosample-viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create side-by-side box plots comparing GDP growth in recession vs. expansion.\n",
    "# Hint: df.boxplot(column='gdp_growth_qoq', by='recession')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 5))\n",
    "\n",
    "...\n",
    "\n",
    "ax.set_title('GDP Growth: Recession vs Expansion Quarters')\n",
    "ax.set_xlabel('Recession (0 = No, 1 = Yes)')\n",
    "ax.set_ylabel('GDP Growth (QoQ %)')\n",
    "plt.suptitle('')  # remove auto-title from .boxplot()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-twosample-interp",
   "metadata": {},
   "source": [
    "**Interpretation prompt** (write 2-4 sentences below):\n",
    "- Is the difference in mean GDP growth between recession and non-recession quarters statistically significant?\n",
    "- Why is Welch's t-test preferred here over the equal-variance version?\n",
    "- Do the box plots visually confirm what the test tells you numerically?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-paired-header",
   "metadata": {},
   "source": [
    "<a id=\"paired-t-test\"></a>\n",
    "## Paired t-Test\n",
    "\n",
    "### Goal\n",
    "Test whether the mean difference between paired observations is zero.\n",
    "\n",
    "### Why this matters in economics\n",
    "Imagine measuring an economic indicator in 10 regions **before** and **after** a policy\n",
    "change. Each region serves as its own control. By computing the difference (after - before)\n",
    "for each region, you remove region-level variation and focus on the treatment effect.\n",
    "A paired t-test is more powerful than an unpaired test when the pairing is meaningful,\n",
    "because it reduces noise.\n",
    "\n",
    "**Test statistic:**\n",
    "$$t = \\frac{\\bar{d}}{s_d / \\sqrt{n}}$$\n",
    "\n",
    "where $d_i = x_{\\text{after},i} - x_{\\text{before},i}$, $\\bar{d}$ is the mean of the\n",
    "differences, $s_d$ is the standard deviation of the differences, and $n$ is the number\n",
    "of pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-paired-turn",
   "metadata": {},
   "source": [
    "### Your Turn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-paired-simulate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We simulate paired data: an economic indicator measured in 10 regions\n",
    "# before and after a policy intervention.\n",
    "# The true effect is a small positive shift of +0.8 percentage points,\n",
    "# but there is substantial region-level variation.\n",
    "\n",
    "np.random.seed(42)\n",
    "n_regions = 10\n",
    "region_baseline = np.random.normal(loc=3.0, scale=2.0, size=n_regions)  # baseline differs by region\n",
    "effect = 0.8  # true policy effect\n",
    "noise = np.random.normal(0, 0.5, size=n_regions)\n",
    "\n",
    "before = region_baseline + np.random.normal(0, 0.3, size=n_regions)\n",
    "after = region_baseline + effect + noise\n",
    "\n",
    "paired_df = pd.DataFrame({\n",
    "    'region': [f'Region_{i+1}' for i in range(n_regions)],\n",
    "    'before': before,\n",
    "    'after': after,\n",
    "})\n",
    "paired_df['difference'] = paired_df['after'] - paired_df['before']\n",
    "paired_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-paired-test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run a paired t-test on the before/after values.\n",
    "# Hint: stats.ttest_rel(after, before)\n",
    "# This tests H0: mean(after - before) = 0.\n",
    "\n",
    "t_stat_paired, p_value_paired = ...\n",
    "\n",
    "print(f'Mean difference:  {paired_df[\"difference\"].mean():.4f}')\n",
    "print(f'Std of differences: {paired_df[\"difference\"].std(ddof=1):.4f}')\n",
    "print(f't-statistic:      {t_stat_paired:.4f}')\n",
    "print(f'p-value:          {p_value_paired:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-paired-vs-unpaired",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compare with an unpaired (independent) two-sample t-test.\n",
    "# Notice how the unpaired test has a higher p-value (less power)\n",
    "# because it cannot account for region-level variation.\n",
    "\n",
    "t_unpaired, p_unpaired = ...\n",
    "\n",
    "print('--- Paired t-test ---')\n",
    "print(f'  t = {t_stat_paired:.4f}, p = {p_value_paired:.4f}')\n",
    "print()\n",
    "print('--- Unpaired (independent) t-test ---')\n",
    "print(f'  t = {t_unpaired:.4f}, p = {p_unpaired:.4f}')\n",
    "print()\n",
    "print('The paired test is more powerful because it removes region-level variation.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-paired-interp",
   "metadata": {},
   "source": [
    "**Interpretation prompt** (write 2-4 sentences below):\n",
    "- Is the mean difference statistically significant at the 5% level using the paired test?\n",
    "- How does the p-value change when you use the unpaired test? Why?\n",
    "- In what real-world economic studies would paired data arise naturally? (Think: same country before/after a trade agreement, same firm before/after a regulation.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-chi2-indep-header",
   "metadata": {},
   "source": [
    "<a id=\"chi-squared-test-of-independence\"></a>\n",
    "## Chi-Squared Test of Independence\n",
    "\n",
    "### Goal\n",
    "Test whether two categorical variables are independent using a contingency table.\n",
    "\n",
    "### Why this matters in economics\n",
    "\"Is there an association between recession quarters and the direction of interest rate\n",
    "changes?\" We can cross-tabulate recession status (yes/no) against whether the Fed\n",
    "raised, lowered, or held rates. The chi-squared test of independence tells us whether\n",
    "the observed pattern differs from what we would expect if the two variables were\n",
    "unrelated.\n",
    "\n",
    "**Test statistic:**\n",
    "$$\\chi^2 = \\sum_{i,j} \\frac{(O_{ij} - E_{ij})^2}{E_{ij}}$$\n",
    "\n",
    "where $O_{ij}$ is the observed count and $E_{ij}$ is the expected count under independence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-chi2-indep-turn",
   "metadata": {},
   "source": [
    "### Your Turn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-chi2-indep-prep",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, create a categorical variable for the direction of interest rate changes.\n",
    "# We compute the quarter-over-quarter change in FEDFUNDS and classify it as\n",
    "# 'cut', 'hold', or 'hike'.\n",
    "\n",
    "df_chi = df[['FEDFUNDS', 'recession']].dropna().copy()\n",
    "df_chi['rate_change'] = df_chi['FEDFUNDS'].diff()\n",
    "df_chi = df_chi.dropna()\n",
    "\n",
    "# Classify direction: cut (< -0.1), hold (between -0.1 and +0.1), hike (> +0.1)\n",
    "df_chi['direction'] = pd.cut(\n",
    "    df_chi['rate_change'],\n",
    "    bins=[-np.inf, -0.1, 0.1, np.inf],\n",
    "    labels=['cut', 'hold', 'hike']\n",
    ")\n",
    "\n",
    "print(df_chi['direction'].value_counts())\n",
    "print(f'\\nRecession quarters: {int(df_chi[\"recession\"].sum())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-chi2-indep-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a contingency table (cross-tabulation) of recession x direction.\n",
    "# Hint: pd.crosstab(df_chi['recession'], df_chi['direction'])\n",
    "\n",
    "contingency_table = ...\n",
    "\n",
    "print('Contingency Table:')\n",
    "print(contingency_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-chi2-indep-test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run the chi-squared test of independence.\n",
    "# Hint: stats.chi2_contingency(contingency_table)\n",
    "# Returns: chi2, p, dof, expected_freq\n",
    "\n",
    "chi2_stat, p_value_chi2, dof_chi2, expected_freq = ...\n",
    "\n",
    "print(f'Chi-squared statistic: {chi2_stat:.4f}')\n",
    "print(f'p-value:               {p_value_chi2:.4f}')\n",
    "print(f'Degrees of freedom:    {dof_chi2}')\n",
    "print(f'\\nExpected frequencies (under independence):')\n",
    "print(pd.DataFrame(expected_freq,\n",
    "                    index=contingency_table.index,\n",
    "                    columns=contingency_table.columns).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-chi2-indep-heatmap",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Visualize the contingency table as a heatmap.\n",
    "# Hint: plt.imshow() or use ax.pcolormesh(), then add annotations.\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Left: observed counts\n",
    "im0 = axes[0].imshow(contingency_table.values, cmap='Blues', aspect='auto')\n",
    "axes[0].set_xticks(range(len(contingency_table.columns)))\n",
    "axes[0].set_xticklabels(contingency_table.columns)\n",
    "axes[0].set_yticks(range(len(contingency_table.index)))\n",
    "axes[0].set_yticklabels(['Expansion', 'Recession'])\n",
    "axes[0].set_title('Observed Counts')\n",
    "for i in range(contingency_table.shape[0]):\n",
    "    for j in range(contingency_table.shape[1]):\n",
    "        axes[0].text(j, i, str(contingency_table.values[i, j]),\n",
    "                     ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "plt.colorbar(im0, ax=axes[0])\n",
    "\n",
    "# Right: expected counts\n",
    "...\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-chi2-indep-interp",
   "metadata": {},
   "source": [
    "**Interpretation prompt** (write 2-4 sentences below):\n",
    "- Is there a statistically significant association between recession status and the direction of rate changes?\n",
    "- Are any expected cell counts below 5? If so, the chi-squared approximation may be unreliable.\n",
    "- Substantively, does it make economic sense that rate changes and recessions are (or are not) associated?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-chi2-gof-header",
   "metadata": {},
   "source": [
    "<a id=\"chi-squared-goodness-of-fit\"></a>\n",
    "## Chi-Squared Goodness-of-Fit\n",
    "\n",
    "### Goal\n",
    "Test whether observed data follows a specific theoretical distribution.\n",
    "\n",
    "### Why this matters in economics\n",
    "Many econometric techniques assume normality of errors. \"Is GDP growth approximately\n",
    "normally distributed?\" is a practical question. The chi-squared goodness-of-fit test\n",
    "bins the data, computes expected frequencies under a normal distribution with the\n",
    "same mean and standard deviation, and checks whether the observed bin counts match.\n",
    "We also compare results with the Shapiro-Wilk test, which is a more powerful\n",
    "test of normality for moderate sample sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-chi2-gof-turn",
   "metadata": {},
   "source": [
    "### Your Turn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-chi2-gof-bin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Bin GDP growth data and compute expected frequencies under normality.\n",
    "# Steps:\n",
    "# 1. Compute the histogram (observed counts) with ~8 bins.\n",
    "# 2. For each bin, compute the expected count from a normal distribution\n",
    "#    with the same mean and std as the data.\n",
    "# 3. Run scipy.stats.chisquare(observed, expected).\n",
    "\n",
    "gdp_gof = df['gdp_growth_qoq'].dropna()\n",
    "mu_hat = gdp_gof.mean()\n",
    "sigma_hat = gdp_gof.std()\n",
    "n_obs = len(gdp_gof)\n",
    "\n",
    "# Create bins and compute observed counts\n",
    "n_bins = 8\n",
    "observed_counts, bin_edges = np.histogram(gdp_gof, bins=n_bins)\n",
    "\n",
    "# Compute expected counts under a normal distribution\n",
    "# For each bin, expected proportion = CDF(right_edge) - CDF(left_edge)\n",
    "expected_probs = ...\n",
    "expected_counts = ...\n",
    "\n",
    "print('Bin edges:', np.round(bin_edges, 2))\n",
    "print('Observed: ', observed_counts)\n",
    "print('Expected: ', np.round(expected_counts, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-chi2-gof-test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run the chi-squared goodness-of-fit test.\n",
    "# Note: we estimated 2 parameters (mean, std) from the data, so the\n",
    "# true degrees of freedom = n_bins - 1 - 2 = n_bins - 3.\n",
    "# scipy.stats.chisquare does not adjust for estimated parameters automatically,\n",
    "# so we pass ddof=2 to account for the 2 estimated parameters.\n",
    "\n",
    "chi2_gof, p_value_gof = ...\n",
    "\n",
    "print(f'Chi-squared statistic: {chi2_gof:.4f}')\n",
    "print(f'p-value:               {p_value_gof:.4f}')\n",
    "print(f'Degrees of freedom:    {n_bins - 1 - 2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-chi2-gof-shapiro",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complement with the Shapiro-Wilk test for normality.\n",
    "# Hint: stats.shapiro(gdp_gof)\n",
    "# The Shapiro-Wilk test is generally more powerful than the chi-squared\n",
    "# goodness-of-fit test for testing normality.\n",
    "\n",
    "shapiro_stat, shapiro_p = ...\n",
    "\n",
    "print(f'Shapiro-Wilk statistic: {shapiro_stat:.4f}')\n",
    "print(f'Shapiro-Wilk p-value:   {shapiro_p:.4f}')\n",
    "print()\n",
    "print('Comparison:')\n",
    "print(f'  Chi-squared GOF p-value: {p_value_gof:.4f}')\n",
    "print(f'  Shapiro-Wilk p-value:    {shapiro_p:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-chi2-gof-interp",
   "metadata": {},
   "source": [
    "**Interpretation prompt** (write 2-4 sentences below):\n",
    "- Do both tests agree on whether GDP growth is normally distributed?\n",
    "- If expected counts in some bins are very small (< 5), how might that affect the chi-squared result?\n",
    "- Why might the Shapiro-Wilk test give a different conclusion than the chi-squared goodness-of-fit?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-ftest-var-header",
   "metadata": {},
   "source": [
    "<a id=\"f-test-for-equality-of-variances\"></a>\n",
    "## F-Test for Equality of Variances\n",
    "\n",
    "### Goal\n",
    "Test whether two groups have equal variance.\n",
    "\n",
    "### Why this matters in economics\n",
    "\"Is GDP growth more volatile during recessions?\" is a question about variances.\n",
    "If the variance of GDP growth differs between recession and expansion quarters,\n",
    "this has implications for risk modeling and for the validity of tests that assume\n",
    "equal variances. In regression, unequal variance of errors across groups is called\n",
    "**heteroskedasticity** -- a key diagnostic you will encounter in the regression\n",
    "module (02_regression/04a).\n",
    "\n",
    "The classic F-test for equal variances ($F = s_1^2 / s_2^2$) is very sensitive to\n",
    "non-normality. **Levene's test** is more robust because it tests equality of\n",
    "variances based on deviations from group medians rather than means."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-ftest-var-turn",
   "metadata": {},
   "source": [
    "### Your Turn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-ftest-var-compute",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute the variance of GDP growth in recession vs. expansion quarters.\n",
    "# Then run Levene's test for equality of variances.\n",
    "\n",
    "gdp_rec = df.loc[df['recession'] == 1, 'gdp_growth_qoq'].dropna()\n",
    "gdp_exp = df.loc[df['recession'] == 0, 'gdp_growth_qoq'].dropna()\n",
    "\n",
    "var_rec = ...\n",
    "var_exp = ...\n",
    "\n",
    "print(f'Variance (recession):  {var_rec:.4f}  (std = {np.sqrt(var_rec):.4f})')\n",
    "print(f'Variance (expansion):  {var_exp:.4f}  (std = {np.sqrt(var_exp):.4f})')\n",
    "print(f'Ratio (recession/expansion): {var_rec / var_exp:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-ftest-var-levene",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run Levene's test.\n",
    "# Hint: stats.levene(gdp_rec, gdp_exp)\n",
    "\n",
    "levene_stat, levene_p = ...\n",
    "\n",
    "print(f'Levene\\'s test statistic: {levene_stat:.4f}')\n",
    "print(f'p-value:                  {levene_p:.4f}')\n",
    "print()\n",
    "if levene_p < 0.05:\n",
    "    print('Reject H0: variances are significantly different at alpha=0.05.')\n",
    "    print('This suggests GDP growth is more (or less) volatile during recessions.')\n",
    "else:\n",
    "    print('Fail to reject H0: no significant difference in variances at alpha=0.05.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-ftest-var-classic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reference: the classic F-test (less robust, but instructive).\n",
    "# F = s1^2 / s2^2, compared to the F-distribution with (n1-1, n2-1) df.\n",
    "\n",
    "F_classic = var_rec / var_exp\n",
    "df1 = len(gdp_rec) - 1\n",
    "df2 = len(gdp_exp) - 1\n",
    "\n",
    "# Two-tailed p-value\n",
    "p_classic = 2 * min(\n",
    "    stats.f.cdf(F_classic, df1, df2),\n",
    "    1 - stats.f.cdf(F_classic, df1, df2)\n",
    ")\n",
    "\n",
    "print(f'Classic F-statistic: {F_classic:.4f}')\n",
    "print(f'Classic F p-value:   {p_classic:.4f}')\n",
    "print(f'Levene p-value:      {levene_p:.4f}')\n",
    "print()\n",
    "print('Levene\\'s test is preferred because it is robust to non-normality.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-ftest-var-interp",
   "metadata": {},
   "source": [
    "**Interpretation prompt** (write 2-4 sentences below):\n",
    "- Is GDP growth significantly more volatile during recessions?\n",
    "- Do Levene's test and the classic F-test agree? Would you expect them to differ if the data were non-normal?\n",
    "- How does this result connect to heteroskedasticity in regression? (Hint: if error variance differs across subgroups, OLS standard errors are biased.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-ftest-reg-header",
   "metadata": {},
   "source": [
    "<a id=\"f-test-in-regression\"></a>\n",
    "## F-Test in Regression (Joint Significance)\n",
    "\n",
    "### Goal\n",
    "Read and interpret the F-statistic from a regression summary, and test a subset\n",
    "of coefficients for joint significance.\n",
    "\n",
    "### Why this matters in economics\n",
    "Every OLS regression summary includes an F-statistic that tests whether **all**\n",
    "predictors are jointly significant (i.e., $H_0$: all slope coefficients = 0).\n",
    "Beyond the overall F-test, you can test whether a **subset** of coefficients is\n",
    "jointly zero using `res.f_test()`. This is essential for questions like:\n",
    "\"Are these three lag variables jointly significant, even if none of them is\n",
    "individually significant?\"\n",
    "\n",
    "**Overall F-statistic:**\n",
    "$$F = \\frac{(\\text{TSS} - \\text{RSS}) / k}{\\text{RSS} / (n - k - 1)} = \\frac{R^2 / k}{(1 - R^2) / (n - k - 1)}$$\n",
    "\n",
    "where $k$ is the number of predictors, $n$ is the sample size, TSS is total sum of squares,\n",
    "and RSS is residual sum of squares."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-ftest-reg-turn",
   "metadata": {},
   "source": [
    "### Your Turn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-ftest-reg-fit",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# TODO: Fit an OLS regression predicting GDP growth from several macro indicators.\n",
    "# Use UNRATE, FEDFUNDS, and T10Y2Y as predictors.\n",
    "# Hint:\n",
    "#   X = df[['UNRATE', 'FEDFUNDS', 'T10Y2Y']].dropna()\n",
    "#   X = sm.add_constant(X)\n",
    "#   y = df.loc[X.index, 'gdp_growth_qoq']\n",
    "#   model = sm.OLS(y, X).fit()\n",
    "\n",
    "predictors = ['UNRATE', 'FEDFUNDS', 'T10Y2Y']\n",
    "reg_df = df[predictors + ['gdp_growth_qoq']].dropna()\n",
    "\n",
    "X = ...\n",
    "y = ...\n",
    "\n",
    "model = ...\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-ftest-reg-read",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Extract and interpret the overall F-statistic and its p-value.\n",
    "# Hint: model.fvalue, model.f_pvalue\n",
    "\n",
    "f_overall = ...\n",
    "f_overall_p = ...\n",
    "\n",
    "print(f'Overall F-statistic: {f_overall:.4f}')\n",
    "print(f'Overall F p-value:   {f_overall_p:.6f}')\n",
    "print()\n",
    "print('This tests H0: all slope coefficients = 0 (the model has no explanatory power).')\n",
    "if f_overall_p < 0.05:\n",
    "    print('We reject H0: the predictors are jointly significant at alpha=0.05.')\n",
    "else:\n",
    "    print('We fail to reject H0: the predictors are not jointly significant at alpha=0.05.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-ftest-reg-tvalues",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at individual t-statistics for each coefficient.\n",
    "# Some may be individually insignificant even though the overall F-test is significant.\n",
    "\n",
    "coef_table = pd.DataFrame({\n",
    "    'coefficient': model.params,\n",
    "    't_stat': model.tvalues,\n",
    "    'p_value': model.pvalues,\n",
    "})\n",
    "coef_table['significant_5pct'] = coef_table['p_value'] < 0.05\n",
    "print(coef_table.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-ftest-reg-joint",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test a subset of coefficients for joint significance.\n",
    "# Test H0: coefficients on FEDFUNDS and T10Y2Y are both zero.\n",
    "# Hint: model.f_test('FEDFUNDS = 0, T10Y2Y = 0')\n",
    "# or: model.f_test(np.array([[0, 0, 1, 0], [0, 0, 0, 1]]))\n",
    "\n",
    "joint_test = ...\n",
    "\n",
    "print('Joint F-test: H0: FEDFUNDS = 0 AND T10Y2Y = 0')\n",
    "print(f'F-statistic: {joint_test.fvalue[0][0]:.4f}')\n",
    "print(f'p-value:     {joint_test.pvalue:.4f}')\n",
    "print()\n",
    "print('This answers: \"Are FEDFUNDS and T10Y2Y jointly significant,\"')\n",
    "print('\"even if individually one or both are not?\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-ftest-reg-interp",
   "metadata": {},
   "source": [
    "**Interpretation prompt** (write 2-4 sentences below):\n",
    "- Is the overall F-test significant? What does that tell you about the model as a whole?\n",
    "- Are all individual coefficients significant? If not, does that contradict the overall F-test?\n",
    "- What does the joint F-test on FEDFUNDS and T10Y2Y tell you that the individual t-tests do not?\n",
    "- Why might variables be jointly significant but individually insignificant? (Hint: think about multicollinearity.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-later-header",
   "metadata": {},
   "source": [
    "## Where This Shows Up Later\n",
    "\n",
    "The tests in this notebook are not isolated exercises -- they appear throughout the rest\n",
    "of the project:\n",
    "\n",
    "- **t-tests on regression coefficients**: Every regression summary (`res.summary()`) reports\n",
    "  a t-statistic and p-value for each coefficient. You will read these in every notebook\n",
    "  from `02_regression` onward.\n",
    "\n",
    "- **F-tests in regression summaries**: The overall F-statistic tests whether your regression\n",
    "  model explains anything at all. Joint F-tests let you compare nested models\n",
    "  (e.g., \"does adding these 3 variables improve the fit?\").\n",
    "\n",
    "- **Chi-squared tests in diagnostics**: The Breusch-Pagan test for heteroskedasticity\n",
    "  (`02_regression/04a_residual_diagnostics`) uses a chi-squared statistic. The Ljung-Box\n",
    "  test for serial correlation also uses chi-squared.\n",
    "\n",
    "- **ADF test for unit roots**: The Augmented Dickey-Fuller test (`07_time_series_econ`)\n",
    "  uses a t-distribution variant (with non-standard critical values) to test whether a\n",
    "  time series has a unit root (is non-stationary).\n",
    "\n",
    "- **Hausman test**: Comparing fixed and random effects estimators (`06_causal/01a`) uses\n",
    "  a chi-squared test statistic.\n",
    "\n",
    "Understanding *which* test is being applied and *why* will help you interpret results\n",
    "across all these contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-checkpoint-header",
   "metadata": {},
   "source": [
    "<a id=\"checkpoint-self-check\"></a>\n",
    "## Checkpoint (Self-Check)\n",
    "Run these asserts to verify your work. If any fail, go back and fix the corresponding section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-checkpoint",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- One-sample t-test ----\n",
    "assert isinstance(t_stat_1samp, float), 't_stat_1samp should be a float'\n",
    "assert isinstance(p_value_1samp, float), 'p_value_1samp should be a float'\n",
    "assert 0 <= p_value_1samp <= 1, 'p-value must be between 0 and 1'\n",
    "assert np.isclose(t_manual, t_stat_1samp, atol=1e-6), 'Manual and scipy t-stats should match'\n",
    "\n",
    "# ---- Two-sample t-test ----\n",
    "assert isinstance(t_stat_2samp, float), 't_stat_2samp should be a float'\n",
    "assert isinstance(p_value_2samp, float), 'p_value_2samp should be a float'\n",
    "assert 0 <= p_value_2samp <= 1, 'p-value must be between 0 and 1'\n",
    "\n",
    "# ---- Paired t-test ----\n",
    "assert isinstance(t_stat_paired, (float, np.floating)), 't_stat_paired should be numeric'\n",
    "assert isinstance(p_value_paired, (float, np.floating)), 'p_value_paired should be numeric'\n",
    "assert p_value_paired < p_unpaired, 'Paired test should have lower p-value than unpaired (more power)'\n",
    "\n",
    "# ---- Chi-squared test of independence ----\n",
    "assert chi2_stat >= 0, 'Chi-squared statistic must be non-negative'\n",
    "assert 0 <= p_value_chi2 <= 1, 'p-value must be between 0 and 1'\n",
    "assert dof_chi2 > 0, 'Degrees of freedom must be positive'\n",
    "\n",
    "# ---- F-test in regression ----\n",
    "assert f_overall > 0, 'F-statistic must be positive'\n",
    "assert 0 <= f_overall_p <= 1, 'F p-value must be between 0 and 1'\n",
    "\n",
    "# ---- Levene's test ----\n",
    "assert levene_stat >= 0, 'Levene statistic must be non-negative'\n",
    "assert 0 <= levene_p <= 1, 'Levene p-value must be between 0 and 1'\n",
    "\n",
    "print('All checkpoint assertions passed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-extensions",
   "metadata": {},
   "source": [
    "## Extensions (Optional)\n",
    "- Run a **one-way ANOVA** using `scipy.stats.f_oneway` to compare GDP growth across three or more groups\n",
    "  (e.g., bin quarters by unemployment level: low/medium/high).\n",
    "- Apply the **Kolmogorov-Smirnov test** (`scipy.stats.kstest`) as an alternative to the chi-squared\n",
    "  goodness-of-fit for testing normality. Compare the results.\n",
    "- Investigate how **sample size** affects the power of the one-sample t-test by subsampling\n",
    "  the data at different sizes and tracking how the p-value changes.\n",
    "- Explore **Fisher's exact test** (`scipy.stats.fisher_exact`) as an alternative to the chi-squared\n",
    "  test when expected cell counts are small."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-reflection",
   "metadata": {},
   "source": [
    "## Reflection\n",
    "- Which test was most intuitive to you? Which was most surprising in its result?\n",
    "- In your own words, what is the difference between the F-test for equal variances and the F-test in regression?\n",
    "- When you see a p-value of 0.06, how would you communicate the result to a non-technical audience?\n",
    "- Think of an economic question you care about. Which test from this notebook would you use to answer it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-solutions",
   "metadata": {},
   "source": [
    "<a id=\"solutions-reference\"></a>\n",
    "## Solutions (Reference)\n",
    "\n",
    "Try the TODOs first. Use these only to unblock yourself or to compare approaches.\n",
    "\n",
    "<details><summary>Solution: One-sample t-test</summary>\n",
    "\n",
    "_One possible approach. Your variable names may differ; align them with the notebook._\n",
    "\n",
    "```python\n",
    "# Reference solution for 06_common_statistical_tests -- One-sample t-test\n",
    "gdp_growth = df['gdp_growth_qoq'].dropna()\n",
    "\n",
    "# Scipy version\n",
    "t_stat_1samp, p_value_1samp = stats.ttest_1samp(gdp_growth, popmean=2.0)\n",
    "\n",
    "print(f'Sample mean:  {gdp_growth.mean():.4f}')\n",
    "print(f't-statistic:  {t_stat_1samp:.4f}')\n",
    "print(f'p-value:      {p_value_1samp:.4f}')\n",
    "\n",
    "# Manual version\n",
    "mu0 = 2.0\n",
    "xbar = gdp_growth.mean()\n",
    "s = gdp_growth.std(ddof=1)\n",
    "n = len(gdp_growth)\n",
    "t_manual = (xbar - mu0) / (s / np.sqrt(n))\n",
    "\n",
    "# Confidence interval\n",
    "alpha = 0.05\n",
    "t_crit = stats.t.ppf(1 - alpha / 2, df=n - 1)\n",
    "margin = t_crit * (s / np.sqrt(n))\n",
    "ci_low = xbar - margin\n",
    "ci_high = xbar + margin\n",
    "print(f'95% CI: [{ci_low:.4f}, {ci_high:.4f}]')\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<details><summary>Solution: Two-sample t-test</summary>\n",
    "\n",
    "_One possible approach. Your variable names may differ; align them with the notebook._\n",
    "\n",
    "```python\n",
    "# Reference solution for 06_common_statistical_tests -- Two-sample t-test\n",
    "gdp_recession = df.loc[df['recession'] == 1, 'gdp_growth_qoq'].dropna()\n",
    "gdp_expansion = df.loc[df['recession'] == 0, 'gdp_growth_qoq'].dropna()\n",
    "\n",
    "# Welch's t-test (unequal variances)\n",
    "t_stat_2samp, p_value_2samp = stats.ttest_ind(\n",
    "    gdp_recession, gdp_expansion, equal_var=False\n",
    ")\n",
    "\n",
    "# Equal-variance version for comparison\n",
    "t_eq, p_eq = stats.ttest_ind(\n",
    "    gdp_recession, gdp_expansion, equal_var=True\n",
    ")\n",
    "\n",
    "# Box plot\n",
    "fig, ax = plt.subplots(figsize=(7, 5))\n",
    "df.boxplot(column='gdp_growth_qoq', by='recession', ax=ax)\n",
    "ax.set_title('GDP Growth: Recession vs Expansion Quarters')\n",
    "ax.set_xlabel('Recession (0 = No, 1 = Yes)')\n",
    "ax.set_ylabel('GDP Growth (QoQ %)')\n",
    "plt.suptitle('')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<details><summary>Solution: Paired t-test</summary>\n",
    "\n",
    "_One possible approach. Your variable names may differ; align them with the notebook._\n",
    "\n",
    "```python\n",
    "# Reference solution for 06_common_statistical_tests -- Paired t-test\n",
    "t_stat_paired, p_value_paired = stats.ttest_rel(after, before)\n",
    "\n",
    "# Compare with unpaired\n",
    "t_unpaired, p_unpaired = stats.ttest_ind(after, before, equal_var=False)\n",
    "\n",
    "print(f'Paired:   t={t_stat_paired:.4f}, p={p_value_paired:.4f}')\n",
    "print(f'Unpaired: t={t_unpaired:.4f}, p={p_unpaired:.4f}')\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<details><summary>Solution: Chi-squared test of independence</summary>\n",
    "\n",
    "_One possible approach. Your variable names may differ; align them with the notebook._\n",
    "\n",
    "```python\n",
    "# Reference solution for 06_common_statistical_tests -- Chi-squared independence\n",
    "contingency_table = pd.crosstab(df_chi['recession'], df_chi['direction'])\n",
    "\n",
    "chi2_stat, p_value_chi2, dof_chi2, expected_freq = stats.chi2_contingency(\n",
    "    contingency_table\n",
    ")\n",
    "\n",
    "print(f'Chi-squared: {chi2_stat:.4f}')\n",
    "print(f'p-value:     {p_value_chi2:.4f}')\n",
    "print(f'DOF:         {dof_chi2}')\n",
    "\n",
    "# Heatmap of expected counts\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Observed (already shown in the notebook)\n",
    "im0 = axes[0].imshow(contingency_table.values, cmap='Blues', aspect='auto')\n",
    "axes[0].set_xticks(range(len(contingency_table.columns)))\n",
    "axes[0].set_xticklabels(contingency_table.columns)\n",
    "axes[0].set_yticks(range(len(contingency_table.index)))\n",
    "axes[0].set_yticklabels(['Expansion', 'Recession'])\n",
    "axes[0].set_title('Observed Counts')\n",
    "for i in range(contingency_table.shape[0]):\n",
    "    for j in range(contingency_table.shape[1]):\n",
    "        axes[0].text(j, i, str(contingency_table.values[i, j]),\n",
    "                     ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "plt.colorbar(im0, ax=axes[0])\n",
    "\n",
    "# Expected\n",
    "im1 = axes[1].imshow(expected_freq, cmap='Oranges', aspect='auto')\n",
    "axes[1].set_xticks(range(len(contingency_table.columns)))\n",
    "axes[1].set_xticklabels(contingency_table.columns)\n",
    "axes[1].set_yticks(range(len(contingency_table.index)))\n",
    "axes[1].set_yticklabels(['Expansion', 'Recession'])\n",
    "axes[1].set_title('Expected Counts (under independence)')\n",
    "for i in range(expected_freq.shape[0]):\n",
    "    for j in range(expected_freq.shape[1]):\n",
    "        axes[1].text(j, i, f'{expected_freq[i, j]:.1f}',\n",
    "                     ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "plt.colorbar(im1, ax=axes[1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<details><summary>Solution: Chi-squared goodness-of-fit</summary>\n",
    "\n",
    "_One possible approach. Your variable names may differ; align them with the notebook._\n",
    "\n",
    "```python\n",
    "# Reference solution for 06_common_statistical_tests -- Chi-squared goodness-of-fit\n",
    "gdp_gof = df['gdp_growth_qoq'].dropna()\n",
    "mu_hat = gdp_gof.mean()\n",
    "sigma_hat = gdp_gof.std()\n",
    "n_obs = len(gdp_gof)\n",
    "\n",
    "n_bins = 8\n",
    "observed_counts, bin_edges = np.histogram(gdp_gof, bins=n_bins)\n",
    "\n",
    "# Expected probabilities from the normal CDF\n",
    "expected_probs = np.diff(stats.norm.cdf(bin_edges, loc=mu_hat, scale=sigma_hat))\n",
    "expected_counts = expected_probs * n_obs\n",
    "\n",
    "# Chi-squared test with ddof=2 (estimated mean and std)\n",
    "chi2_gof, p_value_gof = stats.chisquare(observed_counts, f_exp=expected_counts, ddof=2)\n",
    "\n",
    "# Shapiro-Wilk\n",
    "shapiro_stat, shapiro_p = stats.shapiro(gdp_gof)\n",
    "\n",
    "print(f'Chi-sq GOF:    chi2={chi2_gof:.4f}, p={p_value_gof:.4f}')\n",
    "print(f'Shapiro-Wilk:  W={shapiro_stat:.4f}, p={shapiro_p:.4f}')\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<details><summary>Solution: F-test for equality of variances</summary>\n",
    "\n",
    "_One possible approach. Your variable names may differ; align them with the notebook._\n",
    "\n",
    "```python\n",
    "# Reference solution for 06_common_statistical_tests -- F-test for variances\n",
    "gdp_rec = df.loc[df['recession'] == 1, 'gdp_growth_qoq'].dropna()\n",
    "gdp_exp = df.loc[df['recession'] == 0, 'gdp_growth_qoq'].dropna()\n",
    "\n",
    "var_rec = gdp_rec.var(ddof=1)\n",
    "var_exp = gdp_exp.var(ddof=1)\n",
    "\n",
    "levene_stat, levene_p = stats.levene(gdp_rec, gdp_exp)\n",
    "print(f'Levene test: stat={levene_stat:.4f}, p={levene_p:.4f}')\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<details><summary>Solution: F-test in regression</summary>\n",
    "\n",
    "_One possible approach. Your variable names may differ; align them with the notebook._\n",
    "\n",
    "```python\n",
    "# Reference solution for 06_common_statistical_tests -- F-test in regression\n",
    "import statsmodels.api as sm\n",
    "\n",
    "predictors = ['UNRATE', 'FEDFUNDS', 'T10Y2Y']\n",
    "reg_df = df[predictors + ['gdp_growth_qoq']].dropna()\n",
    "\n",
    "X = sm.add_constant(reg_df[predictors])\n",
    "y = reg_df['gdp_growth_qoq']\n",
    "\n",
    "model = sm.OLS(y, X).fit()\n",
    "print(model.summary())\n",
    "\n",
    "# Overall F-test\n",
    "f_overall = model.fvalue\n",
    "f_overall_p = model.f_pvalue\n",
    "\n",
    "# Joint test: FEDFUNDS = 0 AND T10Y2Y = 0\n",
    "joint_test = model.f_test('FEDFUNDS = 0, T10Y2Y = 0')\n",
    "print(f'Joint F: {joint_test.fvalue[0][0]:.4f}, p={joint_test.pvalue:.4f}')\n",
    "```\n",
    "\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 5,
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
