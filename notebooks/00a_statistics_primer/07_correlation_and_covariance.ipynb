{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-title",
   "metadata": {},
   "source": [
    "# 07 Correlation and Covariance\n",
    "\n",
    "Measuring and visualizing relationships between variables \u2014 and understanding why correlation is not causation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-toc",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [Covariance: measuring joint variability](#covariance)\n",
    "- [Pearson correlation](#pearson-correlation)\n",
    "- [Spearman rank correlation](#spearman-rank-correlation)\n",
    "- [Visualizing relationships](#visualizing-relationships)\n",
    "- [Spurious correlation](#spurious-correlation)\n",
    "- [Correlation does not imply causation](#correlation-does-not-imply-causation)\n",
    "- [The covariance matrix and regression](#the-covariance-matrix-and-regression)\n",
    "- [Checkpoint (Self-Check)](#checkpoint-self-check)\n",
    "- [Solutions (Reference)](#solutions-reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-why",
   "metadata": {},
   "source": [
    "## Why This Notebook Matters\n",
    "Correlation analysis is the gateway to regression. Before you fit any model, you should\n",
    "understand how your variables relate to each other. But correlation is also one of the\n",
    "most misused statistics \u2014 especially in economics, where spurious correlations from\n",
    "trending data and confounded relationships are everywhere. This notebook builds the\n",
    "intuition to use correlation wisely and skeptically.\n",
    "\n",
    "## Prerequisites (Quick Self-Check)\n",
    "- Completed notebooks 00\u201306 (full primer sequence so far).\n",
    "- Understanding of mean, variance, and standard deviation.\n",
    "- Familiarity with scatter plots.\n",
    "\n",
    "## What You Will Produce\n",
    "- (no file output; learning/analysis notebook)\n",
    "\n",
    "## Success Criteria\n",
    "- You can compute and interpret Pearson and Spearman correlations.\n",
    "- You can visualize a correlation matrix and scatter relationships.\n",
    "- You can demonstrate spurious correlation with trending data.\n",
    "- You can explain why correlation does not imply causation with a concrete example.\n",
    "\n",
    "## Common Pitfalls\n",
    "- Computing Pearson correlation on non-stationary time series (produces spurious results).\n",
    "- Interpreting high correlation as evidence of a causal relationship.\n",
    "- Ignoring non-linear relationships that Pearson correlation misses.\n",
    "- Using correlation on data with outliers without checking Spearman.\n",
    "\n",
    "## Quick Fixes (When You Get Stuck)\n",
    "- `df.corr()` for Pearson, `df.corr(method='spearman')` for Spearman.\n",
    "- `np.cov(X, Y)` for the 2x2 covariance matrix.\n",
    "- `seaborn.heatmap(corr_matrix, annot=True)` for visualization.\n",
    "- If you see `ModuleNotFoundError`, re-run the bootstrap cell.\n",
    "\n",
    "## Matching Guide\n",
    "- `docs/guides/00a_statistics_primer/07_correlation_and_covariance.md`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-howto",
   "metadata": {},
   "source": [
    "## How To Use This Notebook\n",
    "- Work section-by-section; don't skip the markdown.\n",
    "- Most code cells are incomplete on purpose: replace TODOs and `...`, then run.\n",
    "- After each section, write 2\u20134 sentences answering the interpretation prompts (what changed, why it matters).\n",
    "- Prefer `data/processed/*` if you have built the real datasets; otherwise use the bundled `data/sample/*` fallbacks.\n",
    "- Use the **Checkpoint (Self-Check)** section to catch mistakes early.\n",
    "- Use **Solutions (Reference)** only to unblock yourself; then re-implement without looking.\n",
    "- Use the matching guide (`docs/guides/00a_statistics_primer/07_correlation_and_covariance.md`) for the math, assumptions, and deeper context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-bootstrap-header",
   "metadata": {},
   "source": [
    "<a id=\"environment-bootstrap\"></a>\n",
    "## Environment Bootstrap\n",
    "Run this cell first. It makes the repo importable and defines common directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-bootstrap",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "\n",
    "def find_repo_root(start: Path) -> Path:\n",
    "    p = start\n",
    "    for _ in range(8):\n",
    "        if (p / 'src').exists() and (p / 'docs').exists():\n",
    "            return p\n",
    "        p = p.parent\n",
    "    raise RuntimeError('Could not find repo root. Start Jupyter from the repo root.')\n",
    "\n",
    "\n",
    "PROJECT_ROOT = find_repo_root(Path.cwd())\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "RAW_DIR = DATA_DIR / 'raw'\n",
    "PROCESSED_DIR = DATA_DIR / 'processed'\n",
    "SAMPLE_DIR = DATA_DIR / 'sample'\n",
    "\n",
    "PROJECT_ROOT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-load-data-header",
   "metadata": {},
   "source": [
    "## Load the Sample Data\n",
    "\n",
    "We will use `macro_quarterly_sample.csv` throughout this notebook.\n",
    "This dataset contains quarterly US macroeconomic indicators including GDP growth,\n",
    "unemployment rate, the federal funds rate, CPI, industrial production, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.read_csv(SAMPLE_DIR / 'macro_quarterly_sample.csv', index_col=0, parse_dates=True)\n",
    "print('Shape:', df.shape)\n",
    "print('Columns:', list(df.columns))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-cov-header",
   "metadata": {},
   "source": [
    "<a id=\"covariance\"></a>\n",
    "## Covariance: Measuring Joint Variability\n",
    "\n",
    "### Goal\n",
    "Understand what covariance measures, compute it manually from the definition, and\n",
    "verify the result with `df.cov()`.\n",
    "\n",
    "### Why this matters in economics\n",
    "Covariance tells us whether two variables tend to move together. For example, if GDP\n",
    "growth and unemployment have *negative* covariance, that means quarters with\n",
    "above-average growth tend to coincide with below-average unemployment (and vice\n",
    "versa) \u2014 exactly what Okun's Law predicts. However, covariance has a critical\n",
    "weakness: its magnitude depends on the scale (units) of the variables. Covariance\n",
    "between GDP (measured in billions of dollars) and unemployment (measured in percent)\n",
    "is hard to interpret on its own.\n",
    "\n",
    "**Key definition:**\n",
    "\n",
    "$$\\text{Cov}(X, Y) = \\frac{1}{n-1} \\sum_{i=1}^{n} (X_i - \\bar{X})(Y_i - \\bar{Y})$$\n",
    "\n",
    "- Positive covariance: $X$ and $Y$ tend to move in the same direction.\n",
    "- Negative covariance: $X$ and $Y$ tend to move in opposite directions.\n",
    "- Problem: the number itself is hard to interpret because it depends on the units of $X$ and $Y$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-cov-turn",
   "metadata": {},
   "source": [
    "### Your Turn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-cov-manual",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute covariance between GDP growth (QoQ) and unemployment rate MANUALLY\n",
    "# from the definition: Cov(X,Y) = (1/(n-1)) * sum((X - mean_X) * (Y - mean_Y))\n",
    "# Then compare with df[['gdp_growth_qoq', 'UNRATE']].cov()\n",
    "\n",
    "# Drop rows where either variable is NaN\n",
    "pair = df[['gdp_growth_qoq', 'UNRATE']].dropna()\n",
    "x = pair['gdp_growth_qoq']\n",
    "y = pair['UNRATE']\n",
    "\n",
    "n = len(x)\n",
    "\n",
    "# Manual calculation\n",
    "cov_manual = ...\n",
    "\n",
    "# Pandas calculation\n",
    "cov_pandas = ...\n",
    "\n",
    "print(f'Manual covariance:  {cov_manual:.6f}')\n",
    "print(f'Pandas covariance:  {cov_pandas:.6f}')\n",
    "print(f'Match: {np.isclose(cov_manual, cov_pandas)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-cov-matrix",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute the full covariance matrix for selected macro columns using df.cov()\n",
    "# Notice: the diagonal entries are the variances of each variable.\n",
    "\n",
    "macro_cols = ['gdp_growth_qoq', 'UNRATE', 'FEDFUNDS', 'CPIAUCSL', 'INDPRO']\n",
    "\n",
    "cov_matrix = ...\n",
    "\n",
    "print('Covariance matrix:')\n",
    "cov_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-cov-interp",
   "metadata": {},
   "source": [
    "**Interpretation prompt** (write 2\u20134 sentences below):\n",
    "- Is the covariance between GDP growth and unemployment positive or negative? Does that match your intuition?\n",
    "- Can you compare the covariance of (GDP, unemployment) with (GDP, CPI) and say which relationship is \"stronger\"? Why or why not?\n",
    "- What problem does the scale-dependence of covariance create for comparing relationships?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-pearson-header",
   "metadata": {},
   "source": [
    "<a id=\"pearson-correlation\"></a>\n",
    "## Pearson Correlation\n",
    "\n",
    "### Goal\n",
    "Compute the Pearson correlation coefficient, understand why it fixes the\n",
    "scale problem, and visualize a correlation matrix.\n",
    "\n",
    "### Why this matters in economics\n",
    "Pearson correlation normalizes covariance by the standard deviations of each\n",
    "variable, giving a unit-free measure bounded between $-1$ and $+1$. This makes it\n",
    "possible to compare the strength of relationships across different pairs of\n",
    "variables. Every empirical economics paper starts with a correlation matrix to\n",
    "understand variable relationships before running regressions.\n",
    "\n",
    "**Key definition:**\n",
    "\n",
    "$$r_{XY} = \\frac{\\text{Cov}(X, Y)}{s_X \\cdot s_Y}$$\n",
    "\n",
    "- $r = +1$: perfect positive linear relationship.\n",
    "- $r = -1$: perfect negative linear relationship.\n",
    "- $r = 0$: no *linear* relationship (but there could be a non-linear one!).\n",
    "- Rule of thumb: $|r| > 0.7$ is \"strong\", $0.3 < |r| < 0.7$ is \"moderate\", $|r| < 0.3$ is \"weak\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-pearson-turn",
   "metadata": {},
   "source": [
    "### Your Turn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-pearson-manual",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute Pearson correlation between GDP growth and unemployment manually\n",
    "# r = Cov(X,Y) / (std_X * std_Y)\n",
    "# Then compare with df[cols].corr()\n",
    "\n",
    "pair = df[['gdp_growth_qoq', 'UNRATE']].dropna()\n",
    "x = pair['gdp_growth_qoq']\n",
    "y = pair['UNRATE']\n",
    "\n",
    "r_manual = ...\n",
    "\n",
    "r_pandas = ...\n",
    "\n",
    "print(f'Manual Pearson r:  {r_manual:.6f}')\n",
    "print(f'Pandas Pearson r:  {r_pandas:.6f}')\n",
    "print(f'Match: {np.isclose(r_manual, r_pandas)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-pearson-corrmatrix",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute the Pearson correlation matrix for key macro variables using df.corr()\n",
    "\n",
    "macro_cols = ['gdp_growth_qoq', 'UNRATE', 'FEDFUNDS', 'CPIAUCSL', 'INDPRO']\n",
    "\n",
    "corr_pearson = ...\n",
    "\n",
    "corr_pearson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-pearson-heatmap",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Visualize the correlation matrix as a heatmap using seaborn\n",
    "# Hint: sns.heatmap(corr_matrix, annot=True, cmap='RdBu_r', center=0, vmin=-1, vmax=1)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "...\n",
    "\n",
    "ax.set_title('Pearson Correlation Matrix \u2014 Macro Variables')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-pearson-interp",
   "metadata": {},
   "source": [
    "**Interpretation prompt** (write 2\u20134 sentences below):\n",
    "- Which pair of variables has the strongest positive correlation? The strongest negative?\n",
    "- The diagonal of the correlation matrix is all 1.0. Why?\n",
    "- Does any correlation surprise you? Which relationship would you investigate further?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-spearman-header",
   "metadata": {},
   "source": [
    "<a id=\"spearman-rank-correlation\"></a>\n",
    "## Spearman Rank Correlation\n",
    "\n",
    "### Goal\n",
    "Compute Spearman rank correlation and understand when it is preferable to Pearson.\n",
    "\n",
    "### Why this matters in economics\n",
    "Pearson correlation measures *linear* association. But many economic relationships\n",
    "are monotonic without being strictly linear (e.g., diminishing returns). Spearman\n",
    "rank correlation works by ranking the data first and then computing the Pearson\n",
    "correlation on the ranks. This makes it:\n",
    "\n",
    "- **Robust to outliers**: a single extreme GDP quarter will not distort the result.\n",
    "- **Sensitive to non-linear monotonic relationships**: if $Y$ increases whenever $X$ increases (but not necessarily at a constant rate), Spearman will capture this even when Pearson underestimates it.\n",
    "- **Applicable to ordinal data**: e.g., credit ratings (AAA, AA, A, ...) can be ranked even though the \"distance\" between grades is not uniform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-spearman-turn",
   "metadata": {},
   "source": [
    "### Your Turn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-spearman-compute",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute Spearman rank correlation matrix and compare it to Pearson\n",
    "# Hint: df[macro_cols].corr(method='spearman')\n",
    "\n",
    "macro_cols = ['gdp_growth_qoq', 'UNRATE', 'FEDFUNDS', 'CPIAUCSL', 'INDPRO']\n",
    "\n",
    "corr_spearman = ...\n",
    "\n",
    "print('Spearman correlation matrix:')\n",
    "corr_spearman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-spearman-compare",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute the difference between Pearson and Spearman correlations.\n",
    "# Large differences indicate non-linear relationships or outlier effects.\n",
    "\n",
    "corr_diff = ...\n",
    "\n",
    "print('Pearson minus Spearman (large values flag non-linearity or outliers):')\n",
    "corr_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-spearman-nonlinear",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Demonstrate a case where Pearson and Spearman disagree.\n",
    "# Simulate a non-linear monotonic relationship: Y = X^3 + noise\n",
    "# Pearson will underestimate the association; Spearman will capture it.\n",
    "\n",
    "np.random.seed(42)\n",
    "n_sim = 200\n",
    "x_sim = np.random.uniform(-3, 3, n_sim)\n",
    "y_sim = ...  # TODO: x_sim ** 3 + some noise\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "r_pearson_sim = ...   # TODO: Pearson r for x_sim, y_sim\n",
    "r_spearman_sim = ...  # TODO: Spearman r for x_sim, y_sim\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 5))\n",
    "ax.scatter(x_sim, y_sim, alpha=0.5, s=20)\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y = X^3 + noise')\n",
    "ax.set_title(f'Non-linear monotonic: Pearson r = {r_pearson_sim:.3f}, Spearman r = {r_spearman_sim:.3f}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-spearman-interp",
   "metadata": {},
   "source": [
    "**Interpretation prompt** (write 2\u20134 sentences below):\n",
    "- Where do Pearson and Spearman differ most on the macro data? What might explain this?\n",
    "- In the simulated $Y = X^3$ example, why is Spearman higher than Pearson?\n",
    "- When would you choose Spearman over Pearson in an applied economics study?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-viz-header",
   "metadata": {},
   "source": [
    "<a id=\"visualizing-relationships\"></a>\n",
    "## Visualizing Relationships\n",
    "\n",
    "### Goal\n",
    "Use scatter plots and pair plots to explore bivariate relationships, and learn\n",
    "what patterns a single correlation number can miss.\n",
    "\n",
    "### Why this matters in economics\n",
    "A correlation coefficient is a single number that summarizes a relationship. But\n",
    "Anscombe's quartet famously showed that very different scatter patterns can produce\n",
    "the same correlation. In economics, you might see clusters (expansion vs recession\n",
    "regimes), outliers (financial crises), or non-linearities (diminishing returns) that\n",
    "a number alone cannot capture. Always plot before you summarize."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-viz-turn",
   "metadata": {},
   "source": [
    "### Your Turn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-viz-scatter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a scatter plot of GDP growth vs unemployment rate.\n",
    "# Color points by recession status if available.\n",
    "# Hint: use df['recession'] for the color.\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "...\n",
    "\n",
    "ax.set_xlabel('GDP Growth (QoQ %)')\n",
    "ax.set_ylabel('Unemployment Rate (%)')\n",
    "ax.set_title('GDP Growth vs Unemployment')\n",
    "ax.legend(title='Recession')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-viz-pairplot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a pairplot (scatter matrix) of 4\u20135 key macro variables.\n",
    "# Hint: sns.pairplot(df[cols].dropna(), diag_kind='kde')\n",
    "# This may take a moment to render.\n",
    "\n",
    "pairplot_cols = ['gdp_growth_qoq', 'UNRATE', 'FEDFUNDS', 'CPIAUCSL', 'INDPRO']\n",
    "\n",
    "...\n",
    "\n",
    "plt.suptitle('Pairplot of Key Macro Variables', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-viz-annotated",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a 2x2 grid of scatter plots for selected pairs.\n",
    "# Annotate each subplot with the Pearson correlation coefficient.\n",
    "\n",
    "pairs = [\n",
    "    ('gdp_growth_qoq', 'UNRATE'),\n",
    "    ('gdp_growth_qoq', 'FEDFUNDS'),\n",
    "    ('UNRATE', 'FEDFUNDS'),\n",
    "    ('CPIAUCSL', 'INDPRO'),\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "for ax, (col_x, col_y) in zip(axes.flat, pairs):\n",
    "    sub = df[[col_x, col_y]].dropna()\n",
    "    ...  # TODO: scatter plot and annotate with r value\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-viz-interp",
   "metadata": {},
   "source": [
    "**Interpretation prompt** (write 2\u20134 sentences below):\n",
    "- What patterns can you see in the scatter plots that the correlation number alone would miss?\n",
    "- Do you see any clusters, outliers, or non-linear patterns?\n",
    "- Which pair of variables has the most \"clean\" linear relationship, and which is the messiest?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-spurious-header",
   "metadata": {},
   "source": [
    "<a id=\"spurious-correlation\"></a>\n",
    "## Spurious Correlation\n",
    "\n",
    "### Goal\n",
    "Demonstrate that two completely independent random walks can appear highly\n",
    "correlated, and understand why this matters for time series analysis.\n",
    "\n",
    "### Why this matters in economics\n",
    "This is THE critical lesson of this notebook. Most macroeconomic variables are\n",
    "**non-stationary**: GDP, prices, and population all trend upward over time. If\n",
    "you compute the Pearson correlation between two trending series, you will\n",
    "almost always get a high value \u2014 even if the series are completely unrelated.\n",
    "This is called **spurious correlation**, and it has led to many false\n",
    "conclusions in economics.\n",
    "\n",
    "The classic example: GDP and the number of sunspots both trend upward, so\n",
    "their correlation is high. Does GDP depend on sunspots? Of course not.\n",
    "\n",
    "The solution is to work with **stationary** data (e.g., first differences\n",
    "or growth rates) before computing correlations. This motivates the concept\n",
    "of stationarity, which is covered in depth in the time series notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-spurious-turn",
   "metadata": {},
   "source": [
    "### Your Turn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-spurious-sim",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Simulate two INDEPENDENT random walks and compute their correlation.\n",
    "# A random walk: S_t = S_{t-1} + e_t, where e_t ~ N(0, 1)\n",
    "# Even though the innovations are independent, the cumulative sums will\n",
    "# appear correlated because both trend stochastically.\n",
    "\n",
    "np.random.seed(123)\n",
    "T = 500  # length of the series\n",
    "\n",
    "# Generate two independent white noise series\n",
    "e1 = np.random.randn(T)\n",
    "e2 = np.random.randn(T)\n",
    "\n",
    "# Cumulative sums = random walks\n",
    "rw1 = ...  # TODO: np.cumsum(e1)\n",
    "rw2 = ...  # TODO: np.cumsum(e2)\n",
    "\n",
    "# Correlation of the random walks (levels)\n",
    "r_levels = ...\n",
    "\n",
    "# Correlation of the innovations (first differences) \u2014 should be near zero\n",
    "r_diffs = ...\n",
    "\n",
    "print(f'Correlation of random walks (levels):        {r_levels:.4f}')\n",
    "print(f'Correlation of innovations (first diffs):    {r_diffs:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-spurious-plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot the two random walks on the same axes to see why they look related.\n",
    "# Then plot the innovations (first differences) to show they are not.\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: random walks (levels)\n",
    "...  # TODO: plot rw1 and rw2\n",
    "axes[0].set_title(f'Two Independent Random Walks (r = {r_levels:.3f})')\n",
    "axes[0].set_xlabel('Time')\n",
    "axes[0].legend(['Walk 1', 'Walk 2'])\n",
    "\n",
    "# Right: innovations (first differences)\n",
    "...  # TODO: plot e1 and e2\n",
    "axes[1].set_title(f'Innovations / First Differences (r = {r_diffs:.3f})')\n",
    "axes[1].set_xlabel('Time')\n",
    "axes[1].legend(['Innovation 1', 'Innovation 2'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-spurious-many",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run the experiment many times to show that spurious correlation is systematic.\n",
    "# For each trial, generate two independent random walks and record their correlation.\n",
    "# Plot the distribution of these spurious correlations.\n",
    "\n",
    "np.random.seed(0)\n",
    "n_trials = 1000\n",
    "T_trial = 200\n",
    "\n",
    "spurious_corrs = []\n",
    "for _ in range(n_trials):\n",
    "    ...  # TODO: generate two random walks, compute correlation, append\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.hist(spurious_corrs, bins=40, edgecolor='black', alpha=0.7)\n",
    "ax.axvline(0, color='red', linestyle='--', label='True correlation = 0')\n",
    "ax.set_xlabel('Spurious Correlation')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title(f'Distribution of Spurious Correlations ({n_trials} trials, T={T_trial})')\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f'Mean |r|: {np.mean(np.abs(spurious_corrs)):.3f}')\n",
    "print(f'Fraction |r| > 0.5: {np.mean(np.abs(spurious_corrs) > 0.5):.1%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-spurious-interp",
   "metadata": {},
   "source": [
    "**Interpretation prompt** (write 2\u20134 sentences below):\n",
    "- The two random walks are generated independently. Why does their correlation in levels end up so high?\n",
    "- What happens to the correlation when you use first differences instead of levels?\n",
    "- What fraction of the simulated spurious correlations exceeded $|r| > 0.5$? What does this imply for correlating trending economic series?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-causation-header",
   "metadata": {},
   "source": [
    "<a id=\"correlation-does-not-imply-causation\"></a>\n",
    "## Correlation Does Not Imply Causation\n",
    "\n",
    "### Goal\n",
    "Understand confounding variables and demonstrate how omitting them leads to\n",
    "misleading correlations and biased regression coefficients.\n",
    "\n",
    "### Why this matters in economics\n",
    "This is the most important mantra in empirical economics. Classic example:\n",
    "ice cream sales and crime both rise in summer. Does ice cream cause crime?\n",
    "No \u2014 temperature is a **confounder** that drives both. In economics, confounders\n",
    "are everywhere:\n",
    "\n",
    "- Education and wages are correlated, but ability (unobserved) drives both.\n",
    "- Government spending and GDP are correlated, but the business cycle drives both.\n",
    "- Minimum wage and unemployment are correlated, but regional economic conditions confound the relationship.\n",
    "\n",
    "The entire field of causal inference (instrumental variables, difference-in-differences,\n",
    "regression discontinuity) exists to address this problem. Here we build the\n",
    "intuition by simulating a simple confounding scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-causation-turn",
   "metadata": {},
   "source": [
    "### Your Turn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-causation-sim",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Simulate a confounding scenario.\n",
    "# Z (temperature) causes both X (ice cream sales) and Y (crime rate).\n",
    "# X does NOT cause Y. But the naive correlation between X and Y will be high.\n",
    "#\n",
    "# DGP (data generating process):\n",
    "#   Z ~ N(0, 1)           (confounder: temperature)\n",
    "#   X = 2*Z + noise_x     (ice cream sales driven by temperature)\n",
    "#   Y = 3*Z + noise_y     (crime driven by temperature, NOT by ice cream)\n",
    "\n",
    "np.random.seed(99)\n",
    "n_obs = 500\n",
    "\n",
    "Z = np.random.randn(n_obs)           # confounder\n",
    "X = ...  # TODO: 2*Z + noise\n",
    "Y = ...  # TODO: 3*Z + noise\n",
    "\n",
    "print(f'Correlation(X, Y) = {np.corrcoef(X, Y)[0, 1]:.4f}  <-- looks causal!')\n",
    "print(f'Correlation(X, Z) = {np.corrcoef(X, Z)[0, 1]:.4f}')\n",
    "print(f'Correlation(Y, Z) = {np.corrcoef(Y, Z)[0, 1]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-causation-regression",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run two regressions to show the effect of controlling for the confounder.\n",
    "# (1) Naive: Y = a + b*X (omitting Z) \u2014 b will be biased (non-zero).\n",
    "# (2) Controlled: Y = a + b*X + c*Z \u2014 b should be close to 0 (the true effect).\n",
    "#\n",
    "# Use numpy (np.linalg.lstsq) or statsmodels.\n",
    "\n",
    "# Naive regression: Y ~ X\n",
    "X_naive = np.column_stack([np.ones(n_obs), X])\n",
    "coef_naive = ...  # TODO: solve for coefficients\n",
    "\n",
    "# Controlled regression: Y ~ X + Z\n",
    "X_ctrl = np.column_stack([np.ones(n_obs), X, Z])\n",
    "coef_ctrl = ...  # TODO: solve for coefficients\n",
    "\n",
    "print('Naive regression (omitting confounder Z):')\n",
    "print(f'  Y = {coef_naive[0]:.4f} + {coef_naive[1]:.4f} * X')\n",
    "print(f'  Coefficient on X: {coef_naive[1]:.4f}  <-- biased! True effect is 0.')\n",
    "print()\n",
    "print('Controlled regression (including confounder Z):')\n",
    "print(f'  Y = {coef_ctrl[0]:.4f} + {coef_ctrl[1]:.4f} * X + {coef_ctrl[2]:.4f} * Z')\n",
    "print(f'  Coefficient on X: {coef_ctrl[1]:.4f}  <-- close to 0 (the true effect).')\n",
    "print(f'  Coefficient on Z: {coef_ctrl[2]:.4f}  <-- close to 3 (the true effect).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-causation-visual",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Visualize the confounding scenario.\n",
    "# Left panel: scatter of X vs Y (looks correlated).\n",
    "# Right panel: scatter of X vs Y, colored by Z (reveals the confounder).\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: naive scatter\n",
    "...  # TODO\n",
    "axes[0].set_xlabel('X (Ice Cream Sales)')\n",
    "axes[0].set_ylabel('Y (Crime Rate)')\n",
    "axes[0].set_title('Naive View: X and Y Look Correlated')\n",
    "\n",
    "# Right: colored by confounder Z\n",
    "...  # TODO: scatter with c=Z, cmap='coolwarm'\n",
    "axes[1].set_xlabel('X (Ice Cream Sales)')\n",
    "axes[1].set_ylabel('Y (Crime Rate)')\n",
    "axes[1].set_title('Colored by Confounder Z (Temperature)')\n",
    "plt.colorbar(axes[1].collections[0], ax=axes[1], label='Z (Temperature)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-causation-interp",
   "metadata": {},
   "source": [
    "**Interpretation prompt** (write 2\u20134 sentences below):\n",
    "- In the naive regression, what is the coefficient on X? Is it close to the true causal effect (zero)?\n",
    "- In the controlled regression, how does the coefficient on X change? Why?\n",
    "- What does the right-panel scatter (colored by Z) reveal that the left panel hides?\n",
    "- Can you think of an economic example where an omitted confounder might lead to wrong policy conclusions?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-covmatrix-header",
   "metadata": {},
   "source": [
    "<a id=\"the-covariance-matrix-and-regression\"></a>\n",
    "## The Covariance Matrix and Its Role in Regression\n",
    "\n",
    "### Goal\n",
    "Understand the covariance matrix of predictors and why high off-diagonal entries\n",
    "signal multicollinearity.\n",
    "\n",
    "### Why this matters in economics\n",
    "In OLS regression, the coefficient estimates depend on the covariance structure of\n",
    "the predictors. The OLS formula is:\n",
    "\n",
    "$$\\hat{\\beta} = (X^\\top X)^{-1} X^\\top y$$\n",
    "\n",
    "When predictors are highly correlated (multicollinearity), $X^\\top X$ becomes\n",
    "nearly singular, and the coefficient estimates become unstable \u2014 small changes\n",
    "in the data lead to large swings in the estimates. The covariance (or correlation)\n",
    "matrix is your first diagnostic tool for multicollinearity.\n",
    "\n",
    "Later, you will learn about the **Variance Inflation Factor (VIF)**, which\n",
    "quantifies multicollinearity for each predictor. But the correlation heatmap\n",
    "is where the diagnosis starts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-covmatrix-turn",
   "metadata": {},
   "source": [
    "### Your Turn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-covmatrix-compute",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Select a set of potential predictors and compute their covariance matrix\n",
    "# using np.cov().\n",
    "\n",
    "predictor_cols = ['UNRATE', 'FEDFUNDS', 'CPIAUCSL', 'INDPRO', 'T10Y2Y']\n",
    "pred_data = df[predictor_cols].dropna()\n",
    "\n",
    "# np.cov expects each variable as a ROW, so transpose\n",
    "cov_np = ...  # TODO: np.cov(pred_data.values, rowvar=False)\n",
    "\n",
    "cov_df = pd.DataFrame(cov_np, index=predictor_cols, columns=predictor_cols)\n",
    "print('Covariance matrix of predictors:')\n",
    "cov_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-covmatrix-corr",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute and visualize the correlation matrix of predictors.\n",
    "# Identify pairs with |r| > 0.7 (potential multicollinearity).\n",
    "\n",
    "corr_pred = ...\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "...  # TODO: heatmap\n",
    "ax.set_title('Predictor Correlation Matrix (Check for Multicollinearity)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Flag pairs with |r| > 0.7\n",
    "print('\\nPairs with |correlation| > 0.7:')\n",
    "for i in range(len(predictor_cols)):\n",
    "    for j in range(i + 1, len(predictor_cols)):\n",
    "        r = corr_pred.iloc[i, j]\n",
    "        if abs(r) > 0.7:\n",
    "            print(f'  {predictor_cols[i]} & {predictor_cols[j]}: r = {r:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-covmatrix-eigenvalues",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute the eigenvalues of the covariance matrix.\n",
    "# If the smallest eigenvalue is very close to zero (relative to the largest),\n",
    "# it signals near-singularity (severe multicollinearity).\n",
    "# The condition number = max_eigenvalue / min_eigenvalue.\n",
    "\n",
    "eigenvalues = ...  # TODO: np.linalg.eigvalsh(cov_np)\n",
    "\n",
    "condition_number = ...\n",
    "\n",
    "print('Eigenvalues of the covariance matrix:')\n",
    "for i, ev in enumerate(sorted(eigenvalues, reverse=True)):\n",
    "    print(f'  lambda_{i+1} = {ev:.4f}')\n",
    "print(f'\\nCondition number: {condition_number:.2f}')\n",
    "print('Rule of thumb: condition number > 30 suggests multicollinearity problems.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-covmatrix-interp",
   "metadata": {},
   "source": [
    "**Interpretation prompt** (write 2\u20134 sentences below):\n",
    "- Which predictor pairs have high correlation? Would including both in a regression be problematic?\n",
    "- What does the condition number tell you about the predictor set?\n",
    "- How does the covariance matrix relate to the OLS formula? Why does near-singularity matter?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-later",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Where This Shows Up Later\n",
    "\n",
    "- **Correlation matrix** is the first step before regression (`02_regression`). You will use it to select predictors and diagnose multicollinearity.\n",
    "- **Spurious correlation** motivates the concept of stationarity (`07_time_series_econ/00`). You must difference or detrend non-stationary series before computing meaningful correlations.\n",
    "- **Confounding** motivates the entire field of causal inference (`06_causal`). Instrumental variables, diff-in-diff, and regression discontinuity all exist to deal with confounders.\n",
    "- **Covariance matrix** appears in PCA (`04_unsupervised/01`), where eigenvectors of the covariance matrix define the principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-checkpoint-header",
   "metadata": {},
   "source": [
    "<a id=\"checkpoint-self-check\"></a>\n",
    "## Checkpoint (Self-Check)\n",
    "Run these asserts to verify your work. If any fail, go back and fix the corresponding section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-checkpoint",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Covariance checks ----\n",
    "assert isinstance(cov_manual, float), 'cov_manual should be a float'\n",
    "assert np.isclose(cov_manual, cov_pandas), 'Manual and pandas covariance should match'\n",
    "\n",
    "# ---- Pearson correlation checks ----\n",
    "assert isinstance(corr_pearson, pd.DataFrame), 'corr_pearson should be a DataFrame'\n",
    "assert corr_pearson.shape[0] == corr_pearson.shape[1], 'Correlation matrix should be square'\n",
    "assert np.allclose(np.diag(corr_pearson.values), 1.0), 'Diagonal of corr matrix should be 1.0'\n",
    "assert (corr_pearson.values >= -1).all() and (corr_pearson.values <= 1).all(), 'Correlations must be in [-1, 1]'\n",
    "\n",
    "# ---- Spearman checks ----\n",
    "assert isinstance(corr_spearman, pd.DataFrame), 'corr_spearman should be a DataFrame'\n",
    "assert corr_spearman.shape == corr_pearson.shape, 'Spearman and Pearson matrices should have same shape'\n",
    "\n",
    "# ---- Spurious correlation checks ----\n",
    "assert abs(r_diffs) < 0.2, f'Correlation of innovations should be near 0, got {r_diffs:.4f}'\n",
    "\n",
    "# ---- Confounding checks ----\n",
    "assert abs(coef_naive[1]) > 0.5, 'Naive coefficient on X should be significantly biased'\n",
    "assert abs(coef_ctrl[1]) < 0.5, 'Controlled coefficient on X should be close to 0'\n",
    "\n",
    "# ---- Covariance matrix checks ----\n",
    "assert cov_np.shape[0] == cov_np.shape[1] == len(predictor_cols), 'Cov matrix shape mismatch'\n",
    "assert np.allclose(cov_np, cov_np.T), 'Covariance matrix must be symmetric'\n",
    "\n",
    "print('All checkpoint assertions passed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-extensions",
   "metadata": {},
   "source": [
    "## Extensions (Optional)\n",
    "- Compute a **rolling correlation** between GDP growth and unemployment (e.g., 20-quarter window) and plot it over time. Has the relationship been stable?\n",
    "- Investigate **partial correlation**: the correlation between X and Y after removing the effect of Z. Compare to the simple correlation.\n",
    "- Download data from FRED for two obviously unrelated trending series (e.g., US GDP and world population) and compute their correlation. Then difference both series and recompute. This is the spurious correlation problem in real data.\n",
    "- Explore the **correlation between lagged variables**: does unemployment at time $t$ correlate more strongly with GDP growth at $t$ or at $t-1$? This foreshadows Granger causality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-reflection",
   "metadata": {},
   "source": [
    "## Reflection\n",
    "- What is the most important lesson you take away from this notebook?\n",
    "- If a colleague shows you a correlation of 0.85 between two macroeconomic series, what questions would you ask before drawing conclusions?\n",
    "- How does the spurious correlation demonstration change how you think about correlation in time series data?\n",
    "- Can you think of a policy-relevant example where confusing correlation with causation would lead to a harmful decision?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-solutions",
   "metadata": {},
   "source": [
    "<a id=\"solutions-reference\"></a>\n",
    "## Solutions (Reference)\n",
    "\n",
    "Try the TODOs first. Use these only to unblock yourself or to compare approaches.\n",
    "\n",
    "<details><summary>Solution: Covariance \u2014 measuring joint variability</summary>\n",
    "\n",
    "_One possible approach. Your variable names may differ; align them with the notebook._\n",
    "\n",
    "```python\n",
    "# Reference solution for 07 \u2014 Covariance\n",
    "pair = df[['gdp_growth_qoq', 'UNRATE']].dropna()\n",
    "x = pair['gdp_growth_qoq']\n",
    "y = pair['UNRATE']\n",
    "n = len(x)\n",
    "\n",
    "# Manual calculation\n",
    "cov_manual = ((x - x.mean()) * (y - y.mean())).sum() / (n - 1)\n",
    "\n",
    "# Pandas calculation\n",
    "cov_pandas = df[['gdp_growth_qoq', 'UNRATE']].cov().loc['gdp_growth_qoq', 'UNRATE']\n",
    "\n",
    "print(f'Manual covariance:  {cov_manual:.6f}')\n",
    "print(f'Pandas covariance:  {cov_pandas:.6f}')\n",
    "print(f'Match: {np.isclose(cov_manual, cov_pandas)}')\n",
    "\n",
    "# Full covariance matrix\n",
    "macro_cols = ['gdp_growth_qoq', 'UNRATE', 'FEDFUNDS', 'CPIAUCSL', 'INDPRO']\n",
    "cov_matrix = df[macro_cols].cov()\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<details><summary>Solution: Pearson correlation</summary>\n",
    "\n",
    "_One possible approach. Your variable names may differ; align them with the notebook._\n",
    "\n",
    "```python\n",
    "# Reference solution for 07 \u2014 Pearson correlation\n",
    "pair = df[['gdp_growth_qoq', 'UNRATE']].dropna()\n",
    "x = pair['gdp_growth_qoq']\n",
    "y = pair['UNRATE']\n",
    "\n",
    "r_manual = ((x - x.mean()) * (y - y.mean())).sum() / ((n - 1) * x.std() * y.std())\n",
    "r_pandas = df[['gdp_growth_qoq', 'UNRATE']].corr().loc['gdp_growth_qoq', 'UNRATE']\n",
    "\n",
    "# Full correlation matrix\n",
    "macro_cols = ['gdp_growth_qoq', 'UNRATE', 'FEDFUNDS', 'CPIAUCSL', 'INDPRO']\n",
    "corr_pearson = df[macro_cols].corr()\n",
    "\n",
    "# Heatmap\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.heatmap(corr_pearson, annot=True, fmt='.2f', cmap='RdBu_r',\n",
    "            center=0, vmin=-1, vmax=1, ax=ax)\n",
    "ax.set_title('Pearson Correlation Matrix \\u2014 Macro Variables')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<details><summary>Solution: Spearman rank correlation</summary>\n",
    "\n",
    "_One possible approach. Your variable names may differ; align them with the notebook._\n",
    "\n",
    "```python\n",
    "# Reference solution for 07 \u2014 Spearman rank correlation\n",
    "macro_cols = ['gdp_growth_qoq', 'UNRATE', 'FEDFUNDS', 'CPIAUCSL', 'INDPRO']\n",
    "\n",
    "corr_spearman = df[macro_cols].corr(method='spearman')\n",
    "\n",
    "corr_diff = corr_pearson - corr_spearman\n",
    "\n",
    "# Non-linear example: Y = X^3 + noise\n",
    "np.random.seed(42)\n",
    "n_sim = 200\n",
    "x_sim = np.random.uniform(-3, 3, n_sim)\n",
    "y_sim = x_sim ** 3 + np.random.randn(n_sim) * 3\n",
    "\n",
    "from scipy import stats\n",
    "r_pearson_sim = np.corrcoef(x_sim, y_sim)[0, 1]\n",
    "r_spearman_sim = stats.spearmanr(x_sim, y_sim).correlation\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<details><summary>Solution: Visualizing relationships</summary>\n",
    "\n",
    "_One possible approach. Your variable names may differ; align them with the notebook._\n",
    "\n",
    "```python\n",
    "# Reference solution for 07 \u2014 Visualizing relationships\n",
    "\n",
    "# Scatter: GDP growth vs unemployment, colored by recession\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "colors = df['recession'].map({0: 'steelblue', 1: 'red'})\n",
    "ax.scatter(df['gdp_growth_qoq'], df['UNRATE'], c=colors, alpha=0.6, s=30)\n",
    "ax.set_xlabel('GDP Growth (QoQ %)')\n",
    "ax.set_ylabel('Unemployment Rate (%)')\n",
    "ax.set_title('GDP Growth vs Unemployment')\n",
    "# Manual legend\n",
    "from matplotlib.lines import Line2D\n",
    "handles = [Line2D([0], [0], marker='o', color='w', markerfacecolor='steelblue', label='No Recession'),\n",
    "           Line2D([0], [0], marker='o', color='w', markerfacecolor='red', label='Recession')]\n",
    "ax.legend(handles=handles, title='Recession')\n",
    "plt.show()\n",
    "\n",
    "# Pairplot\n",
    "pairplot_cols = ['gdp_growth_qoq', 'UNRATE', 'FEDFUNDS', 'CPIAUCSL', 'INDPRO']\n",
    "sns.pairplot(df[pairplot_cols].dropna(), diag_kind='kde')\n",
    "plt.suptitle('Pairplot of Key Macro Variables', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# Annotated scatter grid\n",
    "pairs = [\n",
    "    ('gdp_growth_qoq', 'UNRATE'),\n",
    "    ('gdp_growth_qoq', 'FEDFUNDS'),\n",
    "    ('UNRATE', 'FEDFUNDS'),\n",
    "    ('CPIAUCSL', 'INDPRO'),\n",
    "]\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "for ax, (col_x, col_y) in zip(axes.flat, pairs):\n",
    "    sub = df[[col_x, col_y]].dropna()\n",
    "    ax.scatter(sub[col_x], sub[col_y], alpha=0.5, s=20)\n",
    "    r = sub[col_x].corr(sub[col_y])\n",
    "    ax.set_xlabel(col_x)\n",
    "    ax.set_ylabel(col_y)\n",
    "    ax.set_title(f'{col_x} vs {col_y} (r = {r:.3f})')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<details><summary>Solution: Spurious correlation</summary>\n",
    "\n",
    "_One possible approach. Your variable names may differ; align them with the notebook._\n",
    "\n",
    "```python\n",
    "# Reference solution for 07 \u2014 Spurious correlation\n",
    "np.random.seed(123)\n",
    "T = 500\n",
    "\n",
    "e1 = np.random.randn(T)\n",
    "e2 = np.random.randn(T)\n",
    "\n",
    "rw1 = np.cumsum(e1)\n",
    "rw2 = np.cumsum(e2)\n",
    "\n",
    "r_levels = np.corrcoef(rw1, rw2)[0, 1]\n",
    "r_diffs = np.corrcoef(e1, e2)[0, 1]\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "axes[0].plot(rw1, label='Walk 1')\n",
    "axes[0].plot(rw2, label='Walk 2')\n",
    "axes[0].set_title(f'Two Independent Random Walks (r = {r_levels:.3f})')\n",
    "axes[0].set_xlabel('Time')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].plot(e1, alpha=0.5, label='Innovation 1')\n",
    "axes[1].plot(e2, alpha=0.5, label='Innovation 2')\n",
    "axes[1].set_title(f'Innovations / First Differences (r = {r_diffs:.3f})')\n",
    "axes[1].set_xlabel('Time')\n",
    "axes[1].legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Many trials\n",
    "np.random.seed(0)\n",
    "n_trials = 1000\n",
    "T_trial = 200\n",
    "spurious_corrs = []\n",
    "for _ in range(n_trials):\n",
    "    a = np.cumsum(np.random.randn(T_trial))\n",
    "    b = np.cumsum(np.random.randn(T_trial))\n",
    "    spurious_corrs.append(np.corrcoef(a, b)[0, 1])\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<details><summary>Solution: Correlation does not imply causation</summary>\n",
    "\n",
    "_One possible approach. Your variable names may differ; align them with the notebook._\n",
    "\n",
    "```python\n",
    "# Reference solution for 07 \u2014 Confounding / causation\n",
    "np.random.seed(99)\n",
    "n_obs = 500\n",
    "\n",
    "Z = np.random.randn(n_obs)\n",
    "X = 2 * Z + np.random.randn(n_obs) * 0.5\n",
    "Y = 3 * Z + np.random.randn(n_obs) * 0.5\n",
    "\n",
    "# Naive regression: Y ~ X\n",
    "X_naive = np.column_stack([np.ones(n_obs), X])\n",
    "coef_naive, _, _, _ = np.linalg.lstsq(X_naive, Y, rcond=None)\n",
    "\n",
    "# Controlled regression: Y ~ X + Z\n",
    "X_ctrl = np.column_stack([np.ones(n_obs), X, Z])\n",
    "coef_ctrl, _, _, _ = np.linalg.lstsq(X_ctrl, Y, rcond=None)\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "axes[0].scatter(X, Y, alpha=0.3, s=15)\n",
    "axes[0].set_xlabel('X (Ice Cream Sales)')\n",
    "axes[0].set_ylabel('Y (Crime Rate)')\n",
    "axes[0].set_title('Naive View: X and Y Look Correlated')\n",
    "\n",
    "sc = axes[1].scatter(X, Y, c=Z, cmap='coolwarm', alpha=0.5, s=15)\n",
    "axes[1].set_xlabel('X (Ice Cream Sales)')\n",
    "axes[1].set_ylabel('Y (Crime Rate)')\n",
    "axes[1].set_title('Colored by Confounder Z (Temperature)')\n",
    "plt.colorbar(sc, ax=axes[1], label='Z (Temperature)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<details><summary>Solution: The covariance matrix and regression</summary>\n",
    "\n",
    "_One possible approach. Your variable names may differ; align them with the notebook._\n",
    "\n",
    "```python\n",
    "# Reference solution for 07 \u2014 Covariance matrix and regression\n",
    "predictor_cols = ['UNRATE', 'FEDFUNDS', 'CPIAUCSL', 'INDPRO', 'T10Y2Y']\n",
    "pred_data = df[predictor_cols].dropna()\n",
    "\n",
    "cov_np = np.cov(pred_data.values, rowvar=False)\n",
    "cov_df = pd.DataFrame(cov_np, index=predictor_cols, columns=predictor_cols)\n",
    "\n",
    "# Correlation matrix and heatmap\n",
    "corr_pred = pred_data.corr()\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.heatmap(corr_pred, annot=True, fmt='.2f', cmap='RdBu_r',\n",
    "            center=0, vmin=-1, vmax=1, ax=ax)\n",
    "ax.set_title('Predictor Correlation Matrix (Check for Multicollinearity)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Eigenvalues and condition number\n",
    "eigenvalues = np.linalg.eigvalsh(cov_np)\n",
    "condition_number = eigenvalues.max() / eigenvalues.min()\n",
    "```\n",
    "\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 5,
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}