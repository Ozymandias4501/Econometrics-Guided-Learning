{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-00",
   "metadata": {},
   "source": [
    "# 03 Z-Scores and Standardization\n",
    "\n",
    "Transforming data to a common scale: how and why we standardize, and what z-scores tell you."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-01",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [What is a z-score?](#what-is-a-z-score)\n",
    "- [Computing z-scores](#computing-z-scores)\n",
    "- [The standard normal distribution](#the-standard-normal-distribution)\n",
    "- [The empirical rule (68-95-99.7)](#the-empirical-rule)\n",
    "- [Standardization for comparing across scales](#standardization-for-comparing-across-scales)\n",
    "- [Z-scores as outlier detectors](#z-scores-as-outlier-detectors)\n",
    "- [When standardization is required vs optional](#when-standardization-is-required-vs-optional)\n",
    "- [Checkpoint (Self-Check)](#checkpoint-self-check)\n",
    "- [Solutions (Reference)](#solutions-reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-02",
   "metadata": {},
   "source": [
    "## Why This Notebook Matters\n",
    "Z-scores are one of the simplest yet most powerful tools in statistics. They let you compare\n",
    "values across different scales, identify outliers, and connect raw data to probability\n",
    "statements. Every t-statistic in a regression table is essentially a z-score. Understanding\n",
    "standardization here will make hypothesis testing and regression output far more intuitive.\n",
    "\n",
    "## Prerequisites (Quick Self-Check)\n",
    "- Completed notebooks 00-02 (descriptive statistics, distributions, CLT).\n",
    "- Understanding of mean and standard deviation.\n",
    "- Familiarity with the normal distribution.\n",
    "\n",
    "## What You Will Produce\n",
    "- (no file output; learning/analysis notebook)\n",
    "\n",
    "## Success Criteria\n",
    "- You can compute and interpret z-scores for any dataset.\n",
    "- You can use the empirical rule and know when it breaks down.\n",
    "- You can standardize multiple variables to compare them on a common scale.\n",
    "- You can use z-scores to detect outliers.\n",
    "\n",
    "## Common Pitfalls\n",
    "- Computing z-scores with population parameters when you only have a sample.\n",
    "- Applying the empirical rule to heavily skewed data.\n",
    "- Confusing standardization (z-scores) with normalization (min-max scaling).\n",
    "- Forgetting to standardize features before algorithms that require it.\n",
    "\n",
    "## Quick Fixes (When You Get Stuck)\n",
    "- If z-scores look wrong, double-check that you used the correct mean and std.\n",
    "- If `scipy.stats.zscore` gives unexpected results, check for NaN values first.\n",
    "- If you see `ModuleNotFoundError`, re-run the bootstrap cell.\n",
    "\n",
    "## Matching Guide\n",
    "- `docs/guides/00a_statistics_primer/03_z_scores_and_standardization.md`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-03",
   "metadata": {},
   "source": [
    "## How To Use This Notebook\n",
    "- Work section-by-section; don't skip the markdown.\n",
    "- Most code cells are incomplete on purpose: replace TODOs and `...`, then run.\n",
    "- After each section, write 2\u20134 sentences answering the interpretation prompts (what changed, why it matters).\n",
    "- Prefer `data/processed/*` if you have built the real datasets; otherwise use the bundled `data/sample/*` fallbacks.\n",
    "- Use the **Checkpoint (Self-Check)** section to catch mistakes early.\n",
    "- Use **Solutions (Reference)** only to unblock yourself; then re-implement without looking.\n",
    "- Use the matching guide (`docs/guides/00a_statistics_primer/03_z_scores_and_standardization.md`) for the math, assumptions, and deeper context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-04",
   "metadata": {},
   "source": [
    "<a id=\"environment-bootstrap\"></a>\n",
    "## Environment Bootstrap\n",
    "Run this cell first. It makes the repo importable and defines common directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "\n",
    "def find_repo_root(start: Path) -> Path:\n",
    "    p = start\n",
    "    for _ in range(8):\n",
    "        if (p / 'src').exists() and (p / 'docs').exists():\n",
    "            return p\n",
    "        p = p.parent\n",
    "    raise RuntimeError('Could not find repo root. Start Jupyter from the repo root.')\n",
    "\n",
    "\n",
    "PROJECT_ROOT = find_repo_root(Path.cwd())\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "RAW_DIR = DATA_DIR / 'raw'\n",
    "PROCESSED_DIR = DATA_DIR / 'processed'\n",
    "SAMPLE_DIR = DATA_DIR / 'sample'\n",
    "\n",
    "PROJECT_ROOT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-06",
   "metadata": {},
   "source": [
    "<a id=\"what-is-a-z-score\"></a>\n",
    "## What is a z-score?\n",
    "\n",
    "### Goal\n",
    "Understand the definition and intuition behind z-scores before computing anything.\n",
    "\n",
    "### Why this matters in economics\n",
    "Every t-statistic you see in a regression table is a z-score: it measures how many standard\n",
    "errors a coefficient is away from zero. GDP growth of -4% is meaningless without context;\n",
    "saying it is 3.2 standard deviations below the mean immediately tells you it is extreme.\n",
    "\n",
    "### Definition\n",
    "\n",
    "The **z-score** (or **standard score**) of an observation $x$ is:\n",
    "\n",
    "$$z = \\frac{x - \\mu}{\\sigma}$$\n",
    "\n",
    "where $\\mu$ is the mean and $\\sigma$ is the standard deviation.\n",
    "\n",
    "For a sample, we use $\\bar{x}$ and $s$:\n",
    "\n",
    "$$z = \\frac{x - \\bar{x}}{s}$$\n",
    "\n",
    "### Intuition\n",
    "\n",
    "A z-score answers: **\"How many standard deviations away from the mean is this value?\"**\n",
    "\n",
    "| z-score | Interpretation |\n",
    "|---------|---------------|\n",
    "| z = 0   | Exactly at the mean |\n",
    "| z = +1  | One standard deviation above the mean |\n",
    "| z = -1  | One standard deviation below the mean |\n",
    "| z = +2  | Two standard deviations above the mean (unusually high) |\n",
    "| z = -2  | Two standard deviations below the mean (unusually low) |\n",
    "| z = +3  | Three standard deviations above (very rare if data is normal) |\n",
    "\n",
    "**Key properties:**\n",
    "- Positive z = above average; Negative z = below average.\n",
    "- The magnitude tells you how unusual the value is.\n",
    "- Z-scores are **unitless**: GDP growth in % and unemployment in % both become \"number of standard deviations.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-07",
   "metadata": {},
   "source": [
    "<a id=\"computing-z-scores\"></a>\n",
    "## Computing z-scores\n",
    "\n",
    "### Goal\n",
    "Compute z-scores by hand for a few values, then use `scipy.stats.zscore` on real economic data.\n",
    "Identify which quarters had extreme GDP growth.\n",
    "\n",
    "### Why this matters in economics\n",
    "Extreme GDP growth quarters often correspond to recessions (large negative z) or recovery\n",
    "booms (large positive z). Z-scores give a quantitative threshold for what counts as\n",
    "\"extreme\" rather than relying on subjective judgment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-08",
   "metadata": {},
   "source": [
    "### Your Turn (1): Z-scores by hand\n",
    "\n",
    "Given the following five GDP growth rates (annualized, in percent): 2.5, 3.1, -0.8, 4.2, 1.0\n",
    "\n",
    "Compute the z-score for each value manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Five hypothetical GDP growth rates (annualized %)\n",
    "values = np.array([2.5, 3.1, -0.8, 4.2, 1.0])\n",
    "\n",
    "# TODO: Compute the mean and standard deviation of these values\n",
    "mean_val = ...\n",
    "std_val = ...  # use ddof=0 for population std (all five are the \"population\" here)\n",
    "\n",
    "print(f'Mean: {mean_val:.2f}')\n",
    "print(f'Std:  {std_val:.2f}')\n",
    "\n",
    "# TODO: Compute z-scores by applying the formula z = (x - mean) / std\n",
    "z_manual = ...\n",
    "\n",
    "for val, z in zip(values, z_manual):\n",
    "    print(f'  GDP growth = {val:+.1f}%  ->  z = {z:+.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "**Interpretation prompt:** Which value has the most negative z-score? What does that tell you\n",
    "about that quarter's GDP growth relative to the others?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "### Your Turn (2): Z-scores with scipy on real data\n",
    "\n",
    "Load the macro quarterly sample data and compute z-scores for the `gdp_growth_qoq_annualized`\n",
    "column. Identify quarters with extreme GDP growth (|z| > 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(SAMPLE_DIR / 'macro_quarterly_sample.csv', index_col=0, parse_dates=True)\n",
    "print(f'Shape: {df.shape}')\n",
    "print(f'Date range: {df.index.min()} to {df.index.max()}')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Extract the GDP growth column and drop NaN values\n",
    "gdp_growth = df['gdp_growth_qoq_annualized'].dropna()\n",
    "\n",
    "# TODO: Compute z-scores using scipy.stats.zscore\n",
    "# Hint: stats.zscore(gdp_growth) uses ddof=0 by default\n",
    "z_gdp = ...\n",
    "\n",
    "# Attach z-scores back for inspection\n",
    "gdp_df = pd.DataFrame({\n",
    "    'gdp_growth': gdp_growth,\n",
    "    'z_score': z_gdp\n",
    "})\n",
    "\n",
    "print(f'Mean z-score: {z_gdp.mean():.4f}  (should be ~0)')\n",
    "print(f'Std z-score:  {z_gdp.std():.4f}  (should be ~1)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Identify quarters with |z| > 2 (extreme GDP growth)\n",
    "extreme_quarters = ...\n",
    "\n",
    "print(f'Number of extreme quarters (|z| > 2): {len(extreme_quarters)}')\n",
    "print(f'Total quarters: {len(gdp_df)}')\n",
    "print(f'Percentage extreme: {100 * len(extreme_quarters) / len(gdp_df):.1f}%')\n",
    "print()\n",
    "print('Extreme quarters:')\n",
    "extreme_quarters.sort_values('z_score')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "**Interpretation prompt:**\n",
    "1. Do the extreme negative z-score quarters correspond to known recessions?\n",
    "2. What about the extreme positive quarters -- do they correspond to recovery periods?\n",
    "3. The empirical rule says about 5% of observations should have |z| > 2 for normal data.\n",
    "   Is your percentage close to 5%? What might explain any difference?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "<a id=\"the-standard-normal-distribution\"></a>\n",
    "## The standard normal distribution\n",
    "\n",
    "### Goal\n",
    "Understand that z-scores convert any normal variable to $N(0, 1)$. Plot the standard normal\n",
    "PDF and use the CDF to answer probability questions.\n",
    "\n",
    "### Why this matters in economics\n",
    "When you see a p-value in a regression table, the software computed it by comparing a\n",
    "test statistic (essentially a z-score) to the standard normal or t-distribution.\n",
    "Understanding $N(0, 1)$ makes those p-values concrete."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "### Your Turn (1): Plot the standard normal PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "# TODO: Create an array of x values from -4 to 4\n",
    "x = ...\n",
    "\n",
    "# TODO: Compute the standard normal PDF at each x value\n",
    "pdf_vals = ...\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 5))\n",
    "ax.plot(x, pdf_vals, 'k-', lw=2, label='N(0, 1) PDF')\n",
    "\n",
    "# Shade regions for 1, 2, 3 standard deviations\n",
    "for n_sd, color, alpha in [(1, 'steelblue', 0.4), (2, 'orange', 0.3), (3, 'green', 0.2)]:\n",
    "    mask = (x >= -n_sd) & (x <= n_sd)\n",
    "    ax.fill_between(x[mask], pdf_vals[mask], alpha=alpha, color=color,\n",
    "                    label=f'{n_sd} SD')\n",
    "\n",
    "ax.set_xlabel('z-score')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('Standard Normal Distribution N(0, 1)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "### Your Turn (2): Probability questions using the CDF\n",
    "\n",
    "Use `norm.cdf()` to answer:\n",
    "1. What fraction of observations fall within 1 standard deviation of the mean?\n",
    "2. Within 2 standard deviations?\n",
    "3. Within 3 standard deviations?\n",
    "4. What is the probability of observing z > 1.96? (This is the basis for the 5% significance level.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute P(-1 < Z < 1) using norm.cdf\n",
    "# Hint: P(a < Z < b) = norm.cdf(b) - norm.cdf(a)\n",
    "within_1sd = ...\n",
    "\n",
    "# TODO: Compute P(-2 < Z < 2)\n",
    "within_2sd = ...\n",
    "\n",
    "# TODO: Compute P(-3 < Z < 3)\n",
    "within_3sd = ...\n",
    "\n",
    "# TODO: Compute P(Z > 1.96)\n",
    "above_196 = ...\n",
    "\n",
    "print(f'P(-1 < Z < 1) = {within_1sd:.4f}  (empirical rule says ~0.68)')\n",
    "print(f'P(-2 < Z < 2) = {within_2sd:.4f}  (empirical rule says ~0.95)')\n",
    "print(f'P(-3 < Z < 3) = {within_3sd:.4f}  (empirical rule says ~0.997)')\n",
    "print(f'P(Z > 1.96)   = {above_196:.4f}  (this is the one-tail 2.5%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "**Interpretation prompt:** The exact values from the CDF are close to but not exactly 68%,\n",
    "95%, 99.7%. Why is the empirical rule stated as approximate numbers? Why is z = 1.96\n",
    "(not 2.0) the critical value for 95% two-tailed tests?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "<a id=\"the-empirical-rule\"></a>\n",
    "## The empirical rule (68-95-99.7)\n",
    "\n",
    "### Goal\n",
    "Verify the 68-95-99.7 rule on simulated normal data AND on real economic data.\n",
    "Discuss when the rule breaks down.\n",
    "\n",
    "### Why this matters in economics\n",
    "Economic data is often approximately -- but not exactly -- normal. Fat tails are common:\n",
    "financial crises, pandemics, and policy shocks generate observations far beyond what the\n",
    "normal distribution predicts. Knowing when the rule holds (and when it doesn't) is critical\n",
    "for risk assessment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "### Your Turn (1): Verify on simulated normal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(42)\n",
    "\n",
    "# TODO: Generate 10,000 draws from a normal distribution\n",
    "sim_data = ...\n",
    "\n",
    "# TODO: Compute z-scores for the simulated data\n",
    "sim_z = ...\n",
    "\n",
    "# TODO: Compute the percentage of observations within 1, 2, 3 standard deviations\n",
    "pct_1sd = ...\n",
    "pct_2sd = ...\n",
    "pct_3sd = ...\n",
    "\n",
    "print('--- Simulated Normal Data (n=10,000) ---')\n",
    "print(f'Within 1 SD: {pct_1sd:.2f}%  (expect ~68.27%)')\n",
    "print(f'Within 2 SD: {pct_2sd:.2f}%  (expect ~95.45%)')\n",
    "print(f'Within 3 SD: {pct_3sd:.2f}%  (expect ~99.73%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "### Your Turn (2): Verify on real GDP growth data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the z-scores computed earlier for GDP growth\n",
    "\n",
    "# TODO: Compute actual percentages within 1, 2, 3 SD for real GDP growth\n",
    "gdp_pct_1sd = ...\n",
    "gdp_pct_2sd = ...\n",
    "gdp_pct_3sd = ...\n",
    "\n",
    "print('--- Real GDP Growth Data ---')\n",
    "print(f'Within 1 SD: {gdp_pct_1sd:.2f}%  (rule says ~68%)')\n",
    "print(f'Within 2 SD: {gdp_pct_2sd:.2f}%  (rule says ~95%)')\n",
    "print(f'Within 3 SD: {gdp_pct_3sd:.2f}%  (rule says ~99.7%)')\n",
    "print()\n",
    "\n",
    "# Summary comparison table\n",
    "comparison = pd.DataFrame({\n",
    "    'Rule': [68.27, 95.45, 99.73],\n",
    "    'Simulated': [pct_1sd, pct_2sd, pct_3sd],\n",
    "    'GDP Growth': [gdp_pct_1sd, gdp_pct_2sd, gdp_pct_3sd]\n",
    "}, index=['1 SD', '2 SD', '3 SD'])\n",
    "comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": [
    "**Interpretation prompt:**\n",
    "1. Does the empirical rule hold well for the simulated data? Why?\n",
    "2. How closely does GDP growth follow the rule? If there are deviations, what might cause them?\n",
    "3. In which direction would heavy tails cause the rule to break -- would you see *more* or\n",
    "   *fewer* extreme observations than the rule predicts?\n",
    "4. If you were building a risk model, would you trust the normal-distribution percentages\n",
    "   for tail events? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-28",
   "metadata": {},
   "source": [
    "<a id=\"standardization-for-comparing-across-scales\"></a>\n",
    "## Standardization for comparing across scales\n",
    "\n",
    "### Goal\n",
    "Standardize multiple economic indicators with different units and plot them on the same\n",
    "axis to reveal co-movement patterns.\n",
    "\n",
    "### Why this matters in economics\n",
    "GDP growth is measured in percent, industrial production is an index, retail sales are in\n",
    "billions of dollars, and interest rates are in percentage points. You cannot directly compare\n",
    "these on the same chart. Standardization (converting to z-scores) puts them all in \"standard\n",
    "deviation units,\" making visual comparison meaningful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-29",
   "metadata": {},
   "source": [
    "### Your Turn (1): Standardize multiple columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select key economic indicators (different units and scales)\n",
    "indicators = ['gdp_growth_qoq_annualized', 'UNRATE', 'FEDFUNDS', 'T10Y2Y']\n",
    "\n",
    "df_raw = df[indicators].dropna()\n",
    "\n",
    "print('--- Raw data summary (different scales) ---')\n",
    "df_raw.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Standardize each column to z-scores\n",
    "# Hint: For a DataFrame, (df - df.mean()) / df.std() works column-wise\n",
    "df_standardized = ...\n",
    "\n",
    "print('--- Standardized data summary (should have mean~0, std~1) ---')\n",
    "df_standardized.describe().round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-32",
   "metadata": {},
   "source": [
    "### Your Turn (2): Plot raw vs standardized to see the difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-33",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Left panel: raw data (hard to compare because scales differ)\n",
    "# TODO: Plot all indicators in df_raw on axes[0]\n",
    "...\n",
    "axes[0].set_title('Raw Indicators (different scales)')\n",
    "axes[0].set_ylabel('Original units')\n",
    "axes[0].legend(loc='upper left', fontsize=8)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Right panel: standardized data (easy to compare)\n",
    "# TODO: Plot all indicators in df_standardized on axes[1]\n",
    "...\n",
    "axes[1].set_title('Standardized Indicators (z-scores)')\n",
    "axes[1].set_ylabel('Standard deviations from mean')\n",
    "axes[1].axhline(0, color='black', lw=0.8, ls='--')\n",
    "axes[1].legend(loc='upper left', fontsize=8)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-34",
   "metadata": {},
   "source": [
    "### Your Turn (3): Identify co-movement patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute the correlation matrix of the STANDARDIZED indicators.\n",
    "# Note: correlations are the same whether you standardize or not!\n",
    "# But the standardized plot makes co-movement visually obvious.\n",
    "corr_matrix = ...\n",
    "\n",
    "print('Correlation matrix of economic indicators:')\n",
    "corr_matrix.round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-36",
   "metadata": {},
   "source": [
    "**Interpretation prompt:**\n",
    "1. Which pairs of indicators move together (positive correlation)? Which move in opposite\n",
    "   directions (negative correlation)?\n",
    "2. Does the relationship between unemployment (UNRATE) and GDP growth match your economic\n",
    "   intuition? (Hint: think about Okun's Law.)\n",
    "3. Why is standardization essential for the visual comparison even though it does not\n",
    "   change the correlation coefficients?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-37",
   "metadata": {},
   "source": [
    "<a id=\"z-scores-as-outlier-detectors\"></a>\n",
    "## Z-scores as outlier detectors\n",
    "\n",
    "### Goal\n",
    "Use z-scores to flag outliers in economic data. Cross-reference extreme observations with\n",
    "known economic events.\n",
    "\n",
    "### Why this matters in economics\n",
    "Outliers in economic data are not random noise -- they often correspond to recessions,\n",
    "financial crises, or policy interventions. Identifying them systematically (rather than\n",
    "eyeballing) is the first step toward deciding whether to include, transform, or model\n",
    "them separately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-38",
   "metadata": {},
   "source": [
    "### Your Turn (1): Flag outliers across multiple indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the standardized DataFrame from above\n",
    "# Threshold: |z| > 2.5 is a common outlier cutoff\n",
    "\n",
    "threshold = 2.5\n",
    "\n",
    "# TODO: Create a boolean DataFrame where True = outlier (|z| > threshold)\n",
    "outlier_flags = ...\n",
    "\n",
    "# TODO: Count how many outliers each indicator has\n",
    "outlier_counts = ...\n",
    "print(f'Outlier counts (|z| > {threshold}):')\n",
    "print(outlier_counts)\n",
    "print()\n",
    "\n",
    "# TODO: Find quarters where ANY indicator is an outlier\n",
    "any_outlier = ...\n",
    "outlier_quarters = df_standardized[any_outlier]\n",
    "print(f'Quarters with at least one outlier: {any_outlier.sum()} out of {len(df_standardized)}')\n",
    "outlier_quarters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-39a",
   "metadata": {},
   "source": [
    "### Your Turn (2): Cross-reference with recession indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset has a 'recession' column (1 = recession quarter, 0 = expansion)\n",
    "\n",
    "# TODO: For the outlier quarters identified above, check whether they overlap with recessions\n",
    "outlier_dates = outlier_quarters.index\n",
    "recession_status = df.loc[outlier_dates, 'recession'] if 'recession' in df.columns else None\n",
    "\n",
    "if recession_status is not None:\n",
    "    outlier_detail = pd.DataFrame({\n",
    "        'recession': recession_status,\n",
    "        'gdp_growth': df.loc[outlier_dates, 'gdp_growth_qoq_annualized'],\n",
    "        'z_gdp': df_standardized.loc[outlier_dates, 'gdp_growth_qoq_annualized'],\n",
    "        'z_unrate': df_standardized.loc[outlier_dates, 'UNRATE']\n",
    "    })\n",
    "    print(outlier_detail.round(2))\n",
    "else:\n",
    "    print('No recession column found -- skip cross-referencing.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-41",
   "metadata": {},
   "source": [
    "### Your Turn (3): Visualize outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-42",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "# Plot GDP growth z-scores over time\n",
    "z_gdp_full = df_standardized['gdp_growth_qoq_annualized']\n",
    "ax.plot(z_gdp_full.index, z_gdp_full.values, 'b-', lw=1, label='GDP growth (z-score)')\n",
    "\n",
    "# TODO: Highlight outlier quarters (|z| > threshold) as red dots\n",
    "...\n",
    "\n",
    "# Draw threshold lines\n",
    "ax.axhline(threshold, color='red', ls='--', lw=0.8, alpha=0.7, label=f'z = +/- {threshold}')\n",
    "ax.axhline(-threshold, color='red', ls='--', lw=0.8, alpha=0.7)\n",
    "ax.axhline(0, color='black', ls='-', lw=0.5)\n",
    "\n",
    "# TODO: Shade recession periods if the 'recession' column exists\n",
    "if 'recession' in df.columns:\n",
    "    rec = df['recession'].reindex(z_gdp_full.index)\n",
    "    ax.fill_between(z_gdp_full.index, ax.get_ylim()[0], ax.get_ylim()[1],\n",
    "                    where=rec == 1, alpha=0.15, color='gray', label='Recession')\n",
    "\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Z-score')\n",
    "ax.set_title('GDP Growth Z-Scores with Outlier Detection')\n",
    "ax.legend(loc='lower left', fontsize=8)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-43",
   "metadata": {},
   "source": [
    "**Interpretation prompt:**\n",
    "1. Do the z-score outliers cluster around recession periods?\n",
    "2. Are there any outliers that are *not* during recessions? What might explain them?\n",
    "3. Would a stricter threshold (|z| > 3) miss important events?\n",
    "4. Is the z-score method appropriate for heavily skewed data, or should you consider\n",
    "   alternative outlier detection methods (e.g., IQR-based)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-44",
   "metadata": {},
   "source": [
    "<a id=\"when-standardization-is-required-vs-optional\"></a>\n",
    "## When standardization is required vs optional\n",
    "\n",
    "### Goal\n",
    "Understand which machine learning and econometric methods require standardized inputs\n",
    "and which do not.\n",
    "\n",
    "### Why this matters in economics\n",
    "Using the wrong preprocessing can silently degrade your model or mislead interpretation.\n",
    "For example, running Ridge regression without standardizing features gives the penalty\n",
    "disproportionate influence over large-scale variables, biasing which coefficients get shrunk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-45",
   "metadata": {},
   "source": [
    "### Summary table\n",
    "\n",
    "| Method | Standardization needed? | Why |\n",
    "|--------|------------------------|-----|\n",
    "| OLS (ordinary least squares) | Optional | Coefficients adjust to scale; but standardized coefficients aid interpretation |\n",
    "| Ridge / Lasso regression | **Required** | Penalty treats all coefficients equally; different scales bias the penalty |\n",
    "| PCA (principal component analysis) | **Required** | PCA maximizes variance; high-scale variables dominate without standardization |\n",
    "| K-Means clustering | **Required** | Distance-based; large-scale features dominate distance calculations |\n",
    "| Logistic regression (with regularization) | **Required** | Same reason as Ridge/Lasso |\n",
    "| Decision trees / Random forests | Not needed | Splits are based on thresholds within each feature; scale-invariant |\n",
    "| Gradient boosting (XGBoost, etc.) | Not needed | Also tree-based and scale-invariant |\n",
    "\n",
    "**Rule of thumb:** If the algorithm uses *distances* or *penalties* that combine features,\n",
    "you need standardization. If the algorithm makes *one feature at a time* decisions (trees),\n",
    "you do not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-46",
   "metadata": {},
   "source": [
    "### Your Turn: Demonstrate the effect of standardization on Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Prepare features and target\n",
    "feature_cols = ['UNRATE', 'FEDFUNDS', 'INDPRO', 'CPIAUCSL', 'T10Y2Y']\n",
    "target_col = 'gdp_growth_qoq_annualized'\n",
    "\n",
    "df_model = df[feature_cols + [target_col]].dropna()\n",
    "X = df_model[feature_cols]\n",
    "y = df_model[target_col]\n",
    "\n",
    "# TODO: Fit Ridge WITHOUT standardizing. Print coefficients.\n",
    "ridge_raw = Ridge(alpha=1.0)\n",
    "ridge_raw.fit(X, y)\n",
    "coef_raw = pd.Series(ridge_raw.coef_, index=feature_cols, name='Raw')\n",
    "\n",
    "# TODO: Fit Ridge WITH standardized features. Print coefficients.\n",
    "scaler = StandardScaler()\n",
    "X_scaled = ...\n",
    "\n",
    "ridge_scaled = Ridge(alpha=1.0)\n",
    "ridge_scaled.fit(X_scaled, y)\n",
    "coef_scaled = pd.Series(ridge_scaled.coef_, index=feature_cols, name='Standardized')\n",
    "\n",
    "# Compare\n",
    "comparison_df = pd.DataFrame({'Raw_coef': coef_raw, 'Standardized_coef': coef_scaled})\n",
    "print('Ridge coefficients: raw vs standardized features')\n",
    "comparison_df.round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-48",
   "metadata": {},
   "source": [
    "**Interpretation prompt:**\n",
    "1. How do the coefficients differ between the raw and standardized versions?\n",
    "2. Which set of coefficients is more meaningful for comparing relative feature importance?\n",
    "3. Why does the Ridge penalty distort results when features have different scales?\n",
    "4. In the later regularization notebook (05_regularization_ridge_lasso), you will see this\n",
    "   in a full pipeline. Keep this demonstration in mind."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-49",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Where This Shows Up Later\n",
    "\n",
    "The concepts from this notebook are foundational for many later topics:\n",
    "\n",
    "- **Regression notebooks (02_regression):** Every t-statistic in a regression summary is\n",
    "  essentially a z-score: $t = \\hat{\\beta} / SE(\\hat{\\beta})$. Understanding z-scores makes\n",
    "  hypothesis testing intuitive.\n",
    "- **Regularization (02_regression/05_regularization_ridge_lasso):** Ridge and Lasso require\n",
    "  standardized features. The demonstration above previews why.\n",
    "- **PCA and unsupervised learning (04_unsupervised/01_pca_macro_factors):** PCA operates on\n",
    "  variance. Without standardization, the highest-variance variable dominates the first\n",
    "  principal component regardless of its economic importance.\n",
    "- **Anomaly detection (04_unsupervised/03_anomaly_detection):** Z-score-based outlier\n",
    "  detection is the simplest anomaly detection method. Later notebooks build more\n",
    "  sophisticated approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-50",
   "metadata": {},
   "source": [
    "<a id=\"checkpoint-self-check\"></a>\n",
    "## Checkpoint (Self-Check)\n",
    "\n",
    "Run these assertions to verify your work. If any fail, revisit the corresponding section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Self-check assertions ---\n",
    "\n",
    "# 1. Manual z-score: check that the mean of z-scores is approximately 0\n",
    "assert abs(np.mean(z_manual)) < 1e-10, 'Mean of z-scores should be ~0'\n",
    "\n",
    "# 2. scipy z-scores: mean should be ~0, std ~1\n",
    "assert abs(z_gdp.mean()) < 0.01, 'scipy z-score mean should be ~0'\n",
    "assert abs(z_gdp.std() - 1.0) < 0.05, 'scipy z-score std should be ~1'\n",
    "\n",
    "# 3. CDF values should match known results\n",
    "assert abs(within_1sd - 0.6827) < 0.001, 'P(-1 < Z < 1) should be ~0.6827'\n",
    "assert abs(within_2sd - 0.9545) < 0.001, 'P(-2 < Z < 2) should be ~0.9545'\n",
    "\n",
    "# 4. Standardized DataFrame should have mean ~0 and std ~1\n",
    "assert (df_standardized.mean().abs() < 0.01).all(), 'Standardized means should be ~0'\n",
    "assert ((df_standardized.std() - 1.0).abs() < 0.05).all(), 'Standardized stds should be ~1'\n",
    "\n",
    "print('All checks passed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-52",
   "metadata": {},
   "source": [
    "## Extensions (Optional)\n",
    "\n",
    "1. **Robust z-scores:** Instead of mean/std, use median and MAD (median absolute deviation).\n",
    "   Compare outlier detection results. When would robust z-scores be preferable?\n",
    "2. **Min-max normalization:** Implement min-max scaling and compare with z-score\n",
    "   standardization. When is each appropriate?\n",
    "3. **Rolling z-scores:** Compute z-scores using a rolling window (e.g., 20 quarters) instead\n",
    "   of the full sample. How does this change outlier detection? Why might rolling z-scores\n",
    "   be more appropriate for non-stationary economic data?\n",
    "4. **Skewness and kurtosis:** Compute the skewness and kurtosis of GDP growth. How do these\n",
    "   relate to the empirical rule deviations you found?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-53",
   "metadata": {},
   "source": [
    "## Reflection\n",
    "\n",
    "Write 2-4 sentences on each:\n",
    "\n",
    "1. When you see a t-statistic of 2.5 in a regression table, what does that mean in terms\n",
    "   of z-scores and the standard normal distribution?\n",
    "2. If you were comparing the volatility of GDP growth across countries with different\n",
    "   average growth rates, why would z-scores be more appropriate than raw values?\n",
    "3. What is one situation where you would NOT want to standardize your data before modeling?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-54",
   "metadata": {},
   "source": [
    "<a id=\"solutions-reference\"></a>\n",
    "## Solutions (Reference)\n",
    "\n",
    "Try the TODOs first. Use these only to unblock yourself or to compare approaches.\n",
    "\n",
    "<details><summary>Solution: Computing z-scores by hand</summary>\n",
    "\n",
    "_One possible approach. Your variable names may differ; align them with the notebook._\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "values = np.array([2.5, 3.1, -0.8, 4.2, 1.0])\n",
    "\n",
    "mean_val = values.mean()          # 2.0\n",
    "std_val = values.std(ddof=0)      # ~1.664\n",
    "\n",
    "z_manual = (values - mean_val) / std_val\n",
    "# z_manual: [ 0.30,  0.66, -1.68,  1.32, -0.60]\n",
    "\n",
    "for val, z in zip(values, z_manual):\n",
    "    print(f'  GDP growth = {val:+.1f}%  ->  z = {z:+.2f}')\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<details><summary>Solution: Z-scores with scipy on real data</summary>\n",
    "\n",
    "_One possible approach._\n",
    "\n",
    "```python\n",
    "from scipy import stats\n",
    "\n",
    "gdp_growth = df['gdp_growth_qoq_annualized'].dropna()\n",
    "\n",
    "z_gdp = stats.zscore(gdp_growth)  # ddof=0 by default\n",
    "\n",
    "gdp_df = pd.DataFrame({\n",
    "    'gdp_growth': gdp_growth,\n",
    "    'z_score': z_gdp\n",
    "})\n",
    "\n",
    "# Extreme quarters\n",
    "extreme_quarters = gdp_df[gdp_df['z_score'].abs() > 2]\n",
    "extreme_quarters.sort_values('z_score')\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<details><summary>Solution: Standard normal PDF and CDF</summary>\n",
    "\n",
    "_One possible approach._\n",
    "\n",
    "```python\n",
    "from scipy.stats import norm\n",
    "\n",
    "x = np.linspace(-4, 4, 500)\n",
    "pdf_vals = norm.pdf(x)\n",
    "\n",
    "within_1sd = norm.cdf(1) - norm.cdf(-1)   # 0.6827\n",
    "within_2sd = norm.cdf(2) - norm.cdf(-2)   # 0.9545\n",
    "within_3sd = norm.cdf(3) - norm.cdf(-3)   # 0.9973\n",
    "above_196 = 1 - norm.cdf(1.96)             # 0.0250\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<details><summary>Solution: Empirical rule verification</summary>\n",
    "\n",
    "_One possible approach._\n",
    "\n",
    "```python\n",
    "rng = np.random.default_rng(42)\n",
    "sim_data = rng.normal(loc=0, scale=1, size=10_000)\n",
    "sim_z = (sim_data - sim_data.mean()) / sim_data.std()\n",
    "\n",
    "pct_1sd = 100 * np.mean(np.abs(sim_z) <= 1)\n",
    "pct_2sd = 100 * np.mean(np.abs(sim_z) <= 2)\n",
    "pct_3sd = 100 * np.mean(np.abs(sim_z) <= 3)\n",
    "\n",
    "# For GDP growth z-scores (z_gdp computed earlier):\n",
    "gdp_pct_1sd = 100 * np.mean(np.abs(z_gdp) <= 1)\n",
    "gdp_pct_2sd = 100 * np.mean(np.abs(z_gdp) <= 2)\n",
    "gdp_pct_3sd = 100 * np.mean(np.abs(z_gdp) <= 3)\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<details><summary>Solution: Standardization for comparing across scales</summary>\n",
    "\n",
    "_One possible approach._\n",
    "\n",
    "```python\n",
    "indicators = ['gdp_growth_qoq_annualized', 'UNRATE', 'FEDFUNDS', 'T10Y2Y']\n",
    "df_raw = df[indicators].dropna()\n",
    "\n",
    "df_standardized = (df_raw - df_raw.mean()) / df_raw.std()\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "df_raw.plot(ax=axes[0])\n",
    "axes[0].set_title('Raw Indicators (different scales)')\n",
    "\n",
    "df_standardized.plot(ax=axes[1])\n",
    "axes[1].set_title('Standardized Indicators (z-scores)')\n",
    "axes[1].axhline(0, color='black', lw=0.8, ls='--')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Correlation\n",
    "corr_matrix = df_standardized.corr()\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<details><summary>Solution: Z-scores as outlier detectors</summary>\n",
    "\n",
    "_One possible approach._\n",
    "\n",
    "```python\n",
    "threshold = 2.5\n",
    "\n",
    "outlier_flags = df_standardized.abs() > threshold\n",
    "outlier_counts = outlier_flags.sum()\n",
    "\n",
    "any_outlier = outlier_flags.any(axis=1)\n",
    "outlier_quarters = df_standardized[any_outlier]\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<details><summary>Solution: Ridge with and without standardization</summary>\n",
    "\n",
    "_One possible approach._\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "feature_cols = ['UNRATE', 'FEDFUNDS', 'INDPRO', 'CPIAUCSL', 'T10Y2Y']\n",
    "target_col = 'gdp_growth_qoq_annualized'\n",
    "\n",
    "df_model = df[feature_cols + [target_col]].dropna()\n",
    "X = df_model[feature_cols]\n",
    "y = df_model[target_col]\n",
    "\n",
    "# Without standardization\n",
    "ridge_raw = Ridge(alpha=1.0).fit(X, y)\n",
    "\n",
    "# With standardization\n",
    "scaler = StandardScaler()\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=feature_cols, index=X.index)\n",
    "\n",
    "ridge_scaled = Ridge(alpha=1.0).fit(X_scaled, y)\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Raw_coef': ridge_raw.coef_,\n",
    "    'Standardized_coef': ridge_scaled.coef_\n",
    "}, index=feature_cols)\n",
    "```\n",
    "\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 5,
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
