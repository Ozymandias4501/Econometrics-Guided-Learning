{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 03 Multi-Factor Regression (Macro)\n",
        "\n",
        "Multi-factor GDP growth regression; weights and VIF.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Table of Contents\n",
        "- [Choose features](#choose-features)\n",
        "- [Fit model](#fit-model)\n",
        "- [VIF + stability](#vif-stability)\n",
        "- [Checkpoint (Self-Check)](#checkpoint-self-check)\n",
        "- [Solutions (Reference)](#solutions-reference)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Why This Notebook Matters\n",
        "Regression is the bridge between statistics and ML. You will learn:\n",
        "- single-factor vs multi-factor interpretation,\n",
        "- robust standard errors,\n",
        "- coefficient stability and multicollinearity.\n",
        "\n",
        "\n",
        "## Prerequisites (Quick Self-Check)\n",
        "- Completed Parts 00\u201301 (foundations + data).\n",
        "- Basic algebra comfort (reading coefficient tables, units).\n",
        "\n",
        "## What You Will Produce\n",
        "- (no file output; learning/analysis notebook)\n",
        "\n",
        "## Success Criteria\n",
        "- You can explain what you built and why each step exists.\n",
        "- You can run your work end-to-end without undefined variables.\n",
        "\n",
        "## Common Pitfalls\n",
        "- Running cells top-to-bottom without reading the instructions.\n",
        "- Leaving `...` placeholders in code cells.\n",
        "- Treating coefficients as causal without a causal design.\n",
        "- Ignoring multicollinearity (unstable coefficients).\n",
        "\n",
        "## Quick Fixes (When You Get Stuck)\n",
        "- If you see `ModuleNotFoundError`, re-run the bootstrap cell and restart the kernel; make sure `PROJECT_ROOT` is the repo root.\n",
        "- If a `data/processed/*` file is missing, either run the matching build script (see guide) or use the notebook\u2019s `data/sample/*` fallback.\n",
        "- If results look \u201ctoo good,\u201d suspect leakage; re-check shifts, rolling windows, and time splits.\n",
        "- If a model errors, check dtypes (`astype(float)`) and missingness (`dropna()` on required columns).\n",
        "\n",
        "## Matching Guide\n",
        "- `docs/guides/02_regression/03_multifactor_regression_macro.md`\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## How To Use This Notebook\n",
        "- Work section-by-section; don\u2019t skip the markdown.\n",
        "- Most code cells are incomplete on purpose: replace TODOs and `...`, then run.\n",
        "- After each section, write 2\u20134 sentences answering the interpretation prompts (what changed, why it matters).\n",
        "- Prefer `data/processed/*` if you have built the real datasets; otherwise use the bundled `data/sample/*` fallbacks.\n",
        "- Use the **Checkpoint (Self-Check)** section to catch mistakes early.\n",
        "- Use **Solutions (Reference)** only to unblock yourself; then re-implement without looking.\n",
        "- Use the matching guide (`docs/guides/02_regression/03_multifactor_regression_macro.md`) for the math, assumptions, and deeper context.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"environment-bootstrap\"></a>\n",
        "## Environment Bootstrap\n",
        "Run this cell first. It makes the repo importable and defines common directories.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "\n",
        "def find_repo_root(start: Path) -> Path:\n",
        "    p = start\n",
        "    for _ in range(8):\n",
        "        if (p / 'src').exists() and (p / 'docs').exists():\n",
        "            return p\n",
        "        p = p.parent\n",
        "    raise RuntimeError('Could not find repo root. Start Jupyter from the repo root.')\n",
        "\n",
        "\n",
        "PROJECT_ROOT = find_repo_root(Path.cwd())\n",
        "if str(PROJECT_ROOT) not in sys.path:\n",
        "    sys.path.append(str(PROJECT_ROOT))\n",
        "\n",
        "DATA_DIR = PROJECT_ROOT / 'data'\n",
        "RAW_DIR = DATA_DIR / 'raw'\n",
        "PROCESSED_DIR = DATA_DIR / 'processed'\n",
        "SAMPLE_DIR = DATA_DIR / 'sample'\n",
        "\n",
        "PROJECT_ROOT\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Goal\n",
        "Fit a multi-factor GDP growth regression and learn how to interpret feature weights *carefully*.\n",
        "\n",
        "This notebook is where multicollinearity becomes real:\n",
        "- many macro indicators move together\n",
        "- coefficients can change sign or become unstable when features are correlated\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Primer: pandas time series essentials (indexing, resampling, lags)\n",
        "\n",
        "Most \u201cmysterious bugs\u201d in time series work come from index and alignment mistakes. This primer gives you the minimum patterns to avoid them.\n",
        "\n",
        "### 1) DatetimeIndex (the first thing to verify)\n",
        "\n",
        "Most time-series operations assume a `DatetimeIndex`:\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "df = df.copy()\n",
        "df.index = pd.to_datetime(df.index)\n",
        "df = df.sort_index()\n",
        "assert isinstance(df.index, pd.DatetimeIndex)\n",
        "```\n",
        "\n",
        "**Expected output / sanity checks**\n",
        "- `df.index.min(), df.index.max()` look reasonable\n",
        "- `df.index.is_monotonic_increasing` is `True`\n",
        "\n",
        "### 2) Resampling (frequency alignment)\n",
        "\n",
        "Resampling converts one frequency to another. Choose the aggregation rule intentionally.\n",
        "\n",
        "```python\n",
        "# month-end last value (end-of-period)\n",
        "df_me_last = df.resample(\"ME\").last()\n",
        "\n",
        "# month-end mean (average-of-period)\n",
        "df_me_mean = df.resample(\"ME\").mean()\n",
        "\n",
        "# quarter-end mean\n",
        "df_q_mean = df.resample(\"QE\").mean()\n",
        "```\n",
        "\n",
        "**Interpretation matters**\n",
        "- `.last()` treats end-of-period value as \u201cthe period\u2019s value.\u201d\n",
        "- `.mean()` treats the period average as \u201cthe period\u2019s value.\u201d\n",
        "\n",
        "### 3) Alignment and merging\n",
        "\n",
        "When joining series, always check missingness after the join:\n",
        "\n",
        "```python\n",
        "merged = df1.join(df2, how=\"outer\").sort_index()\n",
        "print(merged.isna().sum().sort_values(ascending=False).head(10))\n",
        "```\n",
        "\n",
        "### 4) Lags and rolling windows (watch for leakage!)\n",
        "\n",
        "```python\n",
        "# lag 1 period (past-only)\n",
        "df[\"x_lag1\"] = df[\"x\"].shift(1)\n",
        "\n",
        "# rolling mean using past values ending at t\n",
        "df[\"x_roll12\"] = df[\"x\"].rolling(12).mean()\n",
        "```\n",
        "\n",
        "**Leakage pitfalls**\n",
        "- `shift(-1)` uses the future.\n",
        "- `rolling(..., center=True)` uses the future.\n",
        "\n",
        "### 5) A quick workflow you should repeat\n",
        "\n",
        "1) Set and verify DatetimeIndex.\n",
        "2) Resample intentionally (mean vs last).\n",
        "3) Join and inspect missingness.\n",
        "4) Add lags/rolls (past-only).\n",
        "5) `dropna()` to build a clean modeling table.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Primer: `statsmodels` vs `scikit-learn` (inference vs prediction)\n",
        "\n",
        "This repo uses both libraries because they serve different goals:\n",
        "\n",
        "- **Prediction (ML):** optimize out-of-sample accuracy \u2192 `scikit-learn`\n",
        "- **Inference (econometrics):** interpret coefficients + quantify uncertainty \u2192 `statsmodels`\n",
        "\n",
        "### Minimal `statsmodels` OLS pattern\n",
        "\n",
        "```python\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# X: DataFrame of features, y: Series target\n",
        "Xc = sm.add_constant(X, has_constant=\"add\")  # add intercept\n",
        "res = sm.OLS(y, Xc).fit()\n",
        "print(res.summary())\n",
        "```\n",
        "\n",
        "**Expected output / sanity check**\n",
        "- a table with `coef`, `std err`, `t`, `P>|t|`, and a CI column\n",
        "- coefficient names match your column names\n",
        "\n",
        "### What you are looking at in `res.summary()`\n",
        "\n",
        "- **coef**: $\\\\hat\\\\beta$ (estimated effect in the model)\n",
        "- **std err**: estimated uncertainty $\\\\widehat{SE}(\\\\hat\\\\beta)$\n",
        "- **t**: $\\\\hat\\\\beta / \\\\widehat{SE}(\\\\hat\\\\beta)$\n",
        "- **P>|t|**: p-value for $H_0: \\\\beta=0$ (conditional on assumptions)\n",
        "- **[0.025, 0.975]**: 95% confidence interval\n",
        "\n",
        "### Robust standard errors (change uncertainty, not coefficients)\n",
        "\n",
        "```python\n",
        "# Cross-section heteroskedasticity\n",
        "res_hc3 = res.get_robustcov_results(cov_type=\"HC3\")\n",
        "\n",
        "# Time series autocorrelation + heteroskedasticity\n",
        "res_hac = res.get_robustcov_results(cov_type=\"HAC\", cov_kwds={\"maxlags\": 4})\n",
        "```\n",
        "\n",
        "### Common pitfalls (and quick fixes)\n",
        "\n",
        "- **Forgetting the intercept**\n",
        "  - Fix: always `add_constant`.\n",
        "- **Wrong SE for time series**\n",
        "  - Fix: use HAC when residuals are autocorrelated.\n",
        "- **Treating p-values as causal proof**\n",
        "  - Fix: write the identification assumption; otherwise interpret as association.\n",
        "- **Mixing prediction and inference**\n",
        "  - Fix: use `sklearn` pipelines + time splits for prediction; use `statsmodels` for coefficient uncertainty.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Primer: Hypothesis testing (p-values, t-stats, confidence intervals)\n",
        "\n",
        "You will see p-values, t-statistics, and confidence intervals in regression output (especially `statsmodels`). This primer gives you the minimum to interpret them correctly.\n",
        "\n",
        "### The objects (plain language)\n",
        "\n",
        "- **Null hypothesis** $H_0$: the default claim (often \u201cno effect\u201d).\n",
        "- **Alternative** $H_1$: the claim you consider if the data looks inconsistent with $H_0$.\n",
        "- **Test statistic**: \u201chow far\u201d your estimate is from the null, in uncertainty units.\n",
        "- **p-value**: probability (under the null *and model assumptions*) of seeing a test statistic at least as extreme as observed.\n",
        "- **Confidence interval (CI)**: a range of parameter values consistent with the data under assumptions.\n",
        "\n",
        "### What a p-value is NOT\n",
        "\n",
        "- Not the probability $H_0$ is true.\n",
        "- Not the probability the model is correct.\n",
        "- Not a measure of economic importance.\n",
        "\n",
        "### Regression t-test intuition\n",
        "\n",
        "In OLS, a common test is $H_0: \\\\beta_j = 0$.\n",
        "\n",
        "$$\n",
        "t_j = \\\\frac{\\\\hat\\\\beta_j}{\\\\widehat{SE}(\\\\hat\\\\beta_j)}\n",
        "$$\n",
        "\n",
        "If you change your SE estimator (HC3/HAC/cluster), you change $\\\\widehat{SE}$ and therefore the p-value, even if the coefficient stays the same.\n",
        "\n",
        "### Expected output / what you should look at in `res.summary()`\n",
        "\n",
        "- `coef`: effect size (in model units)\n",
        "- `std err`: uncertainty\n",
        "- CI columns: magnitude + uncertainty together\n",
        "\n",
        "### Common pitfalls in this project\n",
        "\n",
        "- Macro time series often have autocorrelation \u2192 naive SE too small \u2192 use HAC when interpreting p-values.\n",
        "- Multiple testing/spec-search can produce small p-values by chance.\n",
        "- Predictive success \u2260 causal interpretation.\n",
        "\n",
        "### Tiny demo (toy; not project data)\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "\n",
        "rng = np.random.default_rng(0)\n",
        "n = 300\n",
        "x = rng.normal(size=n)\n",
        "y = 1.0 + 0.5 * x + rng.normal(scale=1.0, size=n)\n",
        "\n",
        "df = pd.DataFrame({\"y\": y, \"x\": x})\n",
        "X = sm.add_constant(df[[\"x\"]])\n",
        "res = sm.OLS(df[\"y\"], X).fit()\n",
        "print(res.summary())\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"choose-features\"></a>\n",
        "## Choose features\n",
        "\n",
        "### Goal\n",
        "Pick a feature set to predict GDP growth.\n",
        "\n",
        "Recommendations:\n",
        "- Start small (3-6 predictors) before you go wide.\n",
        "- Prefer lagged predictors (information available before the quarter).\n",
        "- Keep a record of your feature list.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Your Turn (1): Load macro data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "path = PROCESSED_DIR / 'macro_quarterly.csv'\n",
        "if path.exists():\n",
        "    df = pd.read_csv(path, index_col=0, parse_dates=True)\n",
        "else:\n",
        "    df = pd.read_csv(SAMPLE_DIR / 'macro_quarterly_sample.csv', index_col=0, parse_dates=True)\n",
        "\n",
        "df.head()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Your Turn (2): Choose a target + feature list\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Target\n",
        "y_col = 'gdp_growth_qoq'\n",
        "\n",
        "# TODO: Choose features.\n",
        "# Start with lagged predictors to reduce timing ambiguity.\n",
        "x_cols = [\n",
        "    'T10Y2Y_lag1',\n",
        "    'UNRATE_lag1',\n",
        "    'FEDFUNDS_lag1',\n",
        "    # TODO: add 1-3 more\n",
        "]\n",
        "\n",
        "df_m = df[[y_col] + x_cols].dropna().copy()\n",
        "df_m.tail()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Checkpoint (feature table)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "assert df_m.index.is_monotonic_increasing\n",
        "assert not df_m.isna().any().any()\n",
        "assert df_m.shape[0] > 30\n",
        "...\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"fit-model\"></a>\n",
        "## Fit model\n",
        "\n",
        "### Goal\n",
        "Fit a multi-factor regression and compare:\n",
        "- raw coefficients (units matter)\n",
        "- standardized coefficients (compare relative importance)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Your Turn (1): Time split and fit OLS\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import statsmodels.api as sm\n",
        "from src.evaluation import time_train_test_split_index, regression_metrics\n",
        "\n",
        "split = time_train_test_split_index(len(df_m), test_size=0.2)\n",
        "train = df_m.iloc[split.train_slice]\n",
        "test = df_m.iloc[split.test_slice]\n",
        "\n",
        "X_tr = sm.add_constant(train[x_cols], has_constant='add')\n",
        "y_tr = train[y_col]\n",
        "X_te = sm.add_constant(test[x_cols], has_constant='add')\n",
        "y_te = test[y_col]\n",
        "\n",
        "res = sm.OLS(y_tr, X_tr).fit()\n",
        "y_hat = res.predict(X_te)\n",
        "\n",
        "regression_metrics(y_te.to_numpy(), y_hat.to_numpy())\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Your Turn (2): Standardize predictors and compare standardized coefficients\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Standardize X (train-fitted scaler!)\n",
        "sc = StandardScaler().fit(train[x_cols])\n",
        "X_tr_s = sc.transform(train[x_cols])\n",
        "X_te_s = sc.transform(test[x_cols])\n",
        "\n",
        "# Refit on standardized features\n",
        "X_tr_s = sm.add_constant(X_tr_s, has_constant='add')\n",
        "X_te_s = sm.add_constant(X_te_s, has_constant='add')\n",
        "res_s = sm.OLS(y_tr, X_tr_s).fit()\n",
        "\n",
        "# TODO: Map standardized coefficients back to feature names (excluding intercept)\n",
        "...\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"vif-stability\"></a>\n",
        "## VIF + stability\n",
        "\n",
        "### Goal\n",
        "Measure multicollinearity and see whether coefficients are stable.\n",
        "\n",
        "Two simple stability checks:\n",
        "- VIF (collinearity)\n",
        "- fit on different eras and compare coefficients\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Your Turn (1): VIF table\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from src.econometrics import vif_table\n",
        "\n",
        "# TODO: Compute VIF on the full feature matrix (no intercept).\n",
        "vif = vif_table(df_m, x_cols)\n",
        "vif\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Your Turn (2): Era split coefficient stability\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# TODO: Fit the same model on an early era vs a late era and compare coefficients.\n",
        "mid = int(len(df_m) * 0.5)\n",
        "early = df_m.iloc[:mid]\n",
        "late = df_m.iloc[mid:]\n",
        "\n",
        "res_early = sm.OLS(early[y_col], sm.add_constant(early[x_cols], has_constant='add')).fit()\n",
        "res_late = sm.OLS(late[y_col], sm.add_constant(late[x_cols], has_constant='add')).fit()\n",
        "\n",
        "comp = pd.DataFrame({'early': res_early.params, 'late': res_late.params})\n",
        "comp\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"checkpoint-self-check\"></a>\n",
        "## Checkpoint (Self-Check)\n",
        "Run a few asserts and write 2-3 sentences summarizing what you verified.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# TODO: After you build X/y and split by time, validate the split.\n",
        "# Example (adjust variable names):\n",
        "# assert X_train.index.max() < X_test.index.min()\n",
        "# assert y_train.index.equals(X_train.index)\n",
        "# assert y_test.index.equals(X_test.index)\n",
        "# assert not X_train.isna().any().any()\n",
        "# assert not X_test.isna().any().any()\n",
        "...\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extensions (Optional)\n",
        "- Try one additional variant beyond the main path (different features, different split, different model).\n",
        "- Write down what improved, what got worse, and your hypothesis for why.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reflection\n",
        "- What did you assume implicitly (about timing, availability, stationarity, or costs)?\n",
        "- If you had to ship this model, what would you monitor?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"solutions-reference\"></a>\n",
        "## Solutions (Reference)\n",
        "\n",
        "Try the TODOs first. Use these only to unblock yourself or to compare approaches.\n",
        "\n",
        "<details><summary>Solution: Choose features</summary>\n",
        "\n",
        "_One possible approach. Your variable names may differ; align them with the notebook._\n",
        "\n",
        "```python\n",
        "# Reference solution for 03_multifactor_regression_macro \u2014 Choose features\n",
        "import pandas as pd\n",
        "from src import econometrics\n",
        "\n",
        "df = pd.read_csv(SAMPLE_DIR / 'macro_quarterly_sample.csv', index_col=0, parse_dates=True)\n",
        "x_cols = ['T10Y2Y', 'UNRATE', 'FEDFUNDS']\n",
        "res = econometrics.fit_ols_hac(df, y_col='gdp_growth_qoq', x_cols=x_cols, maxlags=2)\n",
        "print(res.summary())\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "<details><summary>Solution: Fit model</summary>\n",
        "\n",
        "_One possible approach. Your variable names may differ; align them with the notebook._\n",
        "\n",
        "```python\n",
        "# Reference solution for 03_multifactor_regression_macro \u2014 Fit model\n",
        "# See above.\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "<details><summary>Solution: VIF + stability</summary>\n",
        "\n",
        "_One possible approach. Your variable names may differ; align them with the notebook._\n",
        "\n",
        "```python\n",
        "# Reference solution for 03_multifactor_regression_macro \u2014 VIF + stability\n",
        "from src.econometrics import vif_table\n",
        "print(vif_table(df.dropna(), ['T10Y2Y', 'UNRATE', 'FEDFUNDS']))\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "python3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}