{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# 07 GLS and Weighted Least Squares\n",
    "\n",
    "When you know the error structure, you can do better than OLS + robust SE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "toc",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [Why GLS/WLS?](#why-gls-wls)\n",
    "- [WLS: Known Weight Structure](#wls-known-weight-structure)\n",
    "- [Feasible GLS (FGLS)](#feasible-gls-fgls)\n",
    "- [GLS for Autocorrelation (GLSAR)](#gls-for-autocorrelation-glsar)\n",
    "- [When to Use What](#when-to-use-what)\n",
    "- [Checkpoint (Self-Check)](#checkpoint-self-check)\n",
    "- [Extensions (Optional)](#extensions-optional)\n",
    "- [Reflection](#reflection)\n",
    "- [Solutions (Reference)](#solutions-reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "why-matters",
   "metadata": {},
   "source": [
    "## Why This Notebook Matters\n",
    "\n",
    "Notebook 04 showed that robust standard errors (HC3, HAC) protect inference when OLS assumptions fail. But robust SE are a **defensive** move: they fix your confidence intervals without improving your estimates. GLS and WLS go further -- if you can model the error structure correctly, you get **more efficient** (lower-variance) coefficient estimates.\n",
    "\n",
    "This notebook teaches you:\n",
    "- When and why WLS/GLS beats OLS + robust SE in efficiency.\n",
    "- How to implement WLS with a known variance proxy.\n",
    "- How to estimate the variance function when it is unknown (Feasible GLS).\n",
    "- How to handle AR(1) errors with GLSAR.\n",
    "- When to stick with OLS + robust SE instead.\n",
    "\n",
    "\n",
    "## Prerequisites (Quick Self-Check)\n",
    "- Completed Parts 00-04 (foundations + data + regression + inference).\n",
    "- Familiarity with heteroskedasticity and robust SE from Notebooks 04 and 04a.\n",
    "- Basic understanding of variance and weighted averages.\n",
    "\n",
    "## What You Will Produce\n",
    "- (no file output; learning/analysis notebook)\n",
    "\n",
    "## Success Criteria\n",
    "- You can explain when WLS/GLS is preferable to OLS + robust SE.\n",
    "- You can implement WLS with a known variance proxy.\n",
    "- You can implement FGLS by estimating the variance function from residuals.\n",
    "- You can run your work end-to-end without undefined variables.\n",
    "\n",
    "## Common Pitfalls\n",
    "- Running cells top-to-bottom without reading the instructions.\n",
    "- Leaving `...` placeholders in code cells.\n",
    "- Confusing WLS weights with frequency weights (they are precision weights: higher weight = lower variance).\n",
    "- Using FGLS when the variance model is badly misspecified (can be worse than OLS).\n",
    "- Forgetting that GLS efficiency gains require a *correct* variance model.\n",
    "\n",
    "## Quick Fixes (When You Get Stuck)\n",
    "- If you see `ModuleNotFoundError`, re-run the bootstrap cell and restart the kernel; make sure `PROJECT_ROOT` is the repo root.\n",
    "- If a `data/processed/*` file is missing, either run the matching build script (see guide) or use the notebook's `data/sample/*` fallback.\n",
    "- If results look \"too good,\" suspect leakage; re-check shifts, rolling windows, and time splits.\n",
    "- If a model errors, check dtypes (`astype(float)`) and missingness (`dropna()` on required columns).\n",
    "\n",
    "## Matching Guide\n",
    "- `docs/guides/02_regression/07_gls_weighted_least_squares.md`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "how-to-use",
   "metadata": {},
   "source": [
    "## How To Use This Notebook\n",
    "- Work section-by-section; don't skip the markdown.\n",
    "- Most code cells are incomplete on purpose: replace TODOs and `...`, then run.\n",
    "- After each section, write 2-4 sentences answering the interpretation prompts (what changed, why it matters).\n",
    "- Prefer `data/processed/*` if you have built the real datasets; otherwise use the bundled `data/sample/*` fallbacks.\n",
    "- Use the **Checkpoint (Self-Check)** section to catch mistakes early.\n",
    "- Use **Solutions (Reference)** only to unblock yourself; then re-implement without looking.\n",
    "- Use the matching guide (`docs/guides/02_regression/07_gls_weighted_least_squares.md`) for the math, assumptions, and deeper context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bootstrap-header",
   "metadata": {},
   "source": [
    "<a id=\"environment-bootstrap\"></a>\n",
    "## Environment Bootstrap\n",
    "Run this cell first. It makes the repo importable and defines common directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bootstrap",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "\n",
    "def find_repo_root(start: Path) -> Path:\n",
    "    p = start\n",
    "    for _ in range(8):\n",
    "        if (p / 'src').exists() and (p / 'docs').exists():\n",
    "            return p\n",
    "        p = p.parent\n",
    "    raise RuntimeError('Could not find repo root. Start Jupyter from the repo root.')\n",
    "\n",
    "\n",
    "PROJECT_ROOT = find_repo_root(Path.cwd())\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "RAW_DIR = DATA_DIR / 'raw'\n",
    "PROCESSED_DIR = DATA_DIR / 'processed'\n",
    "SAMPLE_DIR = DATA_DIR / 'sample'\n",
    "\n",
    "PROJECT_ROOT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "goal",
   "metadata": {},
   "source": [
    "## Goal\n",
    "\n",
    "Learn when and how to move beyond OLS + robust standard errors to **Generalized Least Squares** (GLS) and **Weighted Least Squares** (WLS).\n",
    "\n",
    "The key insight:\n",
    "- OLS treats every observation equally.\n",
    "- If some observations are noisier than others (heteroskedasticity) or errors are correlated across observations (autocorrelation), OLS is **inefficient** -- it does not make the best use of the data.\n",
    "- GLS/WLS down-weights noisy observations and up-weights precise ones, producing **lower-variance** coefficient estimates.\n",
    "\n",
    "The catch: you need a credible model of the error structure. If that model is wrong, GLS can be *worse* than OLS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "why-gls-wls-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"why-gls-wls\"></a>\n",
    "## 1. Why GLS/WLS?\n",
    "\n",
    "### The efficiency argument\n",
    "\n",
    "Consider two strategies when errors are heteroskedastic:\n",
    "\n",
    "| Strategy | Coefficients | Standard Errors | Efficiency |\n",
    "|----------|-------------|-----------------|------------|\n",
    "| **OLS + robust SE (HC3)** | Consistent but not efficient | Valid (corrected for heteroskedasticity) | Lower -- treats all obs equally |\n",
    "| **WLS / GLS** | Consistent *and* efficient (if variance model correct) | Valid *and* tighter | Higher -- down-weights noisy obs |\n",
    "\n",
    "**Analogy:** Imagine averaging exam scores from two classes. Class A has 200 students (low noise per student), Class B has 10 students (high noise per student). A simple average treats both classes equally. A *weighted* average gives more weight to the larger, more precise class. That is what WLS does.\n",
    "\n",
    "### When does the distinction matter?\n",
    "\n",
    "- **Small samples:** efficiency gains from WLS can be substantial.\n",
    "- **Strong heteroskedasticity patterns:** e.g., variance proportional to population size, income level, or time.\n",
    "- **Forecasting / prediction:** tighter coefficient estimates translate to better predictions.\n",
    "\n",
    "### When to stick with OLS + robust SE\n",
    "\n",
    "- When you have no credible model of the variance structure.\n",
    "- When the sample is large enough that efficiency gains are negligible.\n",
    "- When misspecifying the variance model is a bigger risk than losing some efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-data-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Load Data\n",
    "\n",
    "We use **two** datasets in this notebook:\n",
    "\n",
    "1. **Census county data** (cross-sectional) -- for WLS with a known variance proxy (county population).\n",
    "2. **Macro quarterly data** (time series) -- for FGLS and GLSAR with heteroskedastic/autocorrelated residuals.\n",
    "\n",
    "### Your Turn (1): Load cross-sectional (county) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-county",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# --- Cross-sectional data (county level) ---\n",
    "year = 2022  # TODO: set to the year you fetched\n",
    "path_county = PROCESSED_DIR / f'census_county_{year}.csv'\n",
    "\n",
    "if path_county.exists():\n",
    "    df_county = pd.read_csv(path_county)\n",
    "else:\n",
    "    df_county = pd.read_csv(SAMPLE_DIR / 'census_county_sample.csv')\n",
    "\n",
    "# Build variables for cross-sectional regression\n",
    "income = pd.to_numeric(df_county['B19013_001E'], errors='coerce')   # median household income\n",
    "rent = pd.to_numeric(df_county['B25064_001E'], errors='coerce')     # median gross rent\n",
    "pop = pd.to_numeric(df_county['B01003_001E'], errors='coerce')      # total population\n",
    "\n",
    "mask = (income > 0) & (rent > 0) & (pop > 0)\n",
    "\n",
    "df_cs = pd.DataFrame({\n",
    "    'income': income[mask],\n",
    "    'rent': rent[mask],\n",
    "    'pop': pop[mask],\n",
    "}).dropna()\n",
    "\n",
    "df_cs['log_income'] = np.log(df_cs['income'])\n",
    "df_cs['log_rent'] = np.log(df_cs['rent'])\n",
    "df_cs['log_pop'] = np.log(df_cs['pop'])\n",
    "\n",
    "print(f'Cross-sectional obs: {len(df_cs)}')\n",
    "df_cs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-macro-header",
   "metadata": {},
   "source": [
    "### Your Turn (2): Load time-series (macro) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-macro",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Time-series data (macro quarterly) ---\n",
    "path_macro = PROCESSED_DIR / 'macro_quarterly.csv'\n",
    "\n",
    "if path_macro.exists():\n",
    "    df_macro = pd.read_csv(path_macro, index_col=0, parse_dates=True)\n",
    "else:\n",
    "    df_macro = pd.read_csv(SAMPLE_DIR / 'macro_quarterly_sample.csv', index_col=0, parse_dates=True)\n",
    "\n",
    "y_ts_col = 'gdp_growth_qoq'\n",
    "x_ts_cols = ['T10Y2Y_lag1']\n",
    "\n",
    "df_ts = df_macro[[y_ts_col] + x_ts_cols].dropna().copy()\n",
    "\n",
    "print(f'Time-series obs: {len(df_ts)}')\n",
    "df_ts.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wls-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"wls-known-weight-structure\"></a>\n",
    "## 2. WLS: Known Weight Structure\n",
    "\n",
    "### Goal\n",
    "\n",
    "Fit a weighted least squares regression where the weights come from a known variable -- **county population**.\n",
    "\n",
    "### Intuition\n",
    "\n",
    "County-level statistics (median income, median rent) are themselves estimates from surveys. Larger counties have more respondents, so their estimates are more precise (lower sampling variability). We can model this as:\n",
    "\n",
    "$$\\text{Var}(\\varepsilon_i) \\propto \\frac{1}{\\text{pop}_i}$$\n",
    "\n",
    "WLS uses weights $w_i = \\text{pop}_i$ (inverse of variance) to give more weight to counties with lower error variance.\n",
    "\n",
    "### `statsmodels` WLS API\n",
    "\n",
    "```python\n",
    "sm.WLS(y, X, weights=w).fit()\n",
    "```\n",
    "\n",
    "where `weights` = $1 / \\text{Var}(\\varepsilon_i)$. Higher weight means the observation is treated as **more precise**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wls-baseline-header",
   "metadata": {},
   "source": [
    "### Your Turn (1): Fit OLS baseline with HC3 robust SE\n",
    "\n",
    "First, establish the OLS benchmark with heteroskedasticity-robust standard errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wls-baseline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- OLS baseline: log(rent) ~ log(income) ---\n",
    "X_cs = sm.add_constant(df_cs[['log_income']], has_constant='add')\n",
    "y_cs = df_cs['log_rent']\n",
    "\n",
    "res_ols = sm.OLS(y_cs, X_cs).fit()\n",
    "res_ols_hc3 = res_ols.get_robustcov_results(cov_type='HC3')\n",
    "\n",
    "print('=== OLS (naive SE) ===')\n",
    "print(res_ols.summary())\n",
    "print()\n",
    "print('=== OLS (HC3 robust SE) ===')\n",
    "print(f'  log_income coef : {res_ols_hc3.params[\"log_income\"]:.4f}')\n",
    "print(f'  log_income SE   : {res_ols_hc3.bse[\"log_income\"]:.4f}')\n",
    "print(f'  log_income p    : {res_ols_hc3.pvalues[\"log_income\"]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wls-visual-header",
   "metadata": {},
   "source": [
    "### Your Turn (2): Visualize heteroskedasticity by population\n",
    "\n",
    "Before fitting WLS, confirm that residual variance is related to population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wls-visual",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a scatter plot of OLS residuals vs log(population).\n",
    "# Look for a fan shape: do smaller counties have larger residual spread?\n",
    "# Hint: res_ols.resid on y-axis, df_cs['log_pop'] on x-axis.\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "...\n",
    "\n",
    "ax.axhline(0, color='red', linestyle='--', linewidth=1)\n",
    "ax.set_xlabel('log(Population)')\n",
    "ax.set_ylabel('OLS Residual')\n",
    "ax.set_title('OLS Residuals vs County Population (heteroskedasticity check)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wls-fit-header",
   "metadata": {},
   "source": [
    "### Your Turn (3): Fit WLS with population weights\n",
    "\n",
    "Use `sm.WLS` with `weights = pop` (larger population = more precise = higher weight)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wls-fit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fit WLS using population as the weight.\n",
    "# weights = pop means Var(e_i) ~ 1/pop_i.\n",
    "\n",
    "w = df_cs['pop']\n",
    "res_wls = sm.WLS(..., ..., weights=...).fit()\n",
    "\n",
    "print('=== WLS (population weights) ===')\n",
    "print(res_wls.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wls-compare-header",
   "metadata": {},
   "source": [
    "### Your Turn (4): Compare OLS + HC3 vs WLS side by side\n",
    "\n",
    "The key comparison: do coefficients change? Do standard errors shrink?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wls-compare",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build a comparison table of coefficients and SE for OLS (naive),\n",
    "# OLS (HC3), and WLS.\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'OLS_coef': res_ols.params,\n",
    "    'OLS_SE_naive': res_ols.bse,\n",
    "    'OLS_SE_HC3': res_ols_hc3.bse,\n",
    "    'WLS_coef': ...,\n",
    "    'WLS_SE': ...,\n",
    "})\n",
    "\n",
    "print('Coefficient and SE Comparison: OLS vs WLS')\n",
    "print(comparison.round(4))\n",
    "print()\n",
    "print('Efficiency gain (SE ratio, WLS / OLS_HC3):')\n",
    "print((comparison['WLS_SE'] / comparison['OLS_SE_HC3']).round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wls-interpret",
   "metadata": {},
   "source": [
    "### Interpretation Prompt\n",
    "\n",
    "Write 2-4 sentences:\n",
    "- Did the coefficient on `log_income` change between OLS and WLS? By how much?\n",
    "- Did the WLS standard error shrink compared to OLS HC3? What does that mean for your confidence intervals?\n",
    "- Why might WLS coefficients differ from OLS? (Hint: think about which observations get up-weighted.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wls-interpret-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write your interpretation.\n",
    "notes = \"\"\"\n",
    "...\n",
    "\"\"\"\n",
    "print(notes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fgls-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"feasible-gls-fgls\"></a>\n",
    "## 3. Feasible GLS (FGLS)\n",
    "\n",
    "### Goal\n",
    "\n",
    "When you do not have a natural variance proxy (like population), you can **estimate** the variance structure from OLS residuals. This is called **Feasible GLS**.\n",
    "\n",
    "### The three-step recipe\n",
    "\n",
    "1. **Step 1:** Fit OLS and get residuals $\\hat{e}_i$.\n",
    "2. **Step 2:** Model $\\log(\\hat{e}_i^2)$ as a function of $X$ (regress the log-squared residuals on the regressors).\n",
    "3. **Step 3:** Use the predicted values $\\hat{\\sigma}_i^2 = \\exp(\\hat{g}_i)$ as estimated variances. Set WLS weights $w_i = 1 / \\hat{\\sigma}_i^2$.\n",
    "\n",
    "### Why log-squared residuals?\n",
    "\n",
    "- Squared residuals $\\hat{e}_i^2$ are noisy and right-skewed.\n",
    "- Taking logs stabilizes the variance and makes the auxiliary regression better-behaved.\n",
    "- Exponentiating the fitted values guarantees positive variance estimates.\n",
    "\n",
    "### Caveat\n",
    "\n",
    "FGLS is only as good as the auxiliary variance model. If the variance model is badly wrong, FGLS can be **less** efficient than OLS. When in doubt, compare FGLS to OLS + robust SE.\n",
    "\n",
    "We will demonstrate FGLS on the **macro quarterly** data, where GDP growth residuals may have time-varying volatility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fgls-step1-header",
   "metadata": {},
   "source": [
    "### Your Turn (1): Step 1 -- Fit OLS and inspect residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fgls-step1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: OLS on macro data ---\n",
    "X_ts = sm.add_constant(df_ts[x_ts_cols], has_constant='add')\n",
    "y_ts = df_ts[y_ts_col]\n",
    "\n",
    "res_ols_ts = sm.OLS(y_ts, X_ts).fit()\n",
    "\n",
    "# Inspect residuals visually: do they show changing variance over time?\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(res_ols_ts.resid.index, res_ols_ts.resid.values, linewidth=0.8)\n",
    "axes[0].axhline(0, color='red', linestyle='--', linewidth=1)\n",
    "axes[0].set_title('OLS Residuals Over Time')\n",
    "axes[0].set_xlabel('Date')\n",
    "axes[0].set_ylabel('Residual')\n",
    "\n",
    "axes[1].scatter(res_ols_ts.fittedvalues, res_ols_ts.resid, alpha=0.5, s=15)\n",
    "axes[1].axhline(0, color='red', linestyle='--', linewidth=1)\n",
    "axes[1].set_title('Fitted vs Residuals')\n",
    "axes[1].set_xlabel('Fitted values')\n",
    "axes[1].set_ylabel('Residuals')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(res_ols_ts.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fgls-step2-header",
   "metadata": {},
   "source": [
    "### Your Turn (2): Step 2 -- Model the variance function\n",
    "\n",
    "Regress $\\log(\\hat{e}^2)$ on the regressors to estimate how variance changes with $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fgls-step2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 2: Auxiliary regression for variance ---\n",
    "# Compute log(squared residuals)\n",
    "log_resid_sq = np.log(res_ols_ts.resid ** 2)\n",
    "\n",
    "# TODO: Regress log_resid_sq on X_ts (same regressors as the main model).\n",
    "# This is the auxiliary variance model.\n",
    "res_aux = sm.OLS(..., ...).fit()\n",
    "\n",
    "print('=== Auxiliary Variance Model: log(e^2) ~ X ===')\n",
    "print(res_aux.summary())\n",
    "print()\n",
    "print('If the slope is significant, variance depends on X (heteroskedasticity pattern).')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fgls-step3-header",
   "metadata": {},
   "source": [
    "### Your Turn (3): Step 3 -- Compute FGLS weights and re-estimate\n",
    "\n",
    "Use the predicted variances from Step 2 to construct weights, then fit WLS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fgls-step3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 3: FGLS (WLS with estimated weights) ---\n",
    "\n",
    "# Predicted log-variance from the auxiliary model\n",
    "log_var_hat = res_aux.fittedvalues\n",
    "\n",
    "# TODO: Convert to variance estimates and compute weights.\n",
    "# var_hat = exp(log_var_hat)\n",
    "# weights = 1 / var_hat\n",
    "var_hat = ...\n",
    "fgls_weights = ...\n",
    "\n",
    "# TODO: Fit WLS with the estimated weights.\n",
    "res_fgls = sm.WLS(y_ts, X_ts, weights=...).fit()\n",
    "\n",
    "print('=== FGLS Results ===')\n",
    "print(res_fgls.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fgls-compare-header",
   "metadata": {},
   "source": [
    "### Your Turn (4): Compare OLS, OLS + HAC, and FGLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fgls-compare",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build a comparison table.\n",
    "res_ols_hac = res_ols_ts.get_robustcov_results(cov_type='HAC', cov_kwds={'maxlags': 4})\n",
    "\n",
    "comparison_ts = pd.DataFrame({\n",
    "    'OLS_coef': res_ols_ts.params,\n",
    "    'OLS_SE_naive': res_ols_ts.bse,\n",
    "    'OLS_SE_HAC': res_ols_hac.bse,\n",
    "    'FGLS_coef': ...,\n",
    "    'FGLS_SE': ...,\n",
    "})\n",
    "\n",
    "print('Coefficient and SE Comparison: OLS vs FGLS (Macro Time Series)')\n",
    "print(comparison_ts.round(4))\n",
    "print()\n",
    "print('Efficiency gain (SE ratio, FGLS / OLS_HAC):')\n",
    "print((comparison_ts['FGLS_SE'] / comparison_ts['OLS_SE_HAC']).round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fgls-interpret",
   "metadata": {},
   "source": [
    "### Interpretation Prompt\n",
    "\n",
    "Write 2-4 sentences:\n",
    "- Did the auxiliary regression find a significant variance pattern? What does that tell you?\n",
    "- Did FGLS produce tighter standard errors than OLS + HAC?\n",
    "- Under what circumstances would FGLS be *worse* than OLS + robust SE?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fgls-interpret-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write your interpretation.\n",
    "notes = \"\"\"\n",
    "...\n",
    "\"\"\"\n",
    "print(notes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "glsar-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"gls-for-autocorrelation-glsar\"></a>\n",
    "## 4. GLS for Autocorrelation (GLSAR)\n",
    "\n",
    "### Goal\n",
    "\n",
    "When time-series residuals follow an AR(1) process, GLS can account for the serial correlation directly rather than just fixing the standard errors.\n",
    "\n",
    "### Background\n",
    "\n",
    "If the errors follow $\\varepsilon_t = \\rho \\varepsilon_{t-1} + u_t$, then:\n",
    "- **OLS + HAC:** Coefficients are consistent, SE are corrected. Simple and safe.\n",
    "- **Cochrane-Orcutt / Prais-Winsten:** Transform the data by quasi-differencing ($y_t - \\rho y_{t-1}$), then run OLS on the transformed data. More efficient if $\\rho$ is correctly estimated.\n",
    "- **`sm.GLSAR`:** Iterates between estimating $\\rho$ from residuals and re-estimating the regression. Converges to the GLS solution.\n",
    "\n",
    "### `statsmodels` GLSAR API\n",
    "\n",
    "```python\n",
    "from statsmodels.regression.linear_model import GLSAR\n",
    "\n",
    "model = GLSAR(y, X, rho=1)     # rho=1 means estimate AR(1)\n",
    "res = model.iterative_fit(maxiter=50)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "glsar-resid-header",
   "metadata": {},
   "source": [
    "### Your Turn (1): Check for AR(1) residuals in the OLS model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "glsar-resid",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute lag-1 autocorrelation of OLS residuals.\n",
    "# Hint: res_ols_ts.resid.autocorr(lag=1)\n",
    "\n",
    "rho_hat = ...\n",
    "\n",
    "print(f'Estimated AR(1) coefficient from OLS residuals: {rho_hat:.4f}')\n",
    "print()\n",
    "if abs(rho_hat) > 0.2:\n",
    "    print('Substantial autocorrelation detected. GLSAR may improve efficiency.')\n",
    "else:\n",
    "    print('Weak autocorrelation. OLS + HAC is likely sufficient.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "glsar-fit-header",
   "metadata": {},
   "source": [
    "### Your Turn (2): Fit GLSAR and compare to OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "glsar-fit",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.regression.linear_model import GLSAR\n",
    "\n",
    "# TODO: Fit GLSAR with iterative estimation of rho.\n",
    "# rho=1 tells GLSAR to estimate a single AR(1) parameter.\n",
    "model_glsar = GLSAR(y_ts, X_ts, rho=1)\n",
    "res_glsar = model_glsar.iterative_fit(maxiter=...)\n",
    "\n",
    "print('=== GLSAR Results (AR(1) correction) ===')\n",
    "print(f'Estimated rho: {model_glsar.rho:.4f}')\n",
    "print(res_glsar.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "glsar-compare-header",
   "metadata": {},
   "source": [
    "### Your Turn (3): Compare OLS, OLS + HAC, and GLSAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "glsar-compare",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build a comparison table for all three approaches.\n",
    "\n",
    "comparison_ar = pd.DataFrame({\n",
    "    'OLS_coef': res_ols_ts.params,\n",
    "    'OLS_SE_naive': res_ols_ts.bse,\n",
    "    'OLS_SE_HAC': res_ols_hac.bse,\n",
    "    'GLSAR_coef': ...,\n",
    "    'GLSAR_SE': ...,\n",
    "})\n",
    "\n",
    "print('Coefficient and SE Comparison: OLS vs GLSAR')\n",
    "print(comparison_ar.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "glsar-interpret",
   "metadata": {},
   "source": [
    "### Interpretation Prompt\n",
    "\n",
    "Write 2-4 sentences:\n",
    "- How does the GLSAR-estimated $\\rho$ compare to the residual autocorrelation you computed?\n",
    "- Did GLSAR change the coefficient or mostly the standard errors?\n",
    "- When would you prefer GLSAR over simply using HAC standard errors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "glsar-interpret-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write your interpretation.\n",
    "notes = \"\"\"\n",
    "...\n",
    "\"\"\"\n",
    "print(notes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decision-tree-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"when-to-use-what\"></a>\n",
    "## 5. When to Use What\n",
    "\n",
    "### Decision Tree\n",
    "\n",
    "```\n",
    "Start: You have a regression model with potential error structure issues.\n",
    "  |\n",
    "  +-- Do you have a credible variance model (e.g., variance ~ population)?\n",
    "  |     |\n",
    "  |     +-- YES --> Use WLS with known weights.\n",
    "  |     |           (Most efficient if the variance model is correct.)\n",
    "  |     |\n",
    "  |     +-- NO --> Do you observe a clear heteroskedasticity pattern in residuals?\n",
    "  |                 |\n",
    "  |                 +-- YES --> Consider FGLS (estimate variance from residuals).\n",
    "  |                 |           Compare FGLS SE to OLS + robust SE as a sanity check.\n",
    "  |                 |\n",
    "  |                 +-- NO --> Are the residuals autocorrelated (time series)?\n",
    "  |                             |\n",
    "  |                             +-- YES --> Use GLSAR (AR(1) correction)\n",
    "  |                             |           or OLS + HAC (safer, fewer assumptions).\n",
    "  |                             |\n",
    "  |                             +-- NO --> OLS + robust SE (HC3 for cross-section,\n",
    "  |                                        HAC for time series).\n",
    "  |                                        This is the safe default.\n",
    "```\n",
    "\n",
    "### Summary Table\n",
    "\n",
    "| Method | Assumptions | Coefficients | SE | Efficiency | Risk |\n",
    "|--------|-------------|-------------|----|-----------|----- |\n",
    "| OLS + robust SE | Minimal | Consistent | Valid | Baseline | Low |\n",
    "| WLS (known weights) | Variance model correct | Consistent + efficient | Valid + tight | High | Medium (if weights wrong) |\n",
    "| FGLS (estimated weights) | Variance model approximately correct | Consistent + efficient | Valid + tight | Medium-High | Higher (estimated weights add noise) |\n",
    "| GLSAR (AR correction) | AR(1) error structure | Consistent + efficient | Valid + tight | High (if AR(1) holds) | Medium (if AR structure wrong) |\n",
    "\n",
    "### Rule of thumb\n",
    "\n",
    "**Default to OLS + robust SE.** Only move to GLS/WLS when:\n",
    "1. You have a strong theoretical or empirical reason for the variance model.\n",
    "2. You verify that the GLS/WLS standard errors are actually tighter (not always guaranteed with estimated weights).\n",
    "3. You compare the GLS coefficient to the OLS coefficient -- if they diverge substantially, investigate why."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "checkpoint-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"checkpoint-self-check\"></a>\n",
    "## Checkpoint (Self-Check)\n",
    "\n",
    "Run a few asserts and write 2-3 sentences summarizing what you verified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "checkpoint",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Validate your results. Uncomment and adjust as needed.\n",
    "\n",
    "# Cross-sectional data loaded\n",
    "# assert df_cs.shape[0] > 50, f'Too few county observations: {df_cs.shape[0]}'\n",
    "\n",
    "# Time-series data loaded\n",
    "# assert df_ts.shape[0] > 30, f'Too few time-series observations: {df_ts.shape[0]}'\n",
    "\n",
    "# WLS produced valid results\n",
    "# assert np.isfinite(res_wls.params['log_income']), 'WLS coefficient is not finite'\n",
    "# assert res_wls.bse['log_income'] > 0, 'WLS SE should be positive'\n",
    "\n",
    "# OLS and WLS coefficients should be in the same ballpark\n",
    "# assert abs(res_ols.params['log_income'] - res_wls.params['log_income']) < 1.0, \\\n",
    "#     'OLS and WLS coefficients differ by more than 1.0 -- investigate'\n",
    "\n",
    "# FGLS weights should all be positive\n",
    "# assert (fgls_weights > 0).all(), 'FGLS weights must be positive'\n",
    "\n",
    "# GLSAR rho should be between -1 and 1\n",
    "# assert -1 < model_glsar.rho < 1, f'GLSAR rho out of range: {model_glsar.rho}'\n",
    "\n",
    "print('All checkpoint assertions passed.')\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extensions-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"extensions-optional\"></a>\n",
    "## Extensions (Optional)\n",
    "\n",
    "- Try WLS with `weights = np.sqrt(pop)` instead of `pop`. How sensitive are the results to the weight specification?\n",
    "- Add `log_pop` as a regressor to the cross-sectional model. Does the heteroskedasticity pattern change after controlling for population?\n",
    "- For FGLS, try adding squared terms or additional predictors to the auxiliary variance regression. Does the fit improve?\n",
    "- Fit GLSAR with `rho=2` (AR(2) errors) and compare to the AR(1) version.\n",
    "- Implement Cochrane-Orcutt manually: estimate $\\rho$ from OLS residuals, quasi-difference the data ($y_t - \\hat{\\rho} y_{t-1}$), and run OLS on the transformed data. Compare to `GLSAR`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reflection-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"reflection\"></a>\n",
    "## Reflection\n",
    "\n",
    "- What did you assume about the variance structure when fitting WLS and FGLS? How would you test those assumptions?\n",
    "- If you had to ship a regression model, would you use WLS/GLS or stick with OLS + robust SE? What factors would drive that decision?\n",
    "- How does the efficiency vs. robustness tradeoff in GLS/WLS parallel similar tradeoffs in other areas of statistics and ML (e.g., bias-variance tradeoff)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solutions-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"solutions-reference\"></a>\n",
    "## Solutions (Reference)\n",
    "\n",
    "Try the TODOs first. Use these only to unblock yourself or to compare approaches.\n",
    "\n",
    "<details><summary>Solution: WLS -- Visualize heteroskedasticity</summary>\n",
    "\n",
    "_One possible approach. Your variable names may differ; align them with the notebook._\n",
    "\n",
    "```python\n",
    "# Reference solution for 07_gls_weighted_least_squares -- Visualize heteroskedasticity\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ax.scatter(df_cs['log_pop'], res_ols.resid, alpha=0.3, s=10)\n",
    "ax.axhline(0, color='red', linestyle='--', linewidth=1)\n",
    "ax.set_xlabel('log(Population)')\n",
    "ax.set_ylabel('OLS Residual')\n",
    "ax.set_title('OLS Residuals vs County Population (heteroskedasticity check)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<details><summary>Solution: WLS -- Fit WLS with population weights</summary>\n",
    "\n",
    "_One possible approach. Your variable names may differ; align them with the notebook._\n",
    "\n",
    "```python\n",
    "# Reference solution for 07_gls_weighted_least_squares -- Fit WLS\n",
    "w = df_cs['pop']\n",
    "res_wls = sm.WLS(y_cs, X_cs, weights=w).fit()\n",
    "print(res_wls.summary())\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<details><summary>Solution: WLS -- Compare OLS vs WLS</summary>\n",
    "\n",
    "_One possible approach. Your variable names may differ; align them with the notebook._\n",
    "\n",
    "```python\n",
    "# Reference solution for 07_gls_weighted_least_squares -- Compare OLS vs WLS\n",
    "comparison = pd.DataFrame({\n",
    "    'OLS_coef': res_ols.params,\n",
    "    'OLS_SE_naive': res_ols.bse,\n",
    "    'OLS_SE_HC3': res_ols_hc3.bse,\n",
    "    'WLS_coef': res_wls.params,\n",
    "    'WLS_SE': res_wls.bse,\n",
    "})\n",
    "print(comparison.round(4))\n",
    "print()\n",
    "print('Efficiency gain (SE ratio, WLS / OLS_HC3):')\n",
    "print((comparison['WLS_SE'] / comparison['OLS_SE_HC3']).round(4))\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<details><summary>Solution: FGLS -- Auxiliary variance model</summary>\n",
    "\n",
    "_One possible approach. Your variable names may differ; align them with the notebook._\n",
    "\n",
    "```python\n",
    "# Reference solution for 07_gls_weighted_least_squares -- FGLS Step 2\n",
    "log_resid_sq = np.log(res_ols_ts.resid ** 2)\n",
    "res_aux = sm.OLS(log_resid_sq, X_ts).fit()\n",
    "print(res_aux.summary())\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<details><summary>Solution: FGLS -- Compute weights and re-estimate</summary>\n",
    "\n",
    "_One possible approach. Your variable names may differ; align them with the notebook._\n",
    "\n",
    "```python\n",
    "# Reference solution for 07_gls_weighted_least_squares -- FGLS Step 3\n",
    "log_var_hat = res_aux.fittedvalues\n",
    "var_hat = np.exp(log_var_hat)\n",
    "fgls_weights = 1.0 / var_hat\n",
    "\n",
    "res_fgls = sm.WLS(y_ts, X_ts, weights=fgls_weights).fit()\n",
    "print(res_fgls.summary())\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<details><summary>Solution: FGLS -- Compare OLS vs FGLS</summary>\n",
    "\n",
    "_One possible approach. Your variable names may differ; align them with the notebook._\n",
    "\n",
    "```python\n",
    "# Reference solution for 07_gls_weighted_least_squares -- Compare OLS vs FGLS\n",
    "res_ols_hac = res_ols_ts.get_robustcov_results(cov_type='HAC', cov_kwds={'maxlags': 4})\n",
    "\n",
    "comparison_ts = pd.DataFrame({\n",
    "    'OLS_coef': res_ols_ts.params,\n",
    "    'OLS_SE_naive': res_ols_ts.bse,\n",
    "    'OLS_SE_HAC': res_ols_hac.bse,\n",
    "    'FGLS_coef': res_fgls.params,\n",
    "    'FGLS_SE': res_fgls.bse,\n",
    "})\n",
    "print(comparison_ts.round(4))\n",
    "print()\n",
    "print('Efficiency gain (SE ratio, FGLS / OLS_HAC):')\n",
    "print((comparison_ts['FGLS_SE'] / comparison_ts['OLS_SE_HAC']).round(4))\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<details><summary>Solution: GLSAR -- Check autocorrelation and fit</summary>\n",
    "\n",
    "_One possible approach. Your variable names may differ; align them with the notebook._\n",
    "\n",
    "```python\n",
    "# Reference solution for 07_gls_weighted_least_squares -- GLSAR\n",
    "from statsmodels.regression.linear_model import GLSAR\n",
    "\n",
    "rho_hat = res_ols_ts.resid.autocorr(lag=1)\n",
    "print(f'Residual AR(1): {rho_hat:.4f}')\n",
    "\n",
    "model_glsar = GLSAR(y_ts, X_ts, rho=1)\n",
    "res_glsar = model_glsar.iterative_fit(maxiter=50)\n",
    "print(f'Estimated rho: {model_glsar.rho:.4f}')\n",
    "print(res_glsar.summary())\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<details><summary>Solution: GLSAR -- Compare all methods</summary>\n",
    "\n",
    "_One possible approach. Your variable names may differ; align them with the notebook._\n",
    "\n",
    "```python\n",
    "# Reference solution for 07_gls_weighted_least_squares -- Compare all methods\n",
    "comparison_ar = pd.DataFrame({\n",
    "    'OLS_coef': res_ols_ts.params,\n",
    "    'OLS_SE_naive': res_ols_ts.bse,\n",
    "    'OLS_SE_HAC': res_ols_hac.bse,\n",
    "    'GLSAR_coef': res_glsar.params,\n",
    "    'GLSAR_SE': res_glsar.bse,\n",
    "})\n",
    "print(comparison_ar.round(4))\n",
    "```\n",
    "\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}