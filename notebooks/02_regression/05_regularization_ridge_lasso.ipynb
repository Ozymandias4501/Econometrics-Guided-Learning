{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 05 Regularization: Ridge and Lasso\n",
        "\n",
        "Shrinkage, coefficient paths, feature selection.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Table of Contents\n",
        "- [Build feature matrix](#build-feature-matrix)\n",
        "- [Fit ridge/lasso](#fit-ridge-lasso)\n",
        "- [Coefficient paths](#coefficient-paths)\n",
        "- [Checkpoint (Self-Check)](#checkpoint-self-check)\n",
        "- [Solutions (Reference)](#solutions-reference)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Why This Notebook Matters\n",
        "Regression is the bridge between statistics and ML. You will learn:\n",
        "- single-factor vs multi-factor interpretation,\n",
        "- robust standard errors,\n",
        "- coefficient stability and multicollinearity.\n",
        "\n",
        "\n",
        "## What You Will Produce\n",
        "- (no file output; learning/analysis notebook)\n",
        "\n",
        "## Success Criteria\n",
        "- You can explain what you built and why each step exists.\n",
        "- You can run your work end-to-end without undefined variables.\n",
        "\n",
        "## Common Pitfalls\n",
        "- Running cells top-to-bottom without reading the instructions.\n",
        "- Leaving `...` placeholders in code cells.\n",
        "- Treating coefficients as causal without a causal design.\n",
        "- Ignoring multicollinearity (unstable coefficients).\n",
        "\n",
        "## Matching Guide\n",
        "- `docs/guides/02_regression/05_regularization_ridge_lasso.md`\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## How To Use This Notebook\n",
        "- This notebook is hands-on. Most code cells are incomplete on purpose.\n",
        "- Complete each TODO, then run the cell.\n",
        "- Use the matching guide (`docs/guides/02_regression/05_regularization_ridge_lasso.md`) for deep explanations and alternative examples.\n",
        "- Write short interpretation notes as you go (what changed, why it matters).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"environment-bootstrap\"></a>\n",
        "## Environment Bootstrap\n",
        "Run this cell first. It makes the repo importable and defines common directories.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "\n",
        "def find_repo_root(start: Path) -> Path:\n",
        "    p = start\n",
        "    for _ in range(8):\n",
        "        if (p / 'src').exists() and (p / 'docs').exists():\n",
        "            return p\n",
        "        p = p.parent\n",
        "    raise RuntimeError('Could not find repo root. Start Jupyter from the repo root.')\n",
        "\n",
        "\n",
        "PROJECT_ROOT = find_repo_root(Path.cwd())\n",
        "if str(PROJECT_ROOT) not in sys.path:\n",
        "    sys.path.append(str(PROJECT_ROOT))\n",
        "\n",
        "DATA_DIR = PROJECT_ROOT / 'data'\n",
        "RAW_DIR = DATA_DIR / 'raw'\n",
        "PROCESSED_DIR = DATA_DIR / 'processed'\n",
        "SAMPLE_DIR = DATA_DIR / 'sample'\n",
        "\n",
        "PROJECT_ROOT\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Goal\n",
        "Use ridge and lasso regression to handle correlated macro predictors.\n",
        "\n",
        "Why this notebook exists:\n",
        "- OLS coefficients can be unstable when predictors are correlated.\n",
        "- Ridge shrinks coefficients smoothly.\n",
        "- Lasso can set some coefficients exactly to 0 (feature selection-ish).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Primer: sklearn Pipelines (How To Avoid Preprocessing Leakage)\n",
        "\n",
        "### Why pipelines exist\n",
        "A common ML mistake is fitting preprocessing (scalers, imputers) on the full dataset.\n",
        "That leaks information from the test set into training.\n",
        "\n",
        "A `Pipeline` enforces the correct order:\n",
        "- fit preprocessing on training only\n",
        "- apply preprocessing to test\n",
        "- fit model on training only\n",
        "\n",
        "### Key API concepts\n",
        "- `fit(X, y)`: learn parameters from data (e.g., scaler means/standard deviations, model weights).\n",
        "- `transform(X)`: apply learned parameters to new data (e.g., scale).\n",
        "- `fit_transform(X, y)`: convenience that does both on the same data.\n",
        "\n",
        "If you do `scaler.fit(X_all)` before splitting, you leaked test-set information.\n",
        "\n",
        "### Example pattern\n",
        "```python\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "clf = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('model', LogisticRegression(max_iter=5000)),\n",
        "])\n",
        "\n",
        "# clf.fit(X_train, y_train)\n",
        "# y_prob = clf.predict_proba(X_test)[:, 1]\n",
        "```\n",
        "\n",
        "### Mini demo: the leakage you're avoiding (toy example)\n",
        "```python\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Pretend the last 20% of data comes from a different era with a different mean\n",
        "rng = np.random.default_rng(0)\n",
        "X_train = rng.normal(loc=0.0, scale=1.0, size=(100, 1))\n",
        "X_test  = rng.normal(loc=2.0, scale=1.0, size=(25, 1))\n",
        "\n",
        "# WRONG: fit scaler on train+test (leaks the future)\n",
        "sc_wrong = StandardScaler().fit(np.vstack([X_train, X_test]))\n",
        "X_test_wrong = sc_wrong.transform(X_test)\n",
        "\n",
        "# RIGHT: fit scaler on train only\n",
        "sc_right = StandardScaler().fit(X_train)\n",
        "X_test_right = sc_right.transform(X_test)\n",
        "\n",
        "print(\"test mean after wrong scaling:\", float(X_test_wrong.mean()))\n",
        "print(\"test mean after right scaling:\", float(X_test_right.mean()))\n",
        "```\n",
        "\n",
        "### What to remember\n",
        "- Always split by time first.\n",
        "- Then fit the pipeline on train.\n",
        "- Then evaluate on test.\n",
        "\n",
        "If you need different preprocessing for different columns, look into:\n",
        "- `sklearn.compose.ColumnTransformer`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"build-feature-matrix\"></a>\n",
        "## Build feature matrix\n",
        "\n",
        "### Goal\n",
        "Choose a target and feature set from the macro quarterly table.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Your Turn (1): Load data and pick columns\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "path = PROCESSED_DIR / 'macro_quarterly.csv'\n",
        "if path.exists():\n",
        "    df = pd.read_csv(path, index_col=0, parse_dates=True)\n",
        "else:\n",
        "    df = pd.read_csv(SAMPLE_DIR / 'macro_quarterly_sample.csv', index_col=0, parse_dates=True)\n",
        "\n",
        "y_col = 'gdp_growth_qoq'\n",
        "\n",
        "# TODO: Choose a feature list.\n",
        "# Tip: start with lagged features to avoid timing ambiguity.\n",
        "x_cols = [\n",
        "    'T10Y2Y_lag1',\n",
        "    'UNRATE_lag1',\n",
        "    'FEDFUNDS_lag1',\n",
        "    'INDPRO_lag1',\n",
        "    'RSAFS_lag1',\n",
        "    # TODO: add more lags/features if you want\n",
        "]\n",
        "\n",
        "df_m = df[[y_col] + x_cols].dropna().copy()\n",
        "df_m.tail()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Your Turn (2): Time split\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from src.evaluation import time_train_test_split_index\n",
        "\n",
        "split = time_train_test_split_index(len(df_m), test_size=0.2)\n",
        "train = df_m.iloc[split.train_slice]\n",
        "test = df_m.iloc[split.test_slice]\n",
        "\n",
        "X_train = train[x_cols]\n",
        "y_train = train[y_col]\n",
        "X_test = test[x_cols]\n",
        "y_test = test[y_col]\n",
        "\n",
        "X_train.shape, X_test.shape\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"fit-ridge-lasso\"></a>\n",
        "## Fit ridge/lasso\n",
        "\n",
        "### Goal\n",
        "Fit ridge and lasso over a range of regularization strengths and compare out-of-sample error.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Your Turn (1): Fit ridge and lasso across alpha grid\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import Ridge, Lasso\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "alphas = np.logspace(-3, 2, 20)\n",
        "\n",
        "ridge_rmse = []\n",
        "lasso_rmse = []\n",
        "\n",
        "for a in alphas:\n",
        "    ridge = Pipeline([\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('model', Ridge(alpha=float(a))),\n",
        "    ])\n",
        "    lasso = Pipeline([\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('model', Lasso(alpha=float(a), max_iter=20000)),\n",
        "    ])\n",
        "\n",
        "    ridge.fit(X_train, y_train)\n",
        "    lasso.fit(X_train, y_train)\n",
        "\n",
        "    ridge_pred = ridge.predict(X_test)\n",
        "    lasso_pred = lasso.predict(X_test)\n",
        "\n",
        "    ridge_rmse.append(mean_squared_error(y_test, ridge_pred, squared=False))\n",
        "    lasso_rmse.append(mean_squared_error(y_test, lasso_pred, squared=False))\n",
        "\n",
        "best_ridge = float(alphas[int(np.argmin(ridge_rmse))])\n",
        "best_lasso = float(alphas[int(np.argmin(lasso_rmse))])\n",
        "best_ridge, best_lasso\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Your Turn (2): Plot RMSE vs alpha\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# TODO: Plot ridge_rmse and lasso_rmse vs alphas (log scale).\n",
        "...\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"coefficient-paths\"></a>\n",
        "## Coefficient paths\n",
        "\n",
        "### Goal\n",
        "Visualize how coefficients shrink as regularization increases.\n",
        "\n",
        "This is one of the best ways to build intuition for what ridge/lasso are doing.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Your Turn (1): Fit models and record coefficients across alphas\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import Ridge, Lasso\n",
        "\n",
        "ridge_coefs = []\n",
        "lasso_coefs = []\n",
        "\n",
        "for a in alphas:\n",
        "    ridge = Pipeline([('scaler', StandardScaler()), ('model', Ridge(alpha=float(a)))])\n",
        "    lasso = Pipeline([('scaler', StandardScaler()), ('model', Lasso(alpha=float(a), max_iter=20000))])\n",
        "    ridge.fit(X_train, y_train)\n",
        "    lasso.fit(X_train, y_train)\n",
        "\n",
        "    ridge_coefs.append(ridge.named_steps['model'].coef_)\n",
        "    lasso_coefs.append(lasso.named_steps['model'].coef_)\n",
        "\n",
        "ridge_coefs = pd.DataFrame(ridge_coefs, columns=x_cols, index=alphas)\n",
        "lasso_coefs = pd.DataFrame(lasso_coefs, columns=x_cols, index=alphas)\n",
        "\n",
        "ridge_coefs.head()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Your Turn (2): Plot coefficient paths\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# TODO: Plot coefficient paths for ridge and lasso.\n",
        "# Hint: loop over columns and plot series on same axes.\n",
        "...\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"checkpoint-self-check\"></a>\n",
        "## Checkpoint (Self-Check)\n",
        "Run a few asserts and write 2-3 sentences summarizing what you verified.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# TODO: After you build X/y and split by time, validate the split.\n",
        "# Example (adjust variable names):\n",
        "# assert X_train.index.max() < X_test.index.min()\n",
        "# assert y_train.index.equals(X_train.index)\n",
        "# assert y_test.index.equals(X_test.index)\n",
        "# assert not X_train.isna().any().any()\n",
        "# assert not X_test.isna().any().any()\n",
        "...\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extensions (Optional)\n",
        "- Try one additional variant beyond the main path (different features, different split, different model).\n",
        "- Write down what improved, what got worse, and your hypothesis for why.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reflection\n",
        "- What did you assume implicitly (about timing, availability, stationarity, or costs)?\n",
        "- If you had to ship this model, what would you monitor?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"solutions-reference\"></a>\n",
        "## Solutions (Reference)\n",
        "\n",
        "Try the TODOs first. Use these only to unblock yourself or to compare approaches.\n",
        "\n",
        "<details><summary>Solution: Build feature matrix</summary>\n",
        "\n",
        "_One possible approach. Your variable names may differ; align them with the notebook._\n",
        "\n",
        "```python\n",
        "# Reference solution for 05_regularization_ridge_lasso \u2014 Build feature matrix\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df = pd.read_csv(SAMPLE_DIR / 'macro_quarterly_sample.csv', index_col=0, parse_dates=True).dropna()\n",
        "target = 'gdp_growth_qoq'\n",
        "X = df.drop(columns=[c for c in df.columns if c.startswith('gdp_') or c in {'GDPC1','recession','target_recession_next_q'}], errors='ignore')\n",
        "y = df[target]\n",
        "split = int(len(df)*0.8)\n",
        "X_tr, X_te = X.iloc[:split], X.iloc[split:]\n",
        "y_tr, y_te = y.iloc[:split], y.iloc[split:]\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "<details><summary>Solution: Fit ridge/lasso</summary>\n",
        "\n",
        "_One possible approach. Your variable names may differ; align them with the notebook._\n",
        "\n",
        "```python\n",
        "# Reference solution for 05_regularization_ridge_lasso \u2014 Fit ridge/lasso\n",
        "import numpy as np\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import Ridge, Lasso\n",
        "\n",
        "alphas = [0.01, 0.1, 1.0, 10.0]\n",
        "ridge_coefs = {}\n",
        "lasso_coefs = {}\n",
        "for a in alphas:\n",
        "    r = Pipeline([('scaler', StandardScaler()), ('m', Ridge(alpha=a))]).fit(X_tr, y_tr)\n",
        "    l = Pipeline([('scaler', StandardScaler()), ('m', Lasso(alpha=a, max_iter=5000))]).fit(X_tr, y_tr)\n",
        "    ridge_coefs[a] = r.named_steps['m'].coef_\n",
        "    lasso_coefs[a] = l.named_steps['m'].coef_\n",
        "ridge_coefs.keys(), lasso_coefs.keys()\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "<details><summary>Solution: Coefficient paths</summary>\n",
        "\n",
        "_One possible approach. Your variable names may differ; align them with the notebook._\n",
        "\n",
        "```python\n",
        "# Reference solution for 05_regularization_ridge_lasso \u2014 Coefficient paths\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot a few coefficient paths (first 5 features)\n",
        "feat_names = list(X.columns)\n",
        "for i in range(min(5, len(feat_names))):\n",
        "    plt.plot(alphas, [ridge_coefs[a][i] for a in alphas], label=f'Ridge {feat_names[i]}')\n",
        "plt.xscale('log')\n",
        "plt.legend()\n",
        "plt.title('Ridge coefficient paths (subset)')\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "python3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}