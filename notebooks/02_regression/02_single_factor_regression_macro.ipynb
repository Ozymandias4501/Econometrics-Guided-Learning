{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 02 Single-Factor Regression (Macro)\n",
        "\n",
        "GDP growth vs yield curve spread with time-series inference.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Table of Contents\n",
        "- [Load macro data](#load-macro-data)\n",
        "- [Fit OLS](#fit-ols)\n",
        "- [Fit HAC](#fit-hac)\n",
        "- [Interpretation](#interpretation)\n",
        "- [Checkpoint (Self-Check)](#checkpoint-self-check)\n",
        "- [Solutions (Reference)](#solutions-reference)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Why This Notebook Matters\n",
        "Regression is the bridge between statistics and ML. You will learn:\n",
        "- single-factor vs multi-factor interpretation,\n",
        "- robust standard errors,\n",
        "- coefficient stability and multicollinearity.\n",
        "\n",
        "\n",
        "## What You Will Produce\n",
        "- (no file output; learning/analysis notebook)\n",
        "\n",
        "## Success Criteria\n",
        "- You can explain what you built and why each step exists.\n",
        "- You can run your work end-to-end without undefined variables.\n",
        "\n",
        "## Common Pitfalls\n",
        "- Running cells top-to-bottom without reading the instructions.\n",
        "- Leaving `...` placeholders in code cells.\n",
        "- Treating coefficients as causal without a causal design.\n",
        "- Ignoring multicollinearity (unstable coefficients).\n",
        "\n",
        "## Matching Guide\n",
        "- `docs/guides/02_regression/02_single_factor_regression_macro.md`\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## How To Use This Notebook\n",
        "- This notebook is hands-on. Most code cells are incomplete on purpose.\n",
        "- Complete each TODO, then run the cell.\n",
        "- Use the matching guide (`docs/guides/02_regression/02_single_factor_regression_macro.md`) for deep explanations and alternative examples.\n",
        "- Write short interpretation notes as you go (what changed, why it matters).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"environment-bootstrap\"></a>\n",
        "## Environment Bootstrap\n",
        "Run this cell first. It makes the repo importable and defines common directories.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "\n",
        "def find_repo_root(start: Path) -> Path:\n",
        "    p = start\n",
        "    for _ in range(8):\n",
        "        if (p / 'src').exists() and (p / 'docs').exists():\n",
        "            return p\n",
        "        p = p.parent\n",
        "    raise RuntimeError('Could not find repo root. Start Jupyter from the repo root.')\n",
        "\n",
        "\n",
        "PROJECT_ROOT = find_repo_root(Path.cwd())\n",
        "if str(PROJECT_ROOT) not in sys.path:\n",
        "    sys.path.append(str(PROJECT_ROOT))\n",
        "\n",
        "DATA_DIR = PROJECT_ROOT / 'data'\n",
        "RAW_DIR = DATA_DIR / 'raw'\n",
        "PROCESSED_DIR = DATA_DIR / 'processed'\n",
        "SAMPLE_DIR = DATA_DIR / 'sample'\n",
        "\n",
        "PROJECT_ROOT\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Goal\n",
        "Fit a classic single-factor macro regression: GDP growth vs yield curve spread.\n",
        "\n",
        "This is a great first macro regression because:\n",
        "- it is easy to visualize,\n",
        "- it has a well-known economic story,\n",
        "- it demonstrates why time-series inference (HAC SE) matters.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Primer: pandas Time Series Essentials\n",
        "\n",
        "### DatetimeIndex\n",
        "Most time series work in pandas assumes your DataFrame is indexed by time.\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "# idx = pd.to_datetime(df['date'])\n",
        "# df = df.set_index(idx).sort_index()\n",
        "# assert isinstance(df.index, pd.DatetimeIndex)\n",
        "```\n",
        "\n",
        "If you see weird behavior (resample errors, merges not aligning), check:\n",
        "- `df.index.dtype`\n",
        "- `df.index.min(), df.index.max()`\n",
        "- `df.index.is_monotonic_increasing`\n",
        "\n",
        "### Resampling\n",
        "> **Goal:** convert one frequency to another.\n",
        "\n",
        "Common examples:\n",
        "- daily -> month-end\n",
        "- monthly -> quarter-end\n",
        "\n",
        "```python\n",
        "# month-end last value\n",
        "# df_me_last = df.resample('ME').last()\n",
        "\n",
        "# month-end mean\n",
        "# df_me_mean = df.resample('ME').mean()\n",
        "\n",
        "# quarter-end mean\n",
        "# df_q_mean = df.resample('QE').mean()\n",
        "```\n",
        "\n",
        "**Interpretation matters.** For economic series:\n",
        "- using `.last()` treats the end-of-period value as \u201cthe period\u2019s value\u201d\n",
        "- using `.mean()` treats the period average as \u201cthe period\u2019s value\u201d\n",
        "\n",
        "Neither is universally correct; you should choose based on measurement and use case.\n",
        "\n",
        "### Alignment and merging\n",
        "When joining multiple time series, you need to ensure they share:\n",
        "- the same index type (`DatetimeIndex`)\n",
        "- the same frequency convention (month-end vs month-start; quarter-end vs quarter-start)\n",
        "\n",
        "```python\n",
        "# Example: join two series and inspect missingness\n",
        "# df = df1.join(df2, how='outer').sort_index()\n",
        "# print(df.isna().sum())\n",
        "```\n",
        "\n",
        "### Lags and rolling windows\n",
        "> **Lag:** use past values as features.\n",
        "\n",
        "```python\n",
        "# lag 1 period\n",
        "# df['x_lag1'] = df['x'].shift(1)\n",
        "\n",
        "# rolling mean (past-only)\n",
        "# df['x_roll12'] = df['x'].rolling(12).mean()\n",
        "```\n",
        "\n",
        "### Common gotchas\n",
        "- `shift(-1)` uses the future (leakage for forecasting).\n",
        "- `rolling(..., center=True)` uses future values.\n",
        "- Always `dropna()` after creating lags/rolls to get clean modeling rows.\n",
        "\n",
        "One more gotcha:\n",
        "- If you resample daily -> monthly and then create lags, your lag is \u201cone month\u201d (not one day). Lags are measured in the current index frequency.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Primer: statsmodels vs scikit-learn (Inference vs Prediction)\n",
        "\n",
        "### Two different goals\n",
        "- **Prediction (ML):** optimize out-of-sample accuracy.\n",
        "- **Inference (econometrics):** interpret coefficients + quantify uncertainty.\n",
        "\n",
        "scikit-learn is mostly prediction-focused.\n",
        "statsmodels is built for inference (standard errors, p-values, confidence intervals).\n",
        "\n",
        "### Minimal statsmodels OLS pattern\n",
        "```python\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# X: DataFrame of features\n",
        "# y: Series target\n",
        "\n",
        "# Add intercept column\n",
        "# Xc = sm.add_constant(X, has_constant='add')\n",
        "\n",
        "# Fit OLS\n",
        "# res = sm.OLS(y, Xc).fit()\n",
        "# print(res.summary())\n",
        "```\n",
        "\n",
        "### What you're looking at in `res.summary()`\n",
        "Typical regression output includes:\n",
        "- **coef**: estimated coefficients $\\hat\\beta$\n",
        "- **std err**: estimated standard errors $\\widehat{SE}(\\hat\\beta)$\n",
        "- **t**: t-statistic $\\hat\\beta / \\widehat{SE}(\\hat\\beta)$\n",
        "- **P>|t|**: p-value for $H_0: \\beta=0$ (under assumptions)\n",
        "- **[0.025, 0.975]**: 95% confidence interval for $\\beta$\n",
        "\n",
        "You can access these programmatically:\n",
        "\n",
        "```python\n",
        "# res.params        # coefficients (pandas Series)\n",
        "# res.bse           # standard errors\n",
        "# res.pvalues       # p-values\n",
        "# res.conf_int()    # confidence intervals\n",
        "```\n",
        "\n",
        "### Robust standard errors\n",
        "Robust SE change uncertainty estimates, not coefficients.\n",
        "\n",
        "```python\n",
        "# HC3 (cross-section)\n",
        "# res_hc3 = res.get_robustcov_results(cov_type='HC3')\n",
        "\n",
        "# HAC/Newey-West (time series)\n",
        "# res_hac = res.get_robustcov_results(cov_type='HAC', cov_kwds={'maxlags': 4})\n",
        "```\n",
        "\n",
        "Why this matters:\n",
        "- Cross-sectional data often has **heteroskedasticity** (error variance changes across observations).\n",
        "- Time series often has **autocorrelation** (errors correlated over time).\n",
        "\n",
        "### Practical rule\n",
        "- Use sklearn for predictive pipelines and cross-validation.\n",
        "- Use statsmodels when you need inference and careful interpretation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Primer: Hypothesis Testing (p-values, t-tests, and Confidence Intervals)\n",
        "\n",
        "You will see p-values, t-statistics, and confidence intervals in regression output (especially `statsmodels`).\n",
        "This primer gives you the minimum you need to avoid the most common misunderstandings.\n",
        "\n",
        "### Definitions (in plain language)\n",
        "- **Hypothesis**: a claim about an unknown population quantity (a parameter).\n",
        "- **Null hypothesis** $H_0$: the \"default\" claim (often \u201cno effect\u201d).\n",
        "- **Alternative hypothesis** $H_1$: the claim you consider if the data looks inconsistent with $H_0$.\n",
        "- **Test statistic**: a number computed from data that measures how surprising the data is under $H_0$.\n",
        "- **p-value**: the probability (under the assumptions of the null model) of seeing a test statistic at least as extreme as you observed.\n",
        "- **Significance level** $\\alpha$: a pre-chosen cutoff (commonly 0.05) used to decide whether to reject $H_0$.\n",
        "\n",
        "### What a p-value is NOT\n",
        "- It is **not** the probability $H_0$ is true.\n",
        "- It is **not** the probability your model is correct.\n",
        "- It is **not** a measure of economic importance.\n",
        "\n",
        "### Type I / Type II errors and power\n",
        "- **Type I error (false positive)**: rejecting $H_0$ when $H_0$ is true. Rough probability $\\approx \\alpha$ under assumptions.\n",
        "- **Type II error (false negative)**: failing to reject $H_0$ when $H_1$ is true.\n",
        "- **Power**: $1 - P(\\text{Type II error})$. Power increases with larger samples and larger true effects.\n",
        "\n",
        "### Regression t-test intuition\n",
        "In OLS regression we estimate coefficients $\\hat\\beta$. A common test is:\n",
        "- $H_0: \\beta_j = 0$ (no linear association between $x_j$ and $y$ holding other features fixed)\n",
        "\n",
        "The **t-statistic** is:\n",
        "\n",
        "$$\n",
        "t_j = \\frac{\\hat\\beta_j - 0}{\\widehat{SE}(\\hat\\beta_j)}\n",
        "$$\n",
        "\n",
        "Interpretation (roughly):\n",
        "- if $|t_j|$ is large, $\\hat\\beta_j$ is far from 0 relative to its uncertainty estimate\n",
        "- if $|t_j|$ is small, the data is compatible with $\\beta_j$ being near 0 (given assumptions)\n",
        "\n",
        "### Confidence intervals (CI) connect to hypothesis tests\n",
        "A 95% CI is usually reported as:\n",
        "\n",
        "$$\n",
        "\\hat\\beta_j \\pm t_{0.975}\\cdot \\widehat{SE}(\\hat\\beta_j)\n",
        "$$\n",
        "\n",
        "If the 95% CI does not include 0, the two-sided p-value is typically < 0.05.\n",
        "\n",
        "### Python demo (toy): one-sample t-test and a regression coefficient test\n",
        "This is not your project data; it is purely to make the objects concrete.\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "from scipy import stats\n",
        "\n",
        "rng = np.random.default_rng(0)\n",
        "\n",
        "# 1) One-sample t-test: is the mean of x equal to 0?\n",
        "x = rng.normal(loc=0.2, scale=1.0, size=200)\n",
        "t_stat, p_val = stats.ttest_1samp(x, popmean=0.0)\n",
        "print(\"t-test t:\", t_stat, \"p:\", p_val)\n",
        "\n",
        "# 2) Regression t-test: is the slope on x equal to 0?\n",
        "n = 300\n",
        "x2 = rng.normal(size=n)\n",
        "eps = rng.normal(scale=1.0, size=n)\n",
        "y = 1.0 + 0.5 * x2 + eps\n",
        "\n",
        "df = pd.DataFrame({\"y\": y, \"x\": x2})\n",
        "X = sm.add_constant(df[[\"x\"]])\n",
        "res = sm.OLS(df[\"y\"], X).fit()\n",
        "\n",
        "# The summary includes coef, SE, t, p, and CI\n",
        "print(res.summary())\n",
        "\n",
        "# Manual t-stat for slope (matches summary output)\n",
        "beta_hat = res.params[\"x\"]\n",
        "se_hat = res.bse[\"x\"]\n",
        "print(\"manual t:\", beta_hat / se_hat)\n",
        "```\n",
        "\n",
        "### Common ways hypothesis testing goes wrong in ML + macro\n",
        "- **Multiple testing**: you try many features/specifications; some will look \u201csignificant\u201d by chance.\n",
        "- **Violating assumptions**: autocorrelation and heteroskedasticity can make naive SE too small.\n",
        "- **Confusing predictive success with causal claims**: a coefficient can predict well without being causal.\n",
        "\n",
        "Practical guidance for this project:\n",
        "- Report effect sizes + uncertainty, not just \u201csignificant / not significant.\u201d\n",
        "- For macro time series, prefer robust SE (HAC/Newey-West) when interpreting p-values.\n",
        "- For predictive tasks, always complement p-values with out-of-sample evaluation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"load-macro-data\"></a>\n",
        "## Load macro data\n",
        "\n",
        "### Goal\n",
        "Load the quarterly macro table produced earlier (`macro_quarterly.csv`).\n",
        "\n",
        "If you haven't built it yet, use the bundled sample.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Your Turn (1): Load macro_quarterly.csv (or sample)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "path = PROCESSED_DIR / 'macro_quarterly.csv'\n",
        "if path.exists():\n",
        "    df = pd.read_csv(path, index_col=0, parse_dates=True)\n",
        "else:\n",
        "    df = pd.read_csv(SAMPLE_DIR / 'macro_quarterly_sample.csv', index_col=0, parse_dates=True)\n",
        "\n",
        "df.head()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Your Turn (2): Choose target and predictor\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Target: GDP growth\n",
        "y_col = 'gdp_growth_qoq'\n",
        "\n",
        "# Predictor: yield curve spread (try lagged)\n",
        "# TODO: Try 'T10Y2Y_lag1' first.\n",
        "x_cols = ['T10Y2Y_lag1']\n",
        "\n",
        "# Build modeling table\n",
        "df_m = df[[y_col] + x_cols].dropna().copy()\n",
        "df_m.tail()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Checkpoint (time order + no NaNs)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "assert df_m.index.is_monotonic_increasing\n",
        "assert not df_m.isna().any().any()\n",
        "assert df_m.shape[0] > 30\n",
        "...\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"fit-ols\"></a>\n",
        "## Fit OLS\n",
        "\n",
        "### Goal\n",
        "Fit OLS on a time-based train/test split and evaluate out-of-sample error.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Your Turn (1): Time split\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from src.evaluation import time_train_test_split_index\n",
        "\n",
        "# TODO: Create a time split (first 80% train, last 20% test)\n",
        "split = time_train_test_split_index(len(df_m), test_size=0.2)\n",
        "train = df_m.iloc[split.train_slice]\n",
        "test = df_m.iloc[split.test_slice]\n",
        "\n",
        "train.index.max(), test.index.min()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Your Turn (2): Fit OLS on train and evaluate on test\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import statsmodels.api as sm\n",
        "from src.evaluation import regression_metrics\n",
        "\n",
        "# Build design matrices\n",
        "X_tr = sm.add_constant(train[x_cols], has_constant='add')\n",
        "y_tr = train[y_col]\n",
        "X_te = sm.add_constant(test[x_cols], has_constant='add')\n",
        "y_te = test[y_col]\n",
        "\n",
        "# Fit\n",
        "res_ols = sm.OLS(y_tr, X_tr).fit()\n",
        "y_hat = res_ols.predict(X_te)\n",
        "\n",
        "metrics = regression_metrics(y_te.to_numpy(), y_hat.to_numpy())\n",
        "metrics\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"fit-hac\"></a>\n",
        "## Fit HAC\n",
        "\n",
        "### Goal\n",
        "Compare naive OLS standard errors to HAC/Newey-West robust standard errors.\n",
        "\n",
        "Key idea:\n",
        "- coefficients can stay the same\n",
        "- p-values and confidence intervals can change (sometimes a lot)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Your Turn (1): Fit HAC with different maxlags\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from src import econometrics\n",
        "\n",
        "# TODO: Fit HAC on the FULL sample (inference focus) with different maxlags.\n",
        "res_naive = econometrics.fit_ols(df_m, y_col=y_col, x_cols=x_cols)\n",
        "res_hac1 = econometrics.fit_ols_hac(df_m, y_col=y_col, x_cols=x_cols, maxlags=1)\n",
        "res_hac4 = econometrics.fit_ols_hac(df_m, y_col=y_col, x_cols=x_cols, maxlags=4)\n",
        "\n",
        "print('naive p:', res_naive.pvalues)\n",
        "print('hac1  p:', res_hac1.pvalues)\n",
        "print('hac4  p:', res_hac4.pvalues)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Your Turn (2): Compare confidence intervals\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# TODO: Compare CI for the yield spread coefficient under naive vs HAC.\n",
        "...\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"interpretation\"></a>\n",
        "## Interpretation\n",
        "\n",
        "Write a short interpretation (8-12 sentences):\n",
        "- What sign do you expect for the yield spread coefficient, and why?\n",
        "- What does a 1 percentage-point change in spread mean for predicted GDP growth (units!)?\n",
        "- How does your inference change under HAC SE?\n",
        "- What limitations do you see (endogeneity, omitted variables, regime changes)?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Your Turn: Write your interpretation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "notes = \"\"\"\n",
        "...\n",
        "\"\"\"\n",
        "print(notes)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"checkpoint-self-check\"></a>\n",
        "## Checkpoint (Self-Check)\n",
        "Run a few asserts and write 2-3 sentences summarizing what you verified.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# TODO: After you build X/y and split by time, validate the split.\n",
        "# Example (adjust variable names):\n",
        "# assert X_train.index.max() < X_test.index.min()\n",
        "# assert y_train.index.equals(X_train.index)\n",
        "# assert y_test.index.equals(X_test.index)\n",
        "# assert not X_train.isna().any().any()\n",
        "# assert not X_test.isna().any().any()\n",
        "...\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extensions (Optional)\n",
        "- Try one additional variant beyond the main path (different features, different split, different model).\n",
        "- Write down what improved, what got worse, and your hypothesis for why.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reflection\n",
        "- What did you assume implicitly (about timing, availability, stationarity, or costs)?\n",
        "- If you had to ship this model, what would you monitor?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"solutions-reference\"></a>\n",
        "## Solutions (Reference)\n",
        "\n",
        "Try the TODOs first. Use these only to unblock yourself or to compare approaches.\n",
        "\n",
        "<details><summary>Solution: Load macro data</summary>\n",
        "\n",
        "_One possible approach. Your variable names may differ; align them with the notebook._\n",
        "\n",
        "```python\n",
        "# Reference solution for 02_single_factor_regression_macro \u2014 Load macro data\n",
        "import pandas as pd\n",
        "df = pd.read_csv(SAMPLE_DIR / 'macro_quarterly_sample.csv', index_col=0, parse_dates=True)\n",
        "df.head()\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "<details><summary>Solution: Fit OLS</summary>\n",
        "\n",
        "_One possible approach. Your variable names may differ; align them with the notebook._\n",
        "\n",
        "```python\n",
        "# Reference solution for 02_single_factor_regression_macro \u2014 Fit OLS\n",
        "from src import econometrics\n",
        "\n",
        "res = econometrics.fit_ols(df, y_col='gdp_growth_qoq', x_cols=['T10Y2Y'])\n",
        "print(res.summary())\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "<details><summary>Solution: Fit HAC</summary>\n",
        "\n",
        "_One possible approach. Your variable names may differ; align them with the notebook._\n",
        "\n",
        "```python\n",
        "# Reference solution for 02_single_factor_regression_macro \u2014 Fit HAC\n",
        "res_hac = econometrics.fit_ols_hac(df, y_col='gdp_growth_qoq', x_cols=['T10Y2Y'], maxlags=2)\n",
        "print(res_hac.summary())\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "<details><summary>Solution: Interpretation</summary>\n",
        "\n",
        "_One possible approach. Your variable names may differ; align them with the notebook._\n",
        "\n",
        "```python\n",
        "# Reference solution for 02_single_factor_regression_macro \u2014 Interpretation\n",
        "# Interpret sign/magnitude carefully; time-series inference is fragile.\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "python3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}