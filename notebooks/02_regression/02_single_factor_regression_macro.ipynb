{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 Single-Factor Regression (Macro)\n",
    "\n",
    "GDP growth vs yield curve spread with time-series inference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [Load macro data](#load-macro-data)\n",
    "- [Fit OLS](#fit-ols)\n",
    "- [Fit HAC](#fit-hac)\n",
    "- [Interpretation](#interpretation)\n",
    "- [Checkpoint (Self-Check)](#checkpoint-self-check)\n",
    "- [Solutions (Reference)](#solutions-reference)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why This Notebook Matters\n",
    "Regression is the bridge between statistics and ML. You will learn:\n",
    "- single-factor vs multi-factor interpretation,\n",
    "- robust standard errors,\n",
    "- coefficient stability and multicollinearity.\n",
    "\n",
    "\n",
    "## Prerequisites (Quick Self-Check)\n",
    "- Completed Parts 00–01 (foundations + data).\n",
    "- Basic algebra comfort (reading coefficient tables, units).\n",
    "\n",
    "## What You Will Produce\n",
    "- (no file output; learning/analysis notebook)\n",
    "\n",
    "## Success Criteria\n",
    "- You can explain what you built and why each step exists.\n",
    "- You can run your work end-to-end without undefined variables.\n",
    "\n",
    "## Common Pitfalls\n",
    "- Running cells top-to-bottom without reading the instructions.\n",
    "- Leaving `...` placeholders in code cells.\n",
    "- Treating coefficients as causal without a causal design.\n",
    "- Ignoring multicollinearity (unstable coefficients).\n",
    "\n",
    "## Quick Fixes (When You Get Stuck)\n",
    "- If you see `ModuleNotFoundError`, re-run the bootstrap cell and restart the kernel; make sure `PROJECT_ROOT` is the repo root.\n",
    "- If a `data/processed/*` file is missing, either run the matching build script (see guide) or use the notebook’s `data/sample/*` fallback.\n",
    "- If results look “too good,” suspect leakage; re-check shifts, rolling windows, and time splits.\n",
    "- If a model errors, check dtypes (`astype(float)`) and missingness (`dropna()` on required columns).\n",
    "\n",
    "## Matching Guide\n",
    "- `docs/guides/02_regression/02_single_factor_regression_macro.md`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How To Use This Notebook\n",
    "- Work section-by-section; don’t skip the markdown.\n",
    "- Most code cells are incomplete on purpose: replace TODOs and `...`, then run.\n",
    "- After each section, write 2–4 sentences answering the interpretation prompts (what changed, why it matters).\n",
    "- Prefer `data/processed/*` if you have built the real datasets; otherwise use the bundled `data/sample/*` fallbacks.\n",
    "- Use the **Checkpoint (Self-Check)** section to catch mistakes early.\n",
    "- Use **Solutions (Reference)** only to unblock yourself; then re-implement without looking.\n",
    "- Use the matching guide (`docs/guides/02_regression/02_single_factor_regression_macro.md`) for the math, assumptions, and deeper context.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"environment-bootstrap\"></a>\n",
    "## Environment Bootstrap\n",
    "Run this cell first. It makes the repo importable and defines common directories.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "\n",
    "def find_repo_root(start: Path) -> Path:\n",
    "    p = start\n",
    "    for _ in range(8):\n",
    "        if (p / 'src').exists() and (p / 'docs').exists():\n",
    "            return p\n",
    "        p = p.parent\n",
    "    raise RuntimeError('Could not find repo root. Start Jupyter from the repo root.')\n",
    "\n",
    "\n",
    "PROJECT_ROOT = find_repo_root(Path.cwd())\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "RAW_DIR = DATA_DIR / 'raw'\n",
    "PROCESSED_DIR = DATA_DIR / 'processed'\n",
    "SAMPLE_DIR = DATA_DIR / 'sample'\n",
    "\n",
    "PROJECT_ROOT\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal\n",
    "Fit a classic single-factor macro regression: GDP growth vs yield curve spread.\n",
    "\n",
    "This is a great first macro regression because:\n",
    "- it is easy to visualize,\n",
    "- it has a well-known economic story,\n",
    "- it demonstrates why time-series inference (HAC SE) matters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primer: pandas time series essentials (indexing, resampling, lags)\n",
    "\n",
    "Most “mysterious bugs” in time series work come from index and alignment mistakes. This primer gives you the minimum patterns to avoid them.\n",
    "\n",
    "### 1) DatetimeIndex (the first thing to verify)\n",
    "\n",
    "Most time-series operations assume a `DatetimeIndex`:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "df = df.copy()\n",
    "df.index = pd.to_datetime(df.index)\n",
    "df = df.sort_index()\n",
    "assert isinstance(df.index, pd.DatetimeIndex)\n",
    "```\n",
    "\n",
    "**Expected output / sanity checks**\n",
    "- `df.index.min(), df.index.max()` look reasonable\n",
    "- `df.index.is_monotonic_increasing` is `True`\n",
    "\n",
    "### 2) Resampling (frequency alignment)\n",
    "\n",
    "Resampling converts one frequency to another. Choose the aggregation rule intentionally.\n",
    "\n",
    "```python\n",
    "# month-end last value (end-of-period)\n",
    "df_me_last = df.resample(\"ME\").last()\n",
    "\n",
    "# month-end mean (average-of-period)\n",
    "df_me_mean = df.resample(\"ME\").mean()\n",
    "\n",
    "# quarter-end mean\n",
    "df_q_mean = df.resample(\"QE\").mean()\n",
    "```\n",
    "\n",
    "**Interpretation matters**\n",
    "- `.last()` treats end-of-period value as “the period’s value.”\n",
    "- `.mean()` treats the period average as “the period’s value.”\n",
    "\n",
    "### 3) Alignment and merging\n",
    "\n",
    "When joining series, always check missingness after the join:\n",
    "\n",
    "```python\n",
    "merged = df1.join(df2, how=\"outer\").sort_index()\n",
    "print(merged.isna().sum().sort_values(ascending=False).head(10))\n",
    "```\n",
    "\n",
    "### 4) Lags and rolling windows (watch for leakage!)\n",
    "\n",
    "```python\n",
    "# lag 1 period (past-only)\n",
    "df[\"x_lag1\"] = df[\"x\"].shift(1)\n",
    "\n",
    "# rolling mean using past values ending at t\n",
    "df[\"x_roll12\"] = df[\"x\"].rolling(12).mean()\n",
    "```\n",
    "\n",
    "**Leakage pitfalls**\n",
    "- `shift(-1)` uses the future.\n",
    "- `rolling(..., center=True)` uses the future.\n",
    "\n",
    "### 5) A quick workflow you should repeat\n",
    "\n",
    "1) Set and verify DatetimeIndex.\n",
    "2) Resample intentionally (mean vs last).\n",
    "3) Join and inspect missingness.\n",
    "4) Add lags/rolls (past-only).\n",
    "5) `dropna()` to build a clean modeling table.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primer: `statsmodels` vs `scikit-learn` (inference vs prediction)\n",
    "\n",
    "This repo uses both libraries because they serve different goals:\n",
    "\n",
    "- **Prediction (ML):** optimize out-of-sample accuracy → `scikit-learn`\n",
    "- **Inference (econometrics):** interpret coefficients + quantify uncertainty → `statsmodels`\n",
    "\n",
    "### Minimal `statsmodels` OLS pattern\n",
    "\n",
    "```python\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# X: DataFrame of features, y: Series target\n",
    "Xc = sm.add_constant(X, has_constant=\"add\")  # add intercept\n",
    "res = sm.OLS(y, Xc).fit()\n",
    "print(res.summary())\n",
    "```\n",
    "\n",
    "**Expected output / sanity check**\n",
    "- a table with `coef`, `std err`, `t`, `P>|t|`, and a CI column\n",
    "- coefficient names match your column names\n",
    "\n",
    "### What you are looking at in `res.summary()`\n",
    "\n",
    "- **coef**: $\\\\hat\\\\beta$ (estimated effect in the model)\n",
    "- **std err**: estimated uncertainty $\\\\widehat{SE}(\\\\hat\\\\beta)$\n",
    "- **t**: $\\\\hat\\\\beta / \\\\widehat{SE}(\\\\hat\\\\beta)$\n",
    "- **P>|t|**: p-value for $H_0: \\\\beta=0$ (conditional on assumptions)\n",
    "- **[0.025, 0.975]**: 95% confidence interval\n",
    "\n",
    "### Robust standard errors (change uncertainty, not coefficients)\n",
    "\n",
    "```python\n",
    "# Cross-section heteroskedasticity\n",
    "res_hc3 = res.get_robustcov_results(cov_type=\"HC3\")\n",
    "\n",
    "# Time series autocorrelation + heteroskedasticity\n",
    "res_hac = res.get_robustcov_results(cov_type=\"HAC\", cov_kwds={\"maxlags\": 4})\n",
    "```\n",
    "\n",
    "### Common pitfalls (and quick fixes)\n",
    "\n",
    "- **Forgetting the intercept**\n",
    "  - Fix: always `add_constant`.\n",
    "- **Wrong SE for time series**\n",
    "  - Fix: use HAC when residuals are autocorrelated.\n",
    "- **Treating p-values as causal proof**\n",
    "  - Fix: write the identification assumption; otherwise interpret as association.\n",
    "- **Mixing prediction and inference**\n",
    "  - Fix: use `sklearn` pipelines + time splits for prediction; use `statsmodels` for coefficient uncertainty.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primer: Hypothesis testing (p-values, t-stats, confidence intervals)\n",
    "\n",
    "You will see p-values, t-statistics, and confidence intervals in regression output (especially `statsmodels`). This primer gives you the minimum to interpret them correctly.\n",
    "\n",
    "### The objects (plain language)\n",
    "\n",
    "- **Null hypothesis** $H_0$: the default claim (often “no effect”).\n",
    "- **Alternative** $H_1$: the claim you consider if the data looks inconsistent with $H_0$.\n",
    "- **Test statistic**: “how far” your estimate is from the null, in uncertainty units.\n",
    "- **p-value**: probability (under the null *and model assumptions*) of seeing a test statistic at least as extreme as observed.\n",
    "- **Confidence interval (CI)**: a range of parameter values consistent with the data under assumptions.\n",
    "\n",
    "### What a p-value is NOT\n",
    "\n",
    "- Not the probability $H_0$ is true.\n",
    "- Not the probability the model is correct.\n",
    "- Not a measure of economic importance.\n",
    "\n",
    "### Regression t-test intuition\n",
    "\n",
    "In OLS, a common test is $H_0: \\\\beta_j = 0$.\n",
    "\n",
    "$$\n",
    "t_j = \\\\frac{\\\\hat\\\\beta_j}{\\\\widehat{SE}(\\\\hat\\\\beta_j)}\n",
    "$$\n",
    "\n",
    "If you change your SE estimator (HC3/HAC/cluster), you change $\\\\widehat{SE}$ and therefore the p-value, even if the coefficient stays the same.\n",
    "\n",
    "### Expected output / what you should look at in `res.summary()`\n",
    "\n",
    "- `coef`: effect size (in model units)\n",
    "- `std err`: uncertainty\n",
    "- CI columns: magnitude + uncertainty together\n",
    "\n",
    "### Common pitfalls in this project\n",
    "\n",
    "- Macro time series often have autocorrelation → naive SE too small → use HAC when interpreting p-values.\n",
    "- Multiple testing/spec-search can produce small p-values by chance.\n",
    "- Predictive success ≠ causal interpretation.\n",
    "\n",
    "### Tiny demo (toy; not project data)\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "rng = np.random.default_rng(0)\n",
    "n = 300\n",
    "x = rng.normal(size=n)\n",
    "y = 1.0 + 0.5 * x + rng.normal(scale=1.0, size=n)\n",
    "\n",
    "df = pd.DataFrame({\"y\": y, \"x\": x})\n",
    "X = sm.add_constant(df[[\"x\"]])\n",
    "res = sm.OLS(df[\"y\"], X).fit()\n",
    "print(res.summary())\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"load-macro-data\"></a>\n",
    "## Load macro data\n",
    "\n",
    "### Goal\n",
    "Load the quarterly macro table produced earlier (`macro_quarterly.csv`).\n",
    "\n",
    "If you haven't built it yet, use the bundled sample.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Turn (1): Load macro_quarterly.csv (or sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path = PROCESSED_DIR / 'macro_quarterly.csv'\n",
    "if path.exists():\n",
    "    df = pd.read_csv(path, index_col=0, parse_dates=True)\n",
    "else:\n",
    "    df = pd.read_csv(SAMPLE_DIR / 'macro_quarterly_sample.csv', index_col=0, parse_dates=True)\n",
    "\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Turn (2): Choose target and predictor\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Target: GDP growth\n",
    "y_col = 'gdp_growth_qoq'\n",
    "\n",
    "# Predictor: yield curve spread (try lagged)\n",
    "# TODO: Try 'T10Y2Y_lag1' first.\n",
    "x_cols = ['T10Y2Y_lag1']\n",
    "\n",
    "# Build modeling table\n",
    "df_m = df[[y_col] + x_cols].dropna().copy()\n",
    "df_m.tail()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint (time order + no NaNs)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "assert df_m.index.is_monotonic_increasing\n",
    "assert not df_m.isna().any().any()\n",
    "assert df_m.shape[0] > 30\n",
    "...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"fit-ols\"></a>\n",
    "## Fit OLS\n",
    "\n",
    "### Goal\n",
    "Fit OLS on a time-based train/test split and evaluate out-of-sample error.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Turn (1): Time split\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from src.evaluation import time_train_test_split_index\n",
    "\n",
    "# TODO: Create a time split (first 80% train, last 20% test)\n",
    "split = time_train_test_split_index(len(df_m), test_size=0.2)\n",
    "train = df_m.iloc[split.train_slice]\n",
    "test = df_m.iloc[split.test_slice]\n",
    "\n",
    "train.index.max(), test.index.min()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Turn (2): Fit OLS on train and evaluate on test\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from src.evaluation import regression_metrics\n",
    "\n",
    "# Build design matrices\n",
    "X_tr = sm.add_constant(train[x_cols], has_constant='add')\n",
    "y_tr = train[y_col]\n",
    "X_te = sm.add_constant(test[x_cols], has_constant='add')\n",
    "y_te = test[y_col]\n",
    "\n",
    "# Fit\n",
    "res_ols = sm.OLS(y_tr, X_tr).fit()\n",
    "y_hat = res_ols.predict(X_te)\n",
    "\n",
    "metrics = regression_metrics(y_te.to_numpy(), y_hat.to_numpy())\n",
    "metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"fit-hac\"></a>\n",
    "## Fit HAC\n",
    "\n",
    "### Goal\n",
    "Compare naive OLS standard errors to HAC/Newey-West robust standard errors.\n",
    "\n",
    "Key idea:\n",
    "- coefficients can stay the same\n",
    "- p-values and confidence intervals can change (sometimes a lot)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Turn (1): Fit HAC with different maxlags\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from src import econometrics\n",
    "\n",
    "# TODO: Fit HAC on the FULL sample (inference focus) with different maxlags.\n",
    "res_naive = econometrics.fit_ols(df_m, y_col=y_col, x_cols=x_cols)\n",
    "res_hac1 = econometrics.fit_ols_hac(df_m, y_col=y_col, x_cols=x_cols, maxlags=1)\n",
    "res_hac4 = econometrics.fit_ols_hac(df_m, y_col=y_col, x_cols=x_cols, maxlags=4)\n",
    "\n",
    "print('naive p:', res_naive.pvalues)\n",
    "print('hac1  p:', res_hac1.pvalues)\n",
    "print('hac4  p:', res_hac4.pvalues)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Turn (2): Compare confidence intervals\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: Compare CI for the yield spread coefficient under naive vs HAC.\n",
    "...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "esgml8n96o",
   "source": "## Prediction intervals vs confidence intervals\n\nWhen you use a regression for forecasting, there are two types of intervals:\n\n- **Confidence interval for the mean**: How uncertain is the *average* GDP growth at a given spread value? This reflects uncertainty in the coefficient estimates. It narrows as the sample grows.\n\n- **Prediction interval for a new observation**: How uncertain is the *next actual* GDP growth at a given spread value? This reflects coefficient uncertainty **plus** the irreducible noise in individual outcomes. It is always wider than the confidence interval.\n\nIn `statsmodels`:\n```python\npred = res_ols.get_prediction(X_te)\nsummary = pred.summary_frame(alpha=0.05)\n# 'mean_ci_lower/upper' = confidence interval for the conditional mean\n# 'obs_ci_lower/upper'  = prediction interval for a new observation\n```\n\nFor the recession forecasting application, the prediction interval is what matters for decision-making: you care about the plausible range of *next quarter's* GDP growth, not just where the average might be.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"interpretation\"></a>\n",
    "## Interpretation\n",
    "\n",
    "Write a short interpretation (8-12 sentences):\n",
    "- What sign do you expect for the yield spread coefficient, and why?\n",
    "- What does a 1 percentage-point change in spread mean for predicted GDP growth (units!)?\n",
    "- How does your inference change under HAC SE?\n",
    "- What limitations do you see (endogeneity, omitted variables, regime changes)?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Turn: Write your interpretation\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "notes = \"\"\"\n",
    "...\n",
    "\"\"\"\n",
    "print(notes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"checkpoint-self-check\"></a>\n",
    "## Checkpoint (Self-Check)\n",
    "Run a few asserts and write 2-3 sentences summarizing what you verified.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: After you build X/y and split by time, validate the split.\n",
    "# Example (adjust variable names):\n",
    "# assert X_train.index.max() < X_test.index.min()\n",
    "# assert y_train.index.equals(X_train.index)\n",
    "# assert y_test.index.equals(X_test.index)\n",
    "# assert not X_train.isna().any().any()\n",
    "# assert not X_test.isna().any().any()\n",
    "...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions (Optional)\n",
    "- Try one additional variant beyond the main path (different features, different split, different model).\n",
    "- Write down what improved, what got worse, and your hypothesis for why.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection\n",
    "- What did you assume implicitly (about timing, availability, stationarity, or costs)?\n",
    "- If you had to ship this model, what would you monitor?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"solutions-reference\"></a>\n",
    "## Solutions (Reference)\n",
    "\n",
    "Try the TODOs first. Use these only to unblock yourself or to compare approaches.\n",
    "\n",
    "<details><summary>Solution: Load macro data</summary>\n",
    "\n",
    "_One possible approach. Your variable names may differ; align them with the notebook._\n",
    "\n",
    "```python\n",
    "# Reference solution for 02_single_factor_regression_macro — Load macro data\n",
    "import pandas as pd\n",
    "df = pd.read_csv(SAMPLE_DIR / 'macro_quarterly_sample.csv', index_col=0, parse_dates=True)\n",
    "df.head()\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<details><summary>Solution: Fit OLS</summary>\n",
    "\n",
    "_One possible approach. Your variable names may differ; align them with the notebook._\n",
    "\n",
    "```python\n",
    "# Reference solution for 02_single_factor_regression_macro — Fit OLS\n",
    "from src import econometrics\n",
    "\n",
    "res = econometrics.fit_ols(df, y_col='gdp_growth_qoq', x_cols=['T10Y2Y'])\n",
    "print(res.summary())\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<details><summary>Solution: Fit HAC</summary>\n",
    "\n",
    "_One possible approach. Your variable names may differ; align them with the notebook._\n",
    "\n",
    "```python\n",
    "# Reference solution for 02_single_factor_regression_macro — Fit HAC\n",
    "res_hac = econometrics.fit_ols_hac(df, y_col='gdp_growth_qoq', x_cols=['T10Y2Y'], maxlags=2)\n",
    "print(res_hac.summary())\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<details><summary>Solution: Interpretation</summary>\n",
    "\n",
    "_One possible approach. Your variable names may differ; align them with the notebook._\n",
    "\n",
    "```python\n",
    "# Reference solution for 02_single_factor_regression_macro — Interpretation\n",
    "# Interpret sign/magnitude carefully; time-series inference is fragile.\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}