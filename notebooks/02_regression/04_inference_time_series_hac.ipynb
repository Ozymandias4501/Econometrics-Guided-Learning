{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 04 Inference for Time Series (HAC/Newey-West)\n",
        "\n",
        "Why naive SE break; HAC SE and interpretation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Table of Contents\n",
        "- [Assumptions](#assumptions)\n",
        "- [Autocorrelation](#autocorrelation)\n",
        "- [HAC SE](#hac-se)\n",
        "- [Checkpoint (Self-Check)](#checkpoint-self-check)\n",
        "- [Solutions (Reference)](#solutions-reference)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Why This Notebook Matters\n",
        "Regression is the bridge between statistics and ML. You will learn:\n",
        "- single-factor vs multi-factor interpretation,\n",
        "- robust standard errors,\n",
        "- coefficient stability and multicollinearity.\n",
        "\n",
        "\n",
        "## Prerequisites (Quick Self-Check)\n",
        "- Completed Parts 00\u201301 (foundations + data).\n",
        "- Basic algebra comfort (reading coefficient tables, units).\n",
        "\n",
        "## What You Will Produce\n",
        "- (no file output; learning/analysis notebook)\n",
        "\n",
        "## Success Criteria\n",
        "- You can explain what you built and why each step exists.\n",
        "- You can run your work end-to-end without undefined variables.\n",
        "\n",
        "## Common Pitfalls\n",
        "- Running cells top-to-bottom without reading the instructions.\n",
        "- Leaving `...` placeholders in code cells.\n",
        "- Treating coefficients as causal without a causal design.\n",
        "- Ignoring multicollinearity (unstable coefficients).\n",
        "\n",
        "## Quick Fixes (When You Get Stuck)\n",
        "- If you see `ModuleNotFoundError`, re-run the bootstrap cell and restart the kernel; make sure `PROJECT_ROOT` is the repo root.\n",
        "- If a `data/processed/*` file is missing, either run the matching build script (see guide) or use the notebook\u2019s `data/sample/*` fallback.\n",
        "- If results look \u201ctoo good,\u201d suspect leakage; re-check shifts, rolling windows, and time splits.\n",
        "- If a model errors, check dtypes (`astype(float)`) and missingness (`dropna()` on required columns).\n",
        "\n",
        "## Matching Guide\n",
        "- `docs/guides/02_regression/04_inference_time_series_hac.md`\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## How To Use This Notebook\n",
        "- Work section-by-section; don\u2019t skip the markdown.\n",
        "- Most code cells are incomplete on purpose: replace TODOs and `...`, then run.\n",
        "- After each section, write 2\u20134 sentences answering the interpretation prompts (what changed, why it matters).\n",
        "- Prefer `data/processed/*` if you have built the real datasets; otherwise use the bundled `data/sample/*` fallbacks.\n",
        "- Use the **Checkpoint (Self-Check)** section to catch mistakes early.\n",
        "- Use **Solutions (Reference)** only to unblock yourself; then re-implement without looking.\n",
        "- Use the matching guide (`docs/guides/02_regression/04_inference_time_series_hac.md`) for the math, assumptions, and deeper context.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"environment-bootstrap\"></a>\n",
        "## Environment Bootstrap\n",
        "Run this cell first. It makes the repo importable and defines common directories.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "\n",
        "def find_repo_root(start: Path) -> Path:\n",
        "    p = start\n",
        "    for _ in range(8):\n",
        "        if (p / 'src').exists() and (p / 'docs').exists():\n",
        "            return p\n",
        "        p = p.parent\n",
        "    raise RuntimeError('Could not find repo root. Start Jupyter from the repo root.')\n",
        "\n",
        "\n",
        "PROJECT_ROOT = find_repo_root(Path.cwd())\n",
        "if str(PROJECT_ROOT) not in sys.path:\n",
        "    sys.path.append(str(PROJECT_ROOT))\n",
        "\n",
        "DATA_DIR = PROJECT_ROOT / 'data'\n",
        "RAW_DIR = DATA_DIR / 'raw'\n",
        "PROCESSED_DIR = DATA_DIR / 'processed'\n",
        "SAMPLE_DIR = DATA_DIR / 'sample'\n",
        "\n",
        "PROJECT_ROOT\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Goal\n",
        "Learn why naive OLS standard errors often break in macro time series, and how HAC/Newey-West helps.\n",
        "\n",
        "Big idea:\n",
        "- coefficients answer \"what best fits the line\"\n",
        "- standard errors answer \"how uncertain are we about the coefficients\"\n",
        "\n",
        "Time series often violate the assumptions behind naive SE.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Primer: `statsmodels` vs `scikit-learn` (inference vs prediction)\n",
        "\n",
        "This repo uses both libraries because they serve different goals:\n",
        "\n",
        "- **Prediction (ML):** optimize out-of-sample accuracy \u2192 `scikit-learn`\n",
        "- **Inference (econometrics):** interpret coefficients + quantify uncertainty \u2192 `statsmodels`\n",
        "\n",
        "### Minimal `statsmodels` OLS pattern\n",
        "\n",
        "```python\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# X: DataFrame of features, y: Series target\n",
        "Xc = sm.add_constant(X, has_constant=\"add\")  # add intercept\n",
        "res = sm.OLS(y, Xc).fit()\n",
        "print(res.summary())\n",
        "```\n",
        "\n",
        "**Expected output / sanity check**\n",
        "- a table with `coef`, `std err`, `t`, `P>|t|`, and a CI column\n",
        "- coefficient names match your column names\n",
        "\n",
        "### What you are looking at in `res.summary()`\n",
        "\n",
        "- **coef**: $\\\\hat\\\\beta$ (estimated effect in the model)\n",
        "- **std err**: estimated uncertainty $\\\\widehat{SE}(\\\\hat\\\\beta)$\n",
        "- **t**: $\\\\hat\\\\beta / \\\\widehat{SE}(\\\\hat\\\\beta)$\n",
        "- **P>|t|**: p-value for $H_0: \\\\beta=0$ (conditional on assumptions)\n",
        "- **[0.025, 0.975]**: 95% confidence interval\n",
        "\n",
        "### Robust standard errors (change uncertainty, not coefficients)\n",
        "\n",
        "```python\n",
        "# Cross-section heteroskedasticity\n",
        "res_hc3 = res.get_robustcov_results(cov_type=\"HC3\")\n",
        "\n",
        "# Time series autocorrelation + heteroskedasticity\n",
        "res_hac = res.get_robustcov_results(cov_type=\"HAC\", cov_kwds={\"maxlags\": 4})\n",
        "```\n",
        "\n",
        "### Common pitfalls (and quick fixes)\n",
        "\n",
        "- **Forgetting the intercept**\n",
        "  - Fix: always `add_constant`.\n",
        "- **Wrong SE for time series**\n",
        "  - Fix: use HAC when residuals are autocorrelated.\n",
        "- **Treating p-values as causal proof**\n",
        "  - Fix: write the identification assumption; otherwise interpret as association.\n",
        "- **Mixing prediction and inference**\n",
        "  - Fix: use `sklearn` pipelines + time splits for prediction; use `statsmodels` for coefficient uncertainty.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Primer: Hypothesis testing (p-values, t-stats, confidence intervals)\n",
        "\n",
        "You will see p-values, t-statistics, and confidence intervals in regression output (especially `statsmodels`). This primer gives you the minimum to interpret them correctly.\n",
        "\n",
        "### The objects (plain language)\n",
        "\n",
        "- **Null hypothesis** $H_0$: the default claim (often \u201cno effect\u201d).\n",
        "- **Alternative** $H_1$: the claim you consider if the data looks inconsistent with $H_0$.\n",
        "- **Test statistic**: \u201chow far\u201d your estimate is from the null, in uncertainty units.\n",
        "- **p-value**: probability (under the null *and model assumptions*) of seeing a test statistic at least as extreme as observed.\n",
        "- **Confidence interval (CI)**: a range of parameter values consistent with the data under assumptions.\n",
        "\n",
        "### What a p-value is NOT\n",
        "\n",
        "- Not the probability $H_0$ is true.\n",
        "- Not the probability the model is correct.\n",
        "- Not a measure of economic importance.\n",
        "\n",
        "### Regression t-test intuition\n",
        "\n",
        "In OLS, a common test is $H_0: \\\\beta_j = 0$.\n",
        "\n",
        "$$\n",
        "t_j = \\\\frac{\\\\hat\\\\beta_j}{\\\\widehat{SE}(\\\\hat\\\\beta_j)}\n",
        "$$\n",
        "\n",
        "If you change your SE estimator (HC3/HAC/cluster), you change $\\\\widehat{SE}$ and therefore the p-value, even if the coefficient stays the same.\n",
        "\n",
        "### Expected output / what you should look at in `res.summary()`\n",
        "\n",
        "- `coef`: effect size (in model units)\n",
        "- `std err`: uncertainty\n",
        "- CI columns: magnitude + uncertainty together\n",
        "\n",
        "### Common pitfalls in this project\n",
        "\n",
        "- Macro time series often have autocorrelation \u2192 naive SE too small \u2192 use HAC when interpreting p-values.\n",
        "- Multiple testing/spec-search can produce small p-values by chance.\n",
        "- Predictive success \u2260 causal interpretation.\n",
        "\n",
        "### Tiny demo (toy; not project data)\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "\n",
        "rng = np.random.default_rng(0)\n",
        "n = 300\n",
        "x = rng.normal(size=n)\n",
        "y = 1.0 + 0.5 * x + rng.normal(scale=1.0, size=n)\n",
        "\n",
        "df = pd.DataFrame({\"y\": y, \"x\": x})\n",
        "X = sm.add_constant(df[[\"x\"]])\n",
        "res = sm.OLS(df[\"y\"], X).fit()\n",
        "print(res.summary())\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"assumptions\"></a>\n",
        "## Assumptions\n",
        "\n",
        "### Goal\n",
        "Fit a baseline regression and inspect residuals.\n",
        "\n",
        "Reminder:\n",
        "- OLS coefficients can be computed even when assumptions fail.\n",
        "- Inference (SE / p-values / CI) is what becomes unreliable.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Your Turn (1): Load macro data and pick a simple regression\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "path = PROCESSED_DIR / 'macro_quarterly.csv'\n",
        "if path.exists():\n",
        "    df = pd.read_csv(path, index_col=0, parse_dates=True)\n",
        "else:\n",
        "    df = pd.read_csv(SAMPLE_DIR / 'macro_quarterly_sample.csv', index_col=0, parse_dates=True)\n",
        "\n",
        "y_col = 'gdp_growth_qoq'\n",
        "x_cols = ['T10Y2Y_lag1']\n",
        "\n",
        "df_m = df[[y_col] + x_cols].dropna().copy()\n",
        "df_m.tail()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Your Turn (2): Fit OLS and compute residuals\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import statsmodels.api as sm\n",
        "\n",
        "X = sm.add_constant(df_m[x_cols], has_constant='add')\n",
        "y = df_m[y_col]\n",
        "res = sm.OLS(y, X).fit()\n",
        "\n",
        "# Residuals = y - y_hat\n",
        "resid = res.resid\n",
        "fitted = res.fittedvalues\n",
        "\n",
        "print(res.summary())\n",
        "resid.head()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Your Turn (3): Residual diagnostics plots (simple but useful)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# TODO: Plot residuals over time\n",
        "# TODO: Plot residuals vs fitted values (heteroskedasticity visual check)\n",
        "...\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"autocorrelation\"></a>\n",
        "## Autocorrelation\n",
        "\n",
        "### Goal\n",
        "Measure whether residuals are correlated over time.\n",
        "\n",
        "If residuals are autocorrelated, naive SE can be too small.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Your Turn (1): Autocorrelation by lag\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# TODO: Compute residual autocorrelation at lags 1..8.\n",
        "# Hint: resid.autocorr(lag=k)\n",
        "ac = {k: float(resid.autocorr(lag=k)) for k in range(1, 9)}\n",
        "ac\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Your Turn (2): A simple ACF plot\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# TODO: Plot the autocorrelation values as a bar chart.\n",
        "...\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"hac-se\"></a>\n",
        "## HAC SE\n",
        "\n",
        "### Goal\n",
        "Fit the same model but compute HAC/Newey-West robust SE.\n",
        "\n",
        "Your task:\n",
        "- compare coefficient (should be the same)\n",
        "- compare SE/p-values (can differ)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Your Turn (1): Compare naive vs HAC with different maxlags\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from src import econometrics\n",
        "\n",
        "res_naive = econometrics.fit_ols(df_m, y_col=y_col, x_cols=x_cols)\n",
        "res_hac1 = econometrics.fit_ols_hac(df_m, y_col=y_col, x_cols=x_cols, maxlags=1)\n",
        "res_hac4 = econometrics.fit_ols_hac(df_m, y_col=y_col, x_cols=x_cols, maxlags=4)\n",
        "\n",
        "# TODO: Print SE/p-values side-by-side for the slope coefficient.\n",
        "...\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Your Turn (2): Write a careful interpretation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# TODO: Write 6-10 sentences answering:\n",
        "# - Did HAC increase or decrease your SE?\n",
        "# - How did that affect your p-value/CI?\n",
        "# - What assumptions are still required even with HAC?\n",
        "notes = \"\"\"\n",
        "...\n",
        "\"\"\"\n",
        "print(notes)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"checkpoint-self-check\"></a>\n",
        "## Checkpoint (Self-Check)\n",
        "Run a few asserts and write 2-3 sentences summarizing what you verified.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# TODO: After you build X/y and split by time, validate the split.\n",
        "# Example (adjust variable names):\n",
        "# assert X_train.index.max() < X_test.index.min()\n",
        "# assert y_train.index.equals(X_train.index)\n",
        "# assert y_test.index.equals(X_test.index)\n",
        "# assert not X_train.isna().any().any()\n",
        "# assert not X_test.isna().any().any()\n",
        "...\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extensions (Optional)\n",
        "- Try one additional variant beyond the main path (different features, different split, different model).\n",
        "- Write down what improved, what got worse, and your hypothesis for why.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reflection\n",
        "- What did you assume implicitly (about timing, availability, stationarity, or costs)?\n",
        "- If you had to ship this model, what would you monitor?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"solutions-reference\"></a>\n",
        "## Solutions (Reference)\n",
        "\n",
        "Try the TODOs first. Use these only to unblock yourself or to compare approaches.\n",
        "\n",
        "<details><summary>Solution: Assumptions</summary>\n",
        "\n",
        "_One possible approach. Your variable names may differ; align them with the notebook._\n",
        "\n",
        "```python\n",
        "# Reference solution for 04_inference_time_series_hac \u2014 Assumptions\n",
        "# OLS assumptions (esp. independent errors) often fail in time series.\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "<details><summary>Solution: Autocorrelation</summary>\n",
        "\n",
        "_One possible approach. Your variable names may differ; align them with the notebook._\n",
        "\n",
        "```python\n",
        "# Reference solution for 04_inference_time_series_hac \u2014 Autocorrelation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = pd.read_csv(SAMPLE_DIR / 'macro_quarterly_sample.csv', index_col=0, parse_dates=True)\n",
        "y = df['gdp_growth_qoq'].to_numpy()\n",
        "print('lag1 autocorr (rough):', np.corrcoef(y[1:], y[:-1])[0,1])\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "<details><summary>Solution: HAC SE</summary>\n",
        "\n",
        "_One possible approach. Your variable names may differ; align them with the notebook._\n",
        "\n",
        "```python\n",
        "# Reference solution for 04_inference_time_series_hac \u2014 HAC SE\n",
        "from src import econometrics\n",
        "\n",
        "res = econometrics.fit_ols(df, y_col='gdp_growth_qoq', x_cols=['T10Y2Y'])\n",
        "res_hac = econometrics.fit_ols_hac(df, y_col='gdp_growth_qoq', x_cols=['T10Y2Y'], maxlags=2)\n",
        "print('naive SE:', res.bse)\n",
        "print('HAC SE  :', res_hac.bse)\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "python3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}