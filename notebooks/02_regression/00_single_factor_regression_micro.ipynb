{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 00 Single-Factor Regression (Micro)\n",
    "\n",
    "Single-factor log-log regression on county data; interpret coefficients.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [Load census data](#load-census-data)\n",
    "- [Build log variables](#build-log-variables)\n",
    "- [Fit OLS + HC3](#fit-ols-hc3)\n",
    "- [Interpretation](#interpretation)\n",
    "- [Checkpoint (Self-Check)](#checkpoint-self-check)\n",
    "- [Solutions (Reference)](#solutions-reference)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why This Notebook Matters\n",
    "Regression is the bridge between statistics and ML. You will learn:\n",
    "- single-factor vs multi-factor interpretation,\n",
    "- robust standard errors,\n",
    "- coefficient stability and multicollinearity.\n",
    "\n",
    "\n",
    "## Prerequisites (Quick Self-Check)\n",
    "- Completed Parts 00–01 (foundations + data).\n",
    "- Basic algebra comfort (reading coefficient tables, units).\n",
    "\n",
    "## What You Will Produce\n",
    "- (no file output; learning/analysis notebook)\n",
    "\n",
    "## Success Criteria\n",
    "- You can explain what you built and why each step exists.\n",
    "- You can run your work end-to-end without undefined variables.\n",
    "\n",
    "## Common Pitfalls\n",
    "- Running cells top-to-bottom without reading the instructions.\n",
    "- Leaving `...` placeholders in code cells.\n",
    "- Treating coefficients as causal without a causal design.\n",
    "- Ignoring multicollinearity (unstable coefficients).\n",
    "\n",
    "## Quick Fixes (When You Get Stuck)\n",
    "- If you see `ModuleNotFoundError`, re-run the bootstrap cell and restart the kernel; make sure `PROJECT_ROOT` is the repo root.\n",
    "- If a `data/processed/*` file is missing, either run the matching build script (see guide) or use the notebook’s `data/sample/*` fallback.\n",
    "- If results look “too good,” suspect leakage; re-check shifts, rolling windows, and time splits.\n",
    "- If a model errors, check dtypes (`astype(float)`) and missingness (`dropna()` on required columns).\n",
    "\n",
    "## Matching Guide\n",
    "- `docs/guides/02_regression/00_single_factor_regression_micro.md`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How To Use This Notebook\n",
    "- Work section-by-section; don’t skip the markdown.\n",
    "- Most code cells are incomplete on purpose: replace TODOs and `...`, then run.\n",
    "- After each section, write 2–4 sentences answering the interpretation prompts (what changed, why it matters).\n",
    "- Prefer `data/processed/*` if you have built the real datasets; otherwise use the bundled `data/sample/*` fallbacks.\n",
    "- Use the **Checkpoint (Self-Check)** section to catch mistakes early.\n",
    "- Use **Solutions (Reference)** only to unblock yourself; then re-implement without looking.\n",
    "- Use the matching guide (`docs/guides/02_regression/00_single_factor_regression_micro.md`) for the math, assumptions, and deeper context.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"environment-bootstrap\"></a>\n",
    "## Environment Bootstrap\n",
    "Run this cell first. It makes the repo importable and defines common directories.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "\n",
    "def find_repo_root(start: Path) -> Path:\n",
    "    p = start\n",
    "    for _ in range(8):\n",
    "        if (p / 'src').exists() and (p / 'docs').exists():\n",
    "            return p\n",
    "        p = p.parent\n",
    "    raise RuntimeError('Could not find repo root. Start Jupyter from the repo root.')\n",
    "\n",
    "\n",
    "PROJECT_ROOT = find_repo_root(Path.cwd())\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "RAW_DIR = DATA_DIR / 'raw'\n",
    "PROCESSED_DIR = DATA_DIR / 'processed'\n",
    "SAMPLE_DIR = DATA_DIR / 'sample'\n",
    "\n",
    "PROJECT_ROOT\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal\n",
    "Fit a single-factor log-log regression on county data and interpret the coefficient like an elasticity.\n",
    "\n",
    "Example question:\n",
    "- \"Across counties, how does rent scale with income?\"\n",
    "\n",
    "This is **not** a causal claim by default. It's a structured description of a relationship.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primer: `statsmodels` vs `scikit-learn` (inference vs prediction)\n",
    "\n",
    "This repo uses both libraries because they serve different goals:\n",
    "\n",
    "- **Prediction (ML):** optimize out-of-sample accuracy → `scikit-learn`\n",
    "- **Inference (econometrics):** interpret coefficients + quantify uncertainty → `statsmodels`\n",
    "\n",
    "### Minimal `statsmodels` OLS pattern\n",
    "\n",
    "```python\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# X: DataFrame of features, y: Series target\n",
    "Xc = sm.add_constant(X, has_constant=\"add\")  # add intercept\n",
    "res = sm.OLS(y, Xc).fit()\n",
    "print(res.summary())\n",
    "```\n",
    "\n",
    "**Expected output / sanity check**\n",
    "- a table with `coef`, `std err`, `t`, `P>|t|`, and a CI column\n",
    "- coefficient names match your column names\n",
    "\n",
    "### What you are looking at in `res.summary()`\n",
    "\n",
    "- **coef**: $\\\\hat\\\\beta$ (estimated effect in the model)\n",
    "- **std err**: estimated uncertainty $\\\\widehat{SE}(\\\\hat\\\\beta)$\n",
    "- **t**: $\\\\hat\\\\beta / \\\\widehat{SE}(\\\\hat\\\\beta)$\n",
    "- **P>|t|**: p-value for $H_0: \\\\beta=0$ (conditional on assumptions)\n",
    "- **[0.025, 0.975]**: 95% confidence interval\n",
    "\n",
    "### Robust standard errors (change uncertainty, not coefficients)\n",
    "\n",
    "```python\n",
    "# Cross-section heteroskedasticity\n",
    "res_hc3 = res.get_robustcov_results(cov_type=\"HC3\")\n",
    "\n",
    "# Time series autocorrelation + heteroskedasticity\n",
    "res_hac = res.get_robustcov_results(cov_type=\"HAC\", cov_kwds={\"maxlags\": 4})\n",
    "```\n",
    "\n",
    "### Common pitfalls (and quick fixes)\n",
    "\n",
    "- **Forgetting the intercept**\n",
    "  - Fix: always `add_constant`.\n",
    "- **Wrong SE for time series**\n",
    "  - Fix: use HAC when residuals are autocorrelated.\n",
    "- **Treating p-values as causal proof**\n",
    "  - Fix: write the identification assumption; otherwise interpret as association.\n",
    "- **Mixing prediction and inference**\n",
    "  - Fix: use `sklearn` pipelines + time splits for prediction; use `statsmodels` for coefficient uncertainty.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primer: Hypothesis testing (p-values, t-stats, confidence intervals)\n",
    "\n",
    "You will see p-values, t-statistics, and confidence intervals in regression output (especially `statsmodels`). This primer gives you the minimum to interpret them correctly.\n",
    "\n",
    "### The objects (plain language)\n",
    "\n",
    "- **Null hypothesis** $H_0$: the default claim (often “no effect”).\n",
    "- **Alternative** $H_1$: the claim you consider if the data looks inconsistent with $H_0$.\n",
    "- **Test statistic**: “how far” your estimate is from the null, in uncertainty units.\n",
    "- **p-value**: probability (under the null *and model assumptions*) of seeing a test statistic at least as extreme as observed.\n",
    "- **Confidence interval (CI)**: a range of parameter values consistent with the data under assumptions.\n",
    "\n",
    "### What a p-value is NOT\n",
    "\n",
    "- Not the probability $H_0$ is true.\n",
    "- Not the probability the model is correct.\n",
    "- Not a measure of economic importance.\n",
    "\n",
    "### Regression t-test intuition\n",
    "\n",
    "In OLS, a common test is $H_0: \\\\beta_j = 0$.\n",
    "\n",
    "$$\n",
    "t_j = \\\\frac{\\\\hat\\\\beta_j}{\\\\widehat{SE}(\\\\hat\\\\beta_j)}\n",
    "$$\n",
    "\n",
    "If you change your SE estimator (HC3/HAC/cluster), you change $\\\\widehat{SE}$ and therefore the p-value, even if the coefficient stays the same.\n",
    "\n",
    "### Expected output / what you should look at in `res.summary()`\n",
    "\n",
    "- `coef`: effect size (in model units)\n",
    "- `std err`: uncertainty\n",
    "- CI columns: magnitude + uncertainty together\n",
    "\n",
    "### Common pitfalls in this project\n",
    "\n",
    "- Macro time series often have autocorrelation → naive SE too small → use HAC when interpreting p-values.\n",
    "- Multiple testing/spec-search can produce small p-values by chance.\n",
    "- Predictive success ≠ causal interpretation.\n",
    "\n",
    "### Tiny demo (toy; not project data)\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "rng = np.random.default_rng(0)\n",
    "n = 300\n",
    "x = rng.normal(size=n)\n",
    "y = 1.0 + 0.5 * x + rng.normal(scale=1.0, size=n)\n",
    "\n",
    "df = pd.DataFrame({\"y\": y, \"x\": x})\n",
    "X = sm.add_constant(df[[\"x\"]])\n",
    "res = sm.OLS(df[\"y\"], X).fit()\n",
    "print(res.summary())\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"load-census-data\"></a>\n",
    "## Load census data\n",
    "\n",
    "### Goal\n",
    "Load a county-level dataset created in the Census notebook.\n",
    "\n",
    "If you haven't run the fetch notebook, use the bundled sample.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Turn (1): Load processed county data (or sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "year = 2022  # TODO: set to the year you fetched\n",
    "path = PROCESSED_DIR / f'census_county_{year}.csv'\n",
    "\n",
    "if path.exists():\n",
    "    df = pd.read_csv(path)\n",
    "else:\n",
    "    df = pd.read_csv(SAMPLE_DIR / 'census_county_sample.csv')\n",
    "\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Turn (2): Inspect schema\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: Inspect columns and dtypes.\n",
    "# Identify which columns represent:\n",
    "# - income\n",
    "# - rent\n",
    "print(df.columns.tolist())\n",
    "print(df.dtypes)\n",
    "...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint (required columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: Confirm the starter variables exist.\n",
    "# If you used different ACS vars, update these names.\n",
    "assert 'B19013_001E' in df.columns  # median household income\n",
    "assert 'B25064_001E' in df.columns  # median gross rent\n",
    "...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"build-log-variables\"></a>\n",
    "## Build log variables\n",
    "\n",
    "### Goal\n",
    "Build a clean modeling table with:\n",
    "- `log_income = log(income)`\n",
    "- `log_rent = log(rent)`\n",
    "\n",
    "Log-log regression is common in economics because it turns multiplicative relationships into additive ones.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Turn (1): Clean and log-transform\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Raw variables\n",
    "income = pd.to_numeric(df['B19013_001E'], errors='coerce')\n",
    "rent = pd.to_numeric(df['B25064_001E'], errors='coerce')\n",
    "\n",
    "# TODO: Drop non-positive values before taking logs\n",
    "mask = (income > 0) & (rent > 0)\n",
    "df_m = pd.DataFrame({\n",
    "    'income': income[mask],\n",
    "    'rent': rent[mask],\n",
    "}).dropna()\n",
    "\n",
    "# TODO: Create log variables\n",
    "df_m['log_income'] = ...\n",
    "df_m['log_rent'] = ...\n",
    "\n",
    "df_m[['log_income', 'log_rent']].head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Turn (2): Visualize the relationship\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# TODO: Make a scatter plot of log_income vs log_rent.\n",
    "# Tip: use alpha=0.2 for dense plots.\n",
    "...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"fit-ols-hc3\"></a>\n",
    "## Fit OLS + HC3\n",
    "\n",
    "### Goal\n",
    "Fit an OLS regression and compute HC3 robust SE (common default for cross-sectional data).\n",
    "\n",
    "Model:\n",
    "$$\\log(rent_i) = \\beta_0 + \\beta_1 \\log(income_i) + \\varepsilon_i$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Turn (1): Fit OLS + HC3\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from src import econometrics\n",
    "\n",
    "# TODO: Fit OLS with HC3 robust SE using the helper.\n",
    "res = econometrics.fit_ols_hc3(df_m, y_col='log_rent', x_cols=['log_income'])\n",
    "print(res.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Turn (2): Extract the slope and interpret it\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: Extract coefficient and CI for log_income.\n",
    "beta = float(res.params['log_income'])\n",
    "ci = res.conf_int().loc['log_income'].tolist()\n",
    "\n",
    "print('beta:', beta)\n",
    "print('95% CI:', ci)\n",
    "\n",
    "# Interpretation prompt:\n",
    "# In a log-log model, beta is approximately an elasticity.\n",
    "# Example: a 1% increase in income is associated with about beta% higher rent.\n",
    "# TODO: Compute the implied change for a 10% income increase.\n",
    "pct_income = 10.0\n",
    "approx_pct_rent = ...\n",
    "approx_pct_rent\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"interpretation\"></a>\n",
    "## Interpretation\n",
    "\n",
    "Write a short interpretation (5-8 sentences) that answers:\n",
    "- What is the estimated relationship?\n",
    "- Is it statistically distinguishable from 0 (given assumptions)?\n",
    "- Is it economically large?\n",
    "- What would have to be true for a causal interpretation?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Turn: Write your interpretation\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: Write your interpretation as a Python multiline string.\n",
    "notes = \"\"\"\n",
    "...\n",
    "\"\"\"\n",
    "print(notes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"checkpoint-self-check\"></a>\n",
    "## Checkpoint (Self-Check)\n",
    "Run a few asserts and write 2-3 sentences summarizing what you verified.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# TODO: After fitting the model, validate the data and results.\n# This is cross-sectional data (no time splits needed).\n# Example checks (adjust variable names):\n# assert df_m.shape[0] > 100, \"Too few observations after filtering\"\n# assert not df_m[['log_income', 'log_rent']].isna().any().any(), \"NaNs remain\"\n# assert 0 < beta < 2, f\"Elasticity {beta} outside plausible range\"\n# assert ci[0] < ci[1], \"CI bounds are inverted\"\n..."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions (Optional)\n",
    "- Try one additional variant beyond the main path (different features, different split, different model).\n",
    "- Write down what improved, what got worse, and your hypothesis for why.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection\n",
    "- What did you assume implicitly (about timing, availability, stationarity, or costs)?\n",
    "- If you had to ship this model, what would you monitor?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"solutions-reference\"></a>\n",
    "## Solutions (Reference)\n",
    "\n",
    "Try the TODOs first. Use these only to unblock yourself or to compare approaches.\n",
    "\n",
    "<details><summary>Solution: Load census data</summary>\n",
    "\n",
    "_One possible approach. Your variable names may differ; align them with the notebook._\n",
    "\n",
    "```python\n",
    "# Reference solution for 00_single_factor_regression_micro — Load census data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(SAMPLE_DIR / 'census_county_sample.csv')\n",
    "df.head()\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<details><summary>Solution: Build log variables</summary>\n",
    "\n",
    "_One possible approach. Your variable names may differ; align them with the notebook._\n",
    "\n",
    "```python\n",
    "# Reference solution for 00_single_factor_regression_micro — Build log variables\n",
    "df = df.copy()\n",
    "df = df[(df['B19013_001E'] > 0) & (df['B25064_001E'] > 0)].copy()\n",
    "df['log_income'] = np.log(df['B19013_001E'].astype(float))\n",
    "df['log_rent'] = np.log(df['B25064_001E'].astype(float))\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<details><summary>Solution: Fit OLS + HC3</summary>\n",
    "\n",
    "_One possible approach. Your variable names may differ; align them with the notebook._\n",
    "\n",
    "```python\n",
    "# Reference solution for 00_single_factor_regression_micro — Fit OLS + HC3\n",
    "from src import econometrics\n",
    "\n",
    "res = econometrics.fit_ols_hc3(df, y_col='log_rent', x_cols=['log_income'])\n",
    "print(res.summary())\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<details><summary>Solution: Interpretation</summary>\n",
    "\n",
    "_One possible approach. Your variable names may differ; align them with the notebook._\n",
    "\n",
    "```python\n",
    "# Reference solution for 00_single_factor_regression_micro — Interpretation\n",
    "# In a log-log model, the slope is an elasticity-style interpretation:\n",
    "# a 1% increase in income is associated with ~beta% increase in rent (under assumptions).\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}