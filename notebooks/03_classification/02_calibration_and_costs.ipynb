{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 02 Calibration and Decision Costs\n",
        "\n",
        "Calibration curves, Brier score, and cost-based thresholds.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Table of Contents\n",
        "- [Calibration](#calibration)\n",
        "- [Brier score](#brier-score)\n",
        "- [Decision costs](#decision-costs)\n",
        "- [Checkpoint (Self-Check)](#checkpoint-self-check)\n",
        "- [Solutions (Reference)](#solutions-reference)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Why This Notebook Matters\n",
        "Classification notebooks turn the recession label into a **probability model**.\n",
        "You will learn how to evaluate rare-event prediction and how to choose thresholds intentionally.\n",
        "\n",
        "\n",
        "## Prerequisites (Quick Self-Check)\n",
        "- Completed Part 02 (regression basics) or equivalent.\n",
        "- Comfort interpreting probabilities and trade-offs (false positives vs false negatives).\n",
        "\n",
        "## What You Will Produce\n",
        "- (no file output; learning/analysis notebook)\n",
        "\n",
        "## Success Criteria\n",
        "- You can explain what you built and why each step exists.\n",
        "- You can run your work end-to-end without undefined variables.\n",
        "\n",
        "## Common Pitfalls\n",
        "- Running cells top-to-bottom without reading the instructions.\n",
        "- Leaving `...` placeholders in code cells.\n",
        "- Reporting only accuracy on imbalanced data.\n",
        "- Using threshold=0.5 by default without considering costs.\n",
        "\n",
        "## Quick Fixes (When You Get Stuck)\n",
        "- If you see `ModuleNotFoundError`, re-run the bootstrap cell and restart the kernel; make sure `PROJECT_ROOT` is the repo root.\n",
        "- If a `data/processed/*` file is missing, either run the matching build script (see guide) or use the notebook\u2019s `data/sample/*` fallback.\n",
        "- If results look \u201ctoo good,\u201d suspect leakage; re-check shifts, rolling windows, and time splits.\n",
        "- If a model errors, check dtypes (`astype(float)`) and missingness (`dropna()` on required columns).\n",
        "\n",
        "## Matching Guide\n",
        "- `docs/guides/03_classification/02_calibration_and_costs.md`\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## How To Use This Notebook\n",
        "- Work section-by-section; don\u2019t skip the markdown.\n",
        "- Most code cells are incomplete on purpose: replace TODOs and `...`, then run.\n",
        "- After each section, write 2\u20134 sentences answering the interpretation prompts (what changed, why it matters).\n",
        "- Prefer `data/processed/*` if you have built the real datasets; otherwise use the bundled `data/sample/*` fallbacks.\n",
        "- Use the **Checkpoint (Self-Check)** section to catch mistakes early.\n",
        "- Use **Solutions (Reference)** only to unblock yourself; then re-implement without looking.\n",
        "- Use the matching guide (`docs/guides/03_classification/02_calibration_and_costs.md`) for the math, assumptions, and deeper context.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"environment-bootstrap\"></a>\n",
        "## Environment Bootstrap\n",
        "Run this cell first. It makes the repo importable and defines common directories.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "\n",
        "def find_repo_root(start: Path) -> Path:\n",
        "    p = start\n",
        "    for _ in range(8):\n",
        "        if (p / 'src').exists() and (p / 'docs').exists():\n",
        "            return p\n",
        "        p = p.parent\n",
        "    raise RuntimeError('Could not find repo root. Start Jupyter from the repo root.')\n",
        "\n",
        "\n",
        "PROJECT_ROOT = find_repo_root(Path.cwd())\n",
        "if str(PROJECT_ROOT) not in sys.path:\n",
        "    sys.path.append(str(PROJECT_ROOT))\n",
        "\n",
        "DATA_DIR = PROJECT_ROOT / 'data'\n",
        "RAW_DIR = DATA_DIR / 'raw'\n",
        "PROCESSED_DIR = DATA_DIR / 'processed'\n",
        "SAMPLE_DIR = DATA_DIR / 'sample'\n",
        "\n",
        "PROJECT_ROOT\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Goal\n",
        "Go beyond accuracy: evaluate probability quality (calibration) and choose thresholds based on decision costs.\n",
        "\n",
        "Why this matters:\n",
        "- A recession probability model is only useful if probabilities mean something.\n",
        "- Threshold selection depends on the cost of false positives vs false negatives.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Primer: sklearn pipelines (how to avoid preprocessing leakage)\n",
        "\n",
        "Pipelines prevent a common mistake: fitting preprocessing (scaling, imputation) using information from the test set.\n",
        "\n",
        "### Why pipelines exist (in one sentence)\n",
        "\n",
        "> A `Pipeline` ensures that transformations are fit on training data only, then applied to test data.\n",
        "\n",
        "### The key APIs\n",
        "\n",
        "- `fit(X, y)`: learn parameters (scaler mean/std, model weights) from training.\n",
        "- `transform(X)`: apply learned transform to new data.\n",
        "- `fit_transform(X, y)`: convenience for training data only.\n",
        "\n",
        "### Minimal pattern (classification)\n",
        "\n",
        "```python\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "clf = Pipeline([\n",
        "  (\"scaler\", StandardScaler()),\n",
        "  (\"model\", LogisticRegression(max_iter=5000)),\n",
        "])\n",
        "\n",
        "# clf.fit(X_train, y_train)\n",
        "# y_prob = clf.predict_proba(X_test)[:, 1]\n",
        "```\n",
        "\n",
        "**Expected output / sanity check**\n",
        "- you never call `scaler.fit` on the full dataset\n",
        "- you split by time first, then fit the pipeline on train\n",
        "\n",
        "### Mini demo: the leakage you\u2019re avoiding (toy)\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "rng = np.random.default_rng(0)\n",
        "X_train = rng.normal(loc=0.0, scale=1.0, size=(100, 1))\n",
        "X_test  = rng.normal(loc=2.0, scale=1.0, size=(25, 1))\n",
        "\n",
        "# WRONG: fit on train+test (leaks the future)\n",
        "sc_wrong = StandardScaler().fit(np.vstack([X_train, X_test]))\n",
        "X_test_wrong = sc_wrong.transform(X_test)\n",
        "\n",
        "# RIGHT: fit on train only\n",
        "sc_right = StandardScaler().fit(X_train)\n",
        "X_test_right = sc_right.transform(X_test)\n",
        "\n",
        "print(\"test mean after wrong scaling:\", float(X_test_wrong.mean()))\n",
        "print(\"test mean after right scaling:\", float(X_test_right.mean()))\n",
        "```\n",
        "\n",
        "### Common pitfalls\n",
        "\n",
        "- Splitting after preprocessing (leakage).\n",
        "- Using random splits on time-indexed data (temporal leakage).\n",
        "- Forgetting `ColumnTransformer` for mixed numeric/categorical columns (if needed).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"calibration\"></a>\n",
        "## Calibration\n",
        "\n",
        "### Goal\n",
        "Check whether predicted probabilities match observed frequencies.\n",
        "\n",
        "A calibrated model:\n",
        "- among events predicted at 30%, about 30% should occur (in the long run)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Your Turn (1): Fit a base classifier and get probabilities\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from src.evaluation import time_train_test_split_index\n",
        "\n",
        "# Load data\n",
        "path = PROCESSED_DIR / 'macro_quarterly.csv'\n",
        "if path.exists():\n",
        "    df = pd.read_csv(path, index_col=0, parse_dates=True)\n",
        "else:\n",
        "    df = pd.read_csv(SAMPLE_DIR / 'macro_quarterly_sample.csv', index_col=0, parse_dates=True)\n",
        "\n",
        "y_col = 'target_recession_next_q'\n",
        "x_cols = ['T10Y2Y_lag1', 'UNRATE_lag1', 'FEDFUNDS_lag1', 'INDPRO_lag1']\n",
        "df_m = df[[y_col] + x_cols].dropna().copy()\n",
        "\n",
        "split = time_train_test_split_index(len(df_m), test_size=0.2)\n",
        "train = df_m.iloc[split.train_slice]\n",
        "test = df_m.iloc[split.test_slice]\n",
        "\n",
        "X_train = train[x_cols]\n",
        "y_train = train[y_col].astype(int)\n",
        "X_test = test[x_cols]\n",
        "y_test = test[y_col].astype(int)\n",
        "\n",
        "clf = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('model', LogisticRegression(max_iter=5000)),\n",
        "])\n",
        "clf.fit(X_train, y_train)\n",
        "p_test = clf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "p_test[:5]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Your Turn (2): Calibration curve and reliability plot\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.calibration import calibration_curve\n",
        "\n",
        "# TODO: Compute calibration curve\n",
        "# Hint: calibration_curve(y_true, y_prob, n_bins=...)\n",
        "prob_true, prob_pred = ...\n",
        "\n",
        "# TODO: Plot prob_pred vs prob_true with a y=x reference line\n",
        "...\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"brier-score\"></a>\n",
        "## Brier score\n",
        "\n",
        "### Goal\n",
        "Compute the Brier score (mean squared error of probabilities).\n",
        "\n",
        "Lower is better.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Your Turn: Compute Brier score and interpret\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from sklearn.metrics import brier_score_loss\n",
        "\n",
        "brier = brier_score_loss(y_test.to_numpy(), p_test)\n",
        "brier\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"decision-costs\"></a>\n",
        "## Decision costs\n",
        "\n",
        "### Goal\n",
        "Choose a probability threshold using a simple cost model.\n",
        "\n",
        "Example framing:\n",
        "- False negative (miss a recession) might be more costly than a false positive.\n",
        "- You will encode that as a cost ratio and pick the threshold that minimizes expected cost.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Your Turn (1): Define costs and compute expected cost across thresholds\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Cost of false positives vs false negatives\n",
        "cost_fp = 1.0\n",
        "cost_fn = 5.0\n",
        "\n",
        "thresholds = np.linspace(0.05, 0.95, 19)\n",
        "costs = []\n",
        "for thr in thresholds:\n",
        "    y_pred = (p_test >= thr).astype(int)\n",
        "    fp = ((y_pred == 1) & (y_test.to_numpy() == 0)).sum()\n",
        "    fn = ((y_pred == 0) & (y_test.to_numpy() == 1)).sum()\n",
        "    costs.append(cost_fp * fp + cost_fn * fn)\n",
        "\n",
        "best_thr = float(thresholds[int(np.argmin(costs))])\n",
        "best_thr\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Your Turn (2): Plot cost vs threshold\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# TODO: Plot thresholds vs costs and mark the best_thr.\n",
        "...\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"checkpoint-self-check\"></a>\n",
        "## Checkpoint (Self-Check)\n",
        "Run a few asserts and write 2-3 sentences summarizing what you verified.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# TODO: After you build X/y and split by time, validate the split.\n",
        "# Example (adjust variable names):\n",
        "# assert X_train.index.max() < X_test.index.min()\n",
        "# assert y_train.index.equals(X_train.index)\n",
        "# assert y_test.index.equals(X_test.index)\n",
        "# assert not X_train.isna().any().any()\n",
        "# assert not X_test.isna().any().any()\n",
        "...\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extensions (Optional)\n",
        "- Try one additional variant beyond the main path (different features, different split, different model).\n",
        "- Write down what improved, what got worse, and your hypothesis for why.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reflection\n",
        "- What did you assume implicitly (about timing, availability, stationarity, or costs)?\n",
        "- If you had to ship this model, what would you monitor?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"solutions-reference\"></a>\n",
        "## Solutions (Reference)\n",
        "\n",
        "Try the TODOs first. Use these only to unblock yourself or to compare approaches.\n",
        "\n",
        "<details><summary>Solution: Calibration</summary>\n",
        "\n",
        "_One possible approach. Your variable names may differ; align them with the notebook._\n",
        "\n",
        "```python\n",
        "# Reference solution for 02_calibration_and_costs \u2014 Calibration\n",
        "from sklearn.calibration import calibration_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assume y_te and p from prior notebook\n",
        "prob_true, prob_pred = calibration_curve(y_te, p, n_bins=6)\n",
        "plt.plot(prob_pred, prob_true, marker='o')\n",
        "plt.plot([0,1],[0,1], linestyle='--', color='gray')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Observed')\n",
        "plt.title('Calibration curve')\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "<details><summary>Solution: Brier score</summary>\n",
        "\n",
        "_One possible approach. Your variable names may differ; align them with the notebook._\n",
        "\n",
        "```python\n",
        "# Reference solution for 02_calibration_and_costs \u2014 Brier score\n",
        "from sklearn.metrics import brier_score_loss\n",
        "brier_score_loss(y_te, p)\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "<details><summary>Solution: Decision costs</summary>\n",
        "\n",
        "_One possible approach. Your variable names may differ; align them with the notebook._\n",
        "\n",
        "```python\n",
        "# Reference solution for 02_calibration_and_costs \u2014 Decision costs\n",
        "import numpy as np\n",
        "\n",
        "# Simple cost model: cost_fp and cost_fn\n",
        "cost_fp = 1.0\n",
        "cost_fn = 5.0\n",
        "\n",
        "def expected_cost(thr):\n",
        "    pred = (p >= thr).astype(int)\n",
        "    fp = ((pred == 1) & (y_te.to_numpy() == 0)).sum()\n",
        "    fn = ((pred == 0) & (y_te.to_numpy() == 1)).sum()\n",
        "    return cost_fp*fp + cost_fn*fn\n",
        "\n",
        "candidates = np.linspace(0.05, 0.95, 19)\n",
        "best = min([(thr, expected_cost(thr)) for thr in candidates], key=lambda t: t[1])\n",
        "best\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "python3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}