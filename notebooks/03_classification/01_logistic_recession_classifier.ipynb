{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 Logistic Recession Classifier\n",
    "\n",
    "Logistic regression, ROC/PR, and threshold tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [Train/test split](#train-test-split)\n",
    "- [Fit logistic](#fit-logistic)\n",
    "- [ROC/PR](#roc-pr)\n",
    "- [Threshold tuning](#threshold-tuning)\n",
    "- [Checkpoint (Self-Check)](#checkpoint-self-check)\n",
    "- [Solutions (Reference)](#solutions-reference)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why This Notebook Matters\n",
    "Classification notebooks turn the recession label into a **probability model**.\n",
    "You will learn how to evaluate rare-event prediction and how to choose thresholds intentionally.\n",
    "\n",
    "\n",
    "## Prerequisites (Quick Self-Check)\n",
    "- Completed Part 02 (regression basics) or equivalent.\n",
    "- Comfort interpreting probabilities and trade-offs (false positives vs false negatives).\n",
    "\n",
    "## What You Will Produce\n",
    "- (no file output; learning/analysis notebook)\n",
    "\n",
    "## Success Criteria\n",
    "- You can explain what you built and why each step exists.\n",
    "- You can run your work end-to-end without undefined variables.\n",
    "\n",
    "## Common Pitfalls\n",
    "- Running cells top-to-bottom without reading the instructions.\n",
    "- Leaving `...` placeholders in code cells.\n",
    "- Reporting only accuracy on imbalanced data.\n",
    "- Using threshold=0.5 by default without considering costs.\n",
    "\n",
    "## Quick Fixes (When You Get Stuck)\n",
    "- If you see `ModuleNotFoundError`, re-run the bootstrap cell and restart the kernel; make sure `PROJECT_ROOT` is the repo root.\n",
    "- If a `data/processed/*` file is missing, either run the matching build script (see guide) or use the notebook’s `data/sample/*` fallback.\n",
    "- If results look “too good,” suspect leakage; re-check shifts, rolling windows, and time splits.\n",
    "- If a model errors, check dtypes (`astype(float)`) and missingness (`dropna()` on required columns).\n",
    "\n",
    "## Matching Guide\n",
    "- `docs/guides/03_classification/01_logistic_recession_classifier.md`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How To Use This Notebook\n",
    "- Work section-by-section; don’t skip the markdown.\n",
    "- Most code cells are incomplete on purpose: replace TODOs and `...`, then run.\n",
    "- After each section, write 2–4 sentences answering the interpretation prompts (what changed, why it matters).\n",
    "- Prefer `data/processed/*` if you have built the real datasets; otherwise use the bundled `data/sample/*` fallbacks.\n",
    "- Use the **Checkpoint (Self-Check)** section to catch mistakes early.\n",
    "- Use **Solutions (Reference)** only to unblock yourself; then re-implement without looking.\n",
    "- Use the matching guide (`docs/guides/03_classification/01_logistic_recession_classifier.md`) for the math, assumptions, and deeper context.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"environment-bootstrap\"></a>\n",
    "## Environment Bootstrap\n",
    "Run this cell first. It makes the repo importable and defines common directories.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "\n",
    "def find_repo_root(start: Path) -> Path:\n",
    "    p = start\n",
    "    for _ in range(8):\n",
    "        if (p / 'src').exists() and (p / 'docs').exists():\n",
    "            return p\n",
    "        p = p.parent\n",
    "    raise RuntimeError('Could not find repo root. Start Jupyter from the repo root.')\n",
    "\n",
    "\n",
    "PROJECT_ROOT = find_repo_root(Path.cwd())\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "RAW_DIR = DATA_DIR / 'raw'\n",
    "PROCESSED_DIR = DATA_DIR / 'processed'\n",
    "SAMPLE_DIR = DATA_DIR / 'sample'\n",
    "\n",
    "PROJECT_ROOT\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal\n",
    "Train a logistic regression classifier for next-quarter technical recession.\n",
    "\n",
    "You will learn:\n",
    "- how to do a time-based split\n",
    "- how to fit a probabilistic classifier (outputs probabilities)\n",
    "- how to interpret coefficients as log-odds / odds ratios\n",
    "- why threshold selection is a decision problem, not a default\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primer: sklearn pipelines (how to avoid preprocessing leakage)\n",
    "\n",
    "Pipelines prevent a common mistake: fitting preprocessing (scaling, imputation) using information from the test set.\n",
    "\n",
    "### Why pipelines exist (in one sentence)\n",
    "\n",
    "> A `Pipeline` ensures that transformations are fit on training data only, then applied to test data.\n",
    "\n",
    "### The key APIs\n",
    "\n",
    "- `fit(X, y)`: learn parameters (scaler mean/std, model weights) from training.\n",
    "- `transform(X)`: apply learned transform to new data.\n",
    "- `fit_transform(X, y)`: convenience for training data only.\n",
    "\n",
    "### Minimal pattern (classification)\n",
    "\n",
    "```python\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = Pipeline([\n",
    "  (\"scaler\", StandardScaler()),\n",
    "  (\"model\", LogisticRegression(max_iter=5000)),\n",
    "])\n",
    "\n",
    "# clf.fit(X_train, y_train)\n",
    "# y_prob = clf.predict_proba(X_test)[:, 1]\n",
    "```\n",
    "\n",
    "**Expected output / sanity check**\n",
    "- you never call `scaler.fit` on the full dataset\n",
    "- you split by time first, then fit the pipeline on train\n",
    "\n",
    "### Mini demo: the leakage you’re avoiding (toy)\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "rng = np.random.default_rng(0)\n",
    "X_train = rng.normal(loc=0.0, scale=1.0, size=(100, 1))\n",
    "X_test  = rng.normal(loc=2.0, scale=1.0, size=(25, 1))\n",
    "\n",
    "# WRONG: fit on train+test (leaks the future)\n",
    "sc_wrong = StandardScaler().fit(np.vstack([X_train, X_test]))\n",
    "X_test_wrong = sc_wrong.transform(X_test)\n",
    "\n",
    "# RIGHT: fit on train only\n",
    "sc_right = StandardScaler().fit(X_train)\n",
    "X_test_right = sc_right.transform(X_test)\n",
    "\n",
    "print(\"test mean after wrong scaling:\", float(X_test_wrong.mean()))\n",
    "print(\"test mean after right scaling:\", float(X_test_right.mean()))\n",
    "```\n",
    "\n",
    "### Common pitfalls\n",
    "\n",
    "- Splitting after preprocessing (leakage).\n",
    "- Using random splits on time-indexed data (temporal leakage).\n",
    "- Forgetting `ColumnTransformer` for mixed numeric/categorical columns (if needed).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"train-test-split\"></a>\n",
    "## Train/test split\n",
    "\n",
    "### Goal\n",
    "Split chronologically so the model trains on the past and is evaluated on the future.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Turn (1): Load data and select columns\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path = PROCESSED_DIR / 'macro_quarterly.csv'\n",
    "if path.exists():\n",
    "    df = pd.read_csv(path, index_col=0, parse_dates=True)\n",
    "else:\n",
    "    df = pd.read_csv(SAMPLE_DIR / 'macro_quarterly_sample.csv', index_col=0, parse_dates=True)\n",
    "\n",
    "y_col = 'target_recession_next_q'\n",
    "x_cols = ['T10Y2Y_lag1', 'UNRATE_lag1', 'FEDFUNDS_lag1', 'INDPRO_lag1']\n",
    "\n",
    "df_m = df[[y_col] + x_cols].dropna().copy()\n",
    "df_m.tail()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Turn (2): Create a time split\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from src.evaluation import time_train_test_split_index\n",
    "\n",
    "split = time_train_test_split_index(len(df_m), test_size=0.2)\n",
    "train = df_m.iloc[split.train_slice]\n",
    "test = df_m.iloc[split.test_slice]\n",
    "\n",
    "X_train = train[x_cols]\n",
    "y_train = train[y_col].astype(int)\n",
    "X_test = test[x_cols]\n",
    "y_test = test[y_col].astype(int)\n",
    "\n",
    "train.index.max(), test.index.min()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"fit-logistic\"></a>\n",
    "## Fit logistic\n",
    "\n",
    "### Goal\n",
    "Fit a logistic regression model inside a Pipeline (to avoid preprocessing leakage).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Turn (1): Fit the pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', LogisticRegression(max_iter=5000)),\n",
    "])\n",
    "\n",
    "# TODO: Fit on training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predicted probabilities for class 1\n",
    "p_test = clf.predict_proba(X_test)[:, 1]\n",
    "p_test[:5]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Turn (2): Evaluate metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from src.evaluation import classification_metrics\n",
    "\n",
    "classification_metrics(y_test.to_numpy(), p_test, threshold=0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Turn (3): Interpret coefficients (odds ratios)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Coefficients live in the underlying model\n",
    "coefs = clf.named_steps['model'].coef_[0]\n",
    "\n",
    "# TODO: Build a coefficient table.\n",
    "coef_df = pd.DataFrame({'feature': x_cols, 'coef': coefs})\n",
    "\n",
    "# Odds ratio for a 1-unit increase in standardized feature:\n",
    "# OR = exp(coef)\n",
    "coef_df['odds_ratio'] = np.exp(coef_df['coef'])\n",
    "coef_df.sort_values('coef')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "puiwy3kcti",
   "source": "### Logistic vs Probit and marginal effects\n\n**Logistic vs Probit**: Both model binary outcomes. Logistic regression uses the logistic (sigmoid) link function; probit uses the normal CDF. In practice they give very similar predictions. Logistic is more common in ML (odds ratio interpretation); probit is traditional in econometrics. Both are available in `statsmodels`:\n\n```python\nimport statsmodels.api as sm\n# Logistic\nsm.Logit(y, X).fit()\n# Probit\nsm.Probit(y, X).fit()\n```\n\n**Marginal effects**: The logistic coefficient $\\beta_j$ is a change in *log-odds*, not a change in probability. The actual effect on $P(y=1)$ depends on where you are on the sigmoid curve. To get interpretable probability changes, compute **average marginal effects (AME)**:\n\n```python\nres_logit = sm.Logit(y_train, sm.add_constant(X_train)).fit()\nmfx = res_logit.get_margeff(at='overall')\nprint(mfx.summary())\n```\n\nAME tells you: \"On average across the sample, a one-unit increase in $x_j$ is associated with a ___ percentage point change in $P(y=1)$.\" This is more interpretable than the raw coefficient for decision-making.\n\n### Your Turn (optional): Compute marginal effects",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "ny6gienidlb",
   "source": "# TODO (optional): Fit a statsmodels Logit and compute average marginal effects.\n# Compare the AME to the sklearn odds ratios above.\n# import statsmodels.api as sm\n# X_tr_c = sm.add_constant(X_train.astype(float))\n# res_logit = sm.Logit(y_train.astype(float), X_tr_c).fit(disp=0)\n# mfx = res_logit.get_margeff(at='overall')\n# print(mfx.summary())\n...",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"roc-pr\"></a>\n",
    "## ROC/PR\n",
    "\n",
    "### Goal\n",
    "Plot ROC and precision-recall curves.\n",
    "\n",
    "Why both?\n",
    "- ROC can look optimistic under heavy class imbalance\n",
    "- PR focuses on the positive class (recessions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Turn: Plot ROC and PR curves\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import RocCurveDisplay, PrecisionRecallDisplay\n",
    "\n",
    "# TODO: Create ROC and PR plots.\n",
    "...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"threshold-tuning\"></a>\n",
    "## Threshold tuning\n",
    "\n",
    "### Goal\n",
    "Compare metrics at different probability thresholds.\n",
    "\n",
    "A lower threshold catches more recessions (higher recall) but raises false positives.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Turn: Evaluate multiple thresholds\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from src.evaluation import classification_metrics\n",
    "\n",
    "for thr in [0.3, 0.5, 0.7]:\n",
    "    m = classification_metrics(y_test.to_numpy(), p_test, threshold=thr)\n",
    "    print(thr, m)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"checkpoint-self-check\"></a>\n",
    "## Checkpoint (Self-Check)\n",
    "Run a few asserts and write 2-3 sentences summarizing what you verified.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: After you build X/y and split by time, validate the split.\n",
    "# Example (adjust variable names):\n",
    "# assert X_train.index.max() < X_test.index.min()\n",
    "# assert y_train.index.equals(X_train.index)\n",
    "# assert y_test.index.equals(X_test.index)\n",
    "# assert not X_train.isna().any().any()\n",
    "# assert not X_test.isna().any().any()\n",
    "...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions (Optional)\n",
    "- Try one additional variant beyond the main path (different features, different split, different model).\n",
    "- Write down what improved, what got worse, and your hypothesis for why.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection\n",
    "- What did you assume implicitly (about timing, availability, stationarity, or costs)?\n",
    "- If you had to ship this model, what would you monitor?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"solutions-reference\"></a>\n",
    "## Solutions (Reference)\n",
    "\n",
    "Try the TODOs first. Use these only to unblock yourself or to compare approaches.\n",
    "\n",
    "<details><summary>Solution: Train/test split</summary>\n",
    "\n",
    "_One possible approach. Your variable names may differ; align them with the notebook._\n",
    "\n",
    "```python\n",
    "# Reference solution for 01_logistic_recession_classifier — Train/test split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(SAMPLE_DIR / 'macro_quarterly_sample.csv', index_col=0, parse_dates=True).dropna()\n",
    "target = 'target_recession_next_q'\n",
    "drop_cols = {target, 'recession', 'GDPC1', 'gdp_growth_qoq', 'gdp_growth_qoq_annualized', 'gdp_growth_yoy'}\n",
    "X = df[[c for c in df.columns if c not in drop_cols]].astype(float)\n",
    "y = df[target].astype(int)\n",
    "split = int(len(df)*0.8)\n",
    "X_tr, X_te = X.iloc[:split], X.iloc[split:]\n",
    "y_tr, y_te = y.iloc[:split], y.iloc[split:]\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<details><summary>Solution: Fit logistic</summary>\n",
    "\n",
    "_One possible approach. Your variable names may differ; align them with the notebook._\n",
    "\n",
    "```python\n",
    "# Reference solution for 01_logistic_recession_classifier — Fit logistic\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from src import evaluation\n",
    "\n",
    "clf = Pipeline([('scaler', StandardScaler()), ('m', LogisticRegression(max_iter=5000))])\n",
    "clf.fit(X_tr, y_tr)\n",
    "p = clf.predict_proba(X_te)[:,1]\n",
    "evaluation.classification_metrics(y_te.to_numpy(), p)\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<details><summary>Solution: ROC/PR</summary>\n",
    "\n",
    "_One possible approach. Your variable names may differ; align them with the notebook._\n",
    "\n",
    "```python\n",
    "# Reference solution for 01_logistic_recession_classifier — ROC/PR\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve\n",
    "from src import plots\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_te, p)\n",
    "prec, rec, _ = precision_recall_curve(y_te, p)\n",
    "plots.plot_roc_curve(fpr, tpr)\n",
    "plots.plot_pr_curve(rec, prec)\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<details><summary>Solution: Threshold tuning</summary>\n",
    "\n",
    "_One possible approach. Your variable names may differ; align them with the notebook._\n",
    "\n",
    "```python\n",
    "# Reference solution for 01_logistic_recession_classifier — Threshold tuning\n",
    "from src import evaluation\n",
    "for thr in [0.3, 0.5, 0.7]:\n",
    "    print(thr, evaluation.classification_metrics(y_te.to_numpy(), p, threshold=thr))\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}