{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 Stats Basics for ML\n",
    "\n",
    "Correlation, collinearity, bias/variance, and overfitting basics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [Correlation vs causation](#correlation-vs-causation)\n",
    "- [Multicollinearity (VIF)](#multicollinearity-vif)\n",
    "- [Bias/variance](#bias-variance)\n",
    "- [Hypothesis testing](#hypothesis-testing)\n",
    "- [Checkpoint (Self-Check)](#checkpoint-self-check)\n",
    "- [Solutions (Reference)](#solutions-reference)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why This Notebook Matters\n",
    "Foundations notebooks build the intuition that prevents the most common mistakes in economic ML:\n",
    "- leaking future information,\n",
    "- evaluating with the wrong split strategy,\n",
    "- over-interpreting coefficients.\n",
    "\n",
    "\n",
    "## Prerequisites (Quick Self-Check)\n",
    "- Comfort with basic Python + pandas (reading CSVs, making plots).\n",
    "- Willingness to write short interpretation notes as you go.\n",
    "\n",
    "## What You Will Produce\n",
    "- (no file output; learning/analysis notebook)\n",
    "\n",
    "## Success Criteria\n",
    "- You can explain what you built and why each step exists.\n",
    "- You can run your work end-to-end without undefined variables.\n",
    "\n",
    "## Common Pitfalls\n",
    "- Running cells top-to-bottom without reading the instructions.\n",
    "- Leaving `...` placeholders in code cells.\n",
    "- Using random splits on time series.\n",
    "- Assuming correlation implies causation.\n",
    "\n",
    "## Quick Fixes (When You Get Stuck)\n",
    "- If you see `ModuleNotFoundError`, re-run the bootstrap cell and restart the kernel; make sure `PROJECT_ROOT` is the repo root.\n",
    "- If a `data/processed/*` file is missing, either run the matching build script (see guide) or use the notebook’s `data/sample/*` fallback.\n",
    "- If results look “too good,” suspect leakage; re-check shifts, rolling windows, and time splits.\n",
    "- If a model errors, check dtypes (`astype(float)`) and missingness (`dropna()` on required columns).\n",
    "\n",
    "## Matching Guide\n",
    "- `docs/guides/00b_foundations/02_stats_basics_for_ml.md`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How To Use This Notebook\n",
    "- Work section-by-section; don’t skip the markdown.\n",
    "- Most code cells are incomplete on purpose: replace TODOs and `...`, then run.\n",
    "- After each section, write 2–4 sentences answering the interpretation prompts (what changed, why it matters).\n",
    "- Prefer `data/processed/*` if you have built the real datasets; otherwise use the bundled `data/sample/*` fallbacks.\n",
    "- Use the **Checkpoint (Self-Check)** section to catch mistakes early.\n",
    "- Use **Solutions (Reference)** only to unblock yourself; then re-implement without looking.\n",
    "- Use the matching guide (`docs/guides/00b_foundations/02_stats_basics_for_ml.md`) for the math, assumptions, and deeper context.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"environment-bootstrap\"></a>\n",
    "## Environment Bootstrap\n",
    "Run this cell first. It makes the repo importable and defines common directories.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "\n",
    "def find_repo_root(start: Path) -> Path:\n",
    "    p = start\n",
    "    for _ in range(8):\n",
    "        if (p / 'src').exists() and (p / 'docs').exists():\n",
    "            return p\n",
    "        p = p.parent\n",
    "    raise RuntimeError('Could not find repo root. Start Jupyter from the repo root.')\n",
    "\n",
    "\n",
    "PROJECT_ROOT = find_repo_root(Path.cwd())\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "RAW_DIR = DATA_DIR / 'raw'\n",
    "PROCESSED_DIR = DATA_DIR / 'processed'\n",
    "SAMPLE_DIR = DATA_DIR / 'sample'\n",
    "\n",
    "PROJECT_ROOT\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Concept\nThis notebook gives you the statistical vocabulary you'll use throughout the project.\n\nYou will build intuition for:\n- when correlations are meaningful vs misleading,\n- why coefficients can become unstable when features are correlated,\n- how overfitting shows up as a gap between train and test performance,\n- how to read hypothesis tests (p-values / confidence intervals) without over-trusting them.\n\n## Distributions you will encounter in this project\n\nBefore diving in, here are the key probability distributions that appear in regression output and hypothesis tests:\n\n- **Normal (Gaussian)**: The bell curve. Regression errors are often assumed approximately normal. The CLT says sample means become approximately normal as $n$ grows, even if the underlying data is not.\n- **t-distribution**: Like the normal but with heavier tails. Used for coefficient t-tests when the sample is finite. Approaches the normal as $n \\to \\infty$. Degrees of freedom = $n - k - 1$ (observations minus parameters).\n- **$\\chi^2$ (chi-squared)**: Sum of squared standard normals. Appears in tests for heteroskedasticity (Breusch-Pagan, White) and goodness-of-fit.\n- **F-distribution**: Ratio of two $\\chi^2$ variables. Used for testing whether a group of coefficients are jointly zero (F-test). The `statsmodels` summary reports the overall F-statistic and its p-value.\n\nYou don't need to memorize the PDFs — just know when each distribution appears and what it tests.\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primer: `statsmodels` vs `scikit-learn` (inference vs prediction)\n",
    "\n",
    "This repo uses both libraries because they serve different goals:\n",
    "\n",
    "- **Prediction (ML):** optimize out-of-sample accuracy → `scikit-learn`\n",
    "- **Inference (econometrics):** interpret coefficients + quantify uncertainty → `statsmodels`\n",
    "\n",
    "### Minimal `statsmodels` OLS pattern\n",
    "\n",
    "```python\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# X: DataFrame of features, y: Series target\n",
    "Xc = sm.add_constant(X, has_constant=\"add\")  # add intercept\n",
    "res = sm.OLS(y, Xc).fit()\n",
    "print(res.summary())\n",
    "```\n",
    "\n",
    "**Expected output / sanity check**\n",
    "- a table with `coef`, `std err`, `t`, `P>|t|`, and a CI column\n",
    "- coefficient names match your column names\n",
    "\n",
    "### What you are looking at in `res.summary()`\n",
    "\n",
    "- **coef**: $\\\\hat\\\\beta$ (estimated effect in the model)\n",
    "- **std err**: estimated uncertainty $\\\\widehat{SE}(\\\\hat\\\\beta)$\n",
    "- **t**: $\\\\hat\\\\beta / \\\\widehat{SE}(\\\\hat\\\\beta)$\n",
    "- **P>|t|**: p-value for $H_0: \\\\beta=0$ (conditional on assumptions)\n",
    "- **[0.025, 0.975]**: 95% confidence interval\n",
    "\n",
    "### Robust standard errors (change uncertainty, not coefficients)\n",
    "\n",
    "```python\n",
    "# Cross-section heteroskedasticity\n",
    "res_hc3 = res.get_robustcov_results(cov_type=\"HC3\")\n",
    "\n",
    "# Time series autocorrelation + heteroskedasticity\n",
    "res_hac = res.get_robustcov_results(cov_type=\"HAC\", cov_kwds={\"maxlags\": 4})\n",
    "```\n",
    "\n",
    "### Common pitfalls (and quick fixes)\n",
    "\n",
    "- **Forgetting the intercept**\n",
    "  - Fix: always `add_constant`.\n",
    "- **Wrong SE for time series**\n",
    "  - Fix: use HAC when residuals are autocorrelated.\n",
    "- **Treating p-values as causal proof**\n",
    "  - Fix: write the identification assumption; otherwise interpret as association.\n",
    "- **Mixing prediction and inference**\n",
    "  - Fix: use `sklearn` pipelines + time splits for prediction; use `statsmodels` for coefficient uncertainty.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"correlation-vs-causation\"></a>\n",
    "## Correlation vs causation\n",
    "\n",
    "### Goal\n",
    "Simulate a classic confounding scenario where variables are correlated without a direct causal relationship.\n",
    "\n",
    "### Why this matters in economics\n",
    "Macro indicators often move together. If you interpret correlations as causal effects, you'll make confident but wrong stories.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Turn (1): Simulate a confounder\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# We will build: z -> x, z -> w, and z -> y.\n",
    "# That makes x and y correlated even if x doesn't directly cause y.\n",
    "\n",
    "rng = np.random.default_rng(0)\n",
    "n = 800\n",
    "\n",
    "# TODO: Simulate a hidden confounder z\n",
    "z = ...\n",
    "\n",
    "# TODO: Create x and w that both depend on z (plus noise)\n",
    "x = ...\n",
    "w = ...\n",
    "\n",
    "# TODO: Create y that depends on z (plus noise)\n",
    "y = ...\n",
    "\n",
    "df = pd.DataFrame({'z': z, 'x': x, 'w': w, 'y': y})\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Turn (2): Correlation matrix + interpretation\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: Compute df.corr() and interpret.\n",
    "# Questions:\n",
    "# 1) Are x and y correlated?\n",
    "# 2) Does that mean x causes y?\n",
    "# 3) Which variable is the common cause?\n",
    "corr = df.corr(numeric_only=True)\n",
    "corr\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional extension: a simple regression 'control' demo\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# TODO: Fit two regressions and compare the coefficient on x:\n",
    "# 1) y ~ x\n",
    "# 2) y ~ x + z\n",
    "# Hint: sm.add_constant + sm.OLS\n",
    "...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"multicollinearity-vif\"></a>\n",
    "## Multicollinearity (VIF)\n",
    "\n",
    "### Goal\n",
    "Create highly correlated predictors and see how they affect coefficient stability.\n",
    "\n",
    "### Key term\n",
    "> **Definition:** **Multicollinearity** means your predictors are strongly correlated with each other.\n",
    "It doesn't necessarily hurt prediction, but it can make coefficient interpretation unstable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Turn (1): Build correlated features\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from src.econometrics import vif_table\n",
    "\n",
    "# TODO: Create x1 and x2 that are almost the same\n",
    "rng = np.random.default_rng(1)\n",
    "n = 600\n",
    "x1 = rng.normal(size=n)\n",
    "x2 = ...  # make this highly correlated with x1\n",
    "\n",
    "# Target depends mostly on x1\n",
    "eps = rng.normal(scale=1.0, size=n)\n",
    "y2 = 1.0 + 2.0 * x1 + eps\n",
    "\n",
    "df2 = pd.DataFrame({'y': y2, 'x1': x1, 'x2': x2})\n",
    "df2[['x1','x2']].corr()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Turn (2): Compute VIF + interpret\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: Compute VIF for x1 and x2.\n",
    "# How large are the VIFs? What does that suggest?\n",
    "vif_table(df2, ['x1', 'x2'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Turn (3): Fit a regression and inspect coefficient stability\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# Fit y ~ x1 + x2 and inspect coefficients.\n",
    "# TODO: Compare to fitting y ~ x1 alone.\n",
    "X_both = sm.add_constant(df2[['x1', 'x2']])\n",
    "res_both = sm.OLS(df2['y'], X_both).fit()\n",
    "\n",
    "X_one = sm.add_constant(df2[['x1']])\n",
    "res_one = sm.OLS(df2['y'], X_one).fit()\n",
    "\n",
    "# TODO: Print the two coefficient estimates on x1\n",
    "...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"bias-variance\"></a>\n",
    "## Bias/variance\n",
    "\n",
    "### Goal\n",
    "See overfitting as a train/test gap by comparing a simple model vs a flexible one.\n",
    "\n",
    "### Key term\n",
    "> **Definition:** **Overfitting** happens when a model fits noise in the training data and fails to generalize.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Turn (1): Create a non-linear dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "rng = np.random.default_rng(2)\n",
    "n = 400\n",
    "\n",
    "# TODO: Create x on [-3, 3]\n",
    "x = ...\n",
    "\n",
    "# TODO: Create y = sin(x) + noise (nonlinear)\n",
    "y = ...\n",
    "\n",
    "# Train/test split (random is OK here because this is NOT time series)\n",
    "split = int(n * 0.8)\n",
    "X = x.reshape(-1, 1)\n",
    "X_tr, X_te = X[:split], X[split:]\n",
    "y_tr, y_te = y[:split], y[split:]\n",
    "\n",
    "...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Turn (2): Fit linear vs tree and compare errors\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: Fit LinearRegression and a DecisionTreeRegressor\n",
    "# Compute RMSE on train and test for both\n",
    "...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Turn (3): Control model complexity\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: Refit the tree with different max_depth values (e.g., 2, 4, 8, None)\n",
    "# Track train/test RMSE and describe the pattern.\n",
    "...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primer: Hypothesis testing (p-values, t-stats, confidence intervals)\n",
    "\n",
    "You will see p-values, t-statistics, and confidence intervals in regression output (especially `statsmodels`). This primer gives you the minimum to interpret them correctly.\n",
    "\n",
    "### The objects (plain language)\n",
    "\n",
    "- **Null hypothesis** $H_0$: the default claim (often “no effect”).\n",
    "- **Alternative** $H_1$: the claim you consider if the data looks inconsistent with $H_0$.\n",
    "- **Test statistic**: “how far” your estimate is from the null, in uncertainty units.\n",
    "- **p-value**: probability (under the null *and model assumptions*) of seeing a test statistic at least as extreme as observed.\n",
    "- **Confidence interval (CI)**: a range of parameter values consistent with the data under assumptions.\n",
    "\n",
    "### What a p-value is NOT\n",
    "\n",
    "- Not the probability $H_0$ is true.\n",
    "- Not the probability the model is correct.\n",
    "- Not a measure of economic importance.\n",
    "\n",
    "### Regression t-test intuition\n",
    "\n",
    "In OLS, a common test is $H_0: \\\\beta_j = 0$.\n",
    "\n",
    "$$\n",
    "t_j = \\\\frac{\\\\hat\\\\beta_j}{\\\\widehat{SE}(\\\\hat\\\\beta_j)}\n",
    "$$\n",
    "\n",
    "If you change your SE estimator (HC3/HAC/cluster), you change $\\\\widehat{SE}$ and therefore the p-value, even if the coefficient stays the same.\n",
    "\n",
    "### Expected output / what you should look at in `res.summary()`\n",
    "\n",
    "- `coef`: effect size (in model units)\n",
    "- `std err`: uncertainty\n",
    "- CI columns: magnitude + uncertainty together\n",
    "\n",
    "### Common pitfalls in this project\n",
    "\n",
    "- Macro time series often have autocorrelation → naive SE too small → use HAC when interpreting p-values.\n",
    "- Multiple testing/spec-search can produce small p-values by chance.\n",
    "- Predictive success ≠ causal interpretation.\n",
    "\n",
    "### Tiny demo (toy; not project data)\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "rng = np.random.default_rng(0)\n",
    "n = 300\n",
    "x = rng.normal(size=n)\n",
    "y = 1.0 + 0.5 * x + rng.normal(scale=1.0, size=n)\n",
    "\n",
    "df = pd.DataFrame({\"y\": y, \"x\": x})\n",
    "X = sm.add_constant(df[[\"x\"]])\n",
    "res = sm.OLS(df[\"y\"], X).fit()\n",
    "print(res.summary())\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"hypothesis-testing\"></a>\n",
    "## Hypothesis testing\n",
    "\n",
    "### Goal\n",
    "Make p-values and confidence intervals concrete with a toy example.\n",
    "\n",
    "### Your Turn (1): One-sample t-test\n",
    "Simulate a sample whose true mean is not 0 and test whether you can detect it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "rng = np.random.default_rng(3)\n",
    "\n",
    "# TODO: Simulate x with a small non-zero mean (e.g., 0.1) and some noise\n",
    "x = ...\n",
    "\n",
    "# TODO: Run a one-sample t-test for mean == 0\n",
    "t_stat, p_val = ...\n",
    "print('t:', t_stat, 'p:', p_val)\n",
    "\n",
    "# TODO: Explain: what does this p-value mean in words?\n",
    "...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Turn (2): Regression coefficient test\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# TODO: Simulate a simple linear relationship y = 1 + 0.5*x + noise\n",
    "rng = np.random.default_rng(4)\n",
    "n = 300\n",
    "x = ...\n",
    "y = ...\n",
    "\n",
    "df_ht = pd.DataFrame({'y': y, 'x': x})\n",
    "X = sm.add_constant(df_ht[['x']])\n",
    "res = sm.OLS(df_ht['y'], X).fit()\n",
    "\n",
    "# TODO: Print coefficient, SE, p-value, and 95% CI for x\n",
    "...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"checkpoint-self-check\"></a>\n",
    "## Checkpoint (Self-Check)\n",
    "Run a few asserts and write 2-3 sentences summarizing what you verified.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: Run a quick sanity check on any DataFrame/Series you created in this notebook.\n",
    "# Example (adjust variable names):\n",
    "# assert df.index.is_monotonic_increasing\n",
    "# assert df.isna().sum().sum() == 0\n",
    "#\n",
    "# TODO: Write 2-3 sentences:\n",
    "# - What would leakage look like in YOUR code?\n",
    "# - How would you detect it?\n",
    "...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions (Optional)\n",
    "- Try one additional variant beyond the main path (different features, different split, different model).\n",
    "- Write down what improved, what got worse, and your hypothesis for why.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection\n",
    "- What did you assume implicitly (about timing, availability, stationarity, or costs)?\n",
    "- If you had to ship this model, what would you monitor?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"solutions-reference\"></a>\n",
    "## Solutions (Reference)\n",
    "\n",
    "Try the TODOs first. Use these only to unblock yourself or to compare approaches.\n",
    "\n",
    "<details><summary>Solution: Correlation vs causation</summary>\n",
    "\n",
    "_One possible approach. Your variable names may differ; align them with the notebook._\n",
    "\n",
    "```python\n",
    "# Reference solution for 02_stats_basics_for_ml — Correlation vs causation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "rng = np.random.default_rng(0)\n",
    "n = 800\n",
    "# Hidden confounder\n",
    "z = rng.normal(size=n)\n",
    "\n",
    "# Two observed variables both driven by z\n",
    "x = z + rng.normal(scale=0.8, size=n)\n",
    "w = z + rng.normal(scale=0.8, size=n)\n",
    "\n",
    "# Target driven by z (not by x directly)\n",
    "y = 2.0 * z + rng.normal(scale=1.0, size=n)\n",
    "\n",
    "df = pd.DataFrame({'z': z, 'x': x, 'w': w, 'y': y})\n",
    "df.corr(numeric_only=True)\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<details><summary>Solution: Multicollinearity (VIF)</summary>\n",
    "\n",
    "_One possible approach. Your variable names may differ; align them with the notebook._\n",
    "\n",
    "```python\n",
    "# Reference solution for 02_stats_basics_for_ml — Multicollinearity (VIF)\n",
    "import statsmodels.api as sm\n",
    "from src.econometrics import vif_table\n",
    "\n",
    "# Build highly correlated predictors\n",
    "rng = np.random.default_rng(1)\n",
    "n = 600\n",
    "x1 = rng.normal(size=n)\n",
    "x2 = 0.95 * x1 + rng.normal(scale=0.2, size=n)\n",
    "y = 1.0 + 2.0 * x1 + rng.normal(scale=1.0, size=n)\n",
    "df = pd.DataFrame({'y': y, 'x1': x1, 'x2': x2})\n",
    "\n",
    "vif_table(df, ['x1', 'x2'])\n",
    "\n",
    "X = sm.add_constant(df[['x1', 'x2']])\n",
    "res = sm.OLS(df['y'], X).fit()\n",
    "print(res.summary())\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<details><summary>Solution: Bias/variance</summary>\n",
    "\n",
    "_One possible approach. Your variable names may differ; align them with the notebook._\n",
    "\n",
    "```python\n",
    "# Reference solution for 02_stats_basics_for_ml — Bias/variance\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Simple 1D regression problem\n",
    "rng = np.random.default_rng(2)\n",
    "n = 400\n",
    "x = np.linspace(-3, 3, n)\n",
    "y = np.sin(x) + rng.normal(scale=0.2, size=n)\n",
    "\n",
    "X = x.reshape(-1, 1)\n",
    "split = int(n * 0.8)\n",
    "\n",
    "lin = LinearRegression().fit(X[:split], y[:split])\n",
    "tree = DecisionTreeRegressor(random_state=0).fit(X[:split], y[:split])\n",
    "\n",
    "rmse_lin_tr = mean_squared_error(y[:split], lin.predict(X[:split]), squared=False)\n",
    "rmse_lin_te = mean_squared_error(y[split:], lin.predict(X[split:]), squared=False)\n",
    "rmse_tree_tr = mean_squared_error(y[:split], tree.predict(X[:split]), squared=False)\n",
    "rmse_tree_te = mean_squared_error(y[split:], tree.predict(X[split:]), squared=False)\n",
    "\n",
    "(\n",
    "    {'linear_train': rmse_lin_tr, 'linear_test': rmse_lin_te},\n",
    "    {'tree_train': rmse_tree_tr, 'tree_test': rmse_tree_te},\n",
    ")\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<details><summary>Solution: Hypothesis testing</summary>\n",
    "\n",
    "_One possible approach. Your variable names may differ; align them with the notebook._\n",
    "\n",
    "```python\n",
    "# Reference solution for 02_stats_basics_for_ml — Hypothesis testing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "\n",
    "# One-sample t-test: mean(x) == 0?\n",
    "rng = np.random.default_rng(3)\n",
    "x = rng.normal(loc=0.1, scale=1.0, size=200)\n",
    "t_stat, p_val = stats.ttest_1samp(x, popmean=0.0)\n",
    "print('t:', t_stat, 'p:', p_val)\n",
    "\n",
    "# Regression coefficient test: slope == 0?\n",
    "rng = np.random.default_rng(4)\n",
    "n = 300\n",
    "x2 = rng.normal(size=n)\n",
    "y2 = 1.0 + 0.5 * x2 + rng.normal(scale=1.0, size=n)\n",
    "\n",
    "df = pd.DataFrame({'y': y2, 'x': x2})\n",
    "X = sm.add_constant(df[['x']], has_constant='add')\n",
    "res = sm.OLS(df['y'], X).fit()\n",
    "\n",
    "print(res.summary())\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}